{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author Survival Analysis\n",
    "===\n",
    "\n",
    "Do sites last longer when receiving interactions from a peer?\n",
    "\n",
    "Potential analysis: does receiving an interaction from an existing author influence survival?  Could use survival analysis code from Ruyuan's work... maybe?\n",
    " - For people who receive some bucket of interactions (i.e. 0-10, 10-50, 50-1000, etc.) in the first 30 days, compare the survival of people who receive interactions from authors. Do those people survive longer?\n",
    " - Could try to use internal data; would be a good test\n",
    " - Could extend this analysis by looking at magnitude (% of interactions from authors)\n",
    " - Could also look at author similarity or some similar measure: does getting interacted with by an author who is more cosine similar to you in their writings better than an interaction from someone who is less similar to you?\n",
    " \n",
    "Tangential: Are people who are initiated with more likely to initiate with OTHERS i.e. not just reciprocate, but actually reach out to a third party as well?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import sqlite3\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import lifelines\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)\n",
    "import cbcore.data.paths as paths\n",
    "import cbcore.data.dates as dates\n",
    "import cbcore.data.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "figures_dir = os.path.join(git_root_dir, 'figures')\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "user_site_df = pd.read_csv(os.path.join(model_data_dir, 'user_site_df.csv'))\n",
    "valid_user_ids = set(user_site_df.user_id)\n",
    "valid_site_ids = set(user_site_df.site_id)\n",
    "print(f\"Read {len(user_site_df)} user_site_df rows ({len(valid_user_ids)} unique users, {len(valid_site_ids)} unique sites) in {datetime.now() - s}.\")\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal metadata\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(journal_df.created_at.max() / 1000).isoformat(),\\\n",
    "datetime.utcfromtimestamp(journal_df.published_at.max() / 1000).isoformat(),\\\n",
    "datetime.utcfromtimestamp(np.quantile(journal_df.created_at, 0.999999) / 1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = journal_df[journal_df.user_id.isin(valid_user_ids)]\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.is_nontrivial.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_start_date = datetime.fromisoformat('2005-01-01').replace(tzinfo=pytz.UTC)\n",
    "#invalid_end_date = datetime.fromisoformat('2021-08-01').replace(tzinfo=pytz.UTC)\n",
    "invalid_end_date = datetime.fromisoformat('2022-04-01').replace(tzinfo=pytz.UTC)\n",
    "print(f\"Keeping journals between {invalid_start_date.isoformat()} and {invalid_end_date.isoformat()}.\")\n",
    "invalid_start_timestamp = invalid_start_date.timestamp() * 1000\n",
    "invalid_end_timestamp = invalid_end_date.timestamp() * 1000\n",
    "journal_df = journal_df[(journal_df.created_at>=invalid_start_timestamp)&(journal_df.created_at<=invalid_end_timestamp)]\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(journal_df.created_at.max() / 1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compute_early_site_interaction_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_count_dfs = []\n",
    "for int_type in ['amp', 'comment', 'guestbook']:\n",
    "    int_count_df = compute_early_site_interaction_counts.load_counts(int_type, as_dataframe=True)\n",
    "    int_count_dfs.append(int_count_df)\n",
    "len(int_count_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_count_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_count_df = pd.concat(int_count_dfs, axis=1, join='outer').fillna(0).astype(int)\n",
    "for total_col in ['n_early_int', 'n_early_author_int', 'n_early_self_int']:\n",
    "    cols = [total_col + \"_\" + int_type for int_type in ['amp', 'comment', 'guestbook']]\n",
    "    int_count_df[total_col] = int_count_df[cols].sum(1)  # sum by column\n",
    "print(f\"Merged int_count_df has shape {int_count_df.shape}.\")\n",
    "int_count_df.sort_values(by='n_early_int', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_count_df.sort_values(by='n_early_int_comment', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(int_count_df[['n_early_int', 'n_early_author_int', 'n_early_self_int']], bins=np.linspace(0, 100, 20), log=True, label=int_count_df[['n_early_int', 'n_early_author_int', 'n_early_self_int']].columns)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_start_dict, _ = compute_early_site_interaction_counts.build_start_timestamp_dicts(journal_df, invalid_end_date_str='2022-04-01')\n",
    "len(site_start_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify updates before threshold\n",
    "end_timestamp = np.max(journal_df.created_at)\n",
    "right_censor_threshold_days = 365\n",
    "right_censor_threshold_ms = 1000 * 60 * 60 * 24 * right_censor_threshold_days\n",
    "right_censored_timestamp_threshold = end_timestamp - right_censor_threshold_ms\n",
    "print(f\"Considering site censored if journal update after {datetime.utcfromtimestamp(int(right_censored_timestamp_threshold / 1000))}.\")\n",
    "\n",
    "site_activity_data = []\n",
    "for site_id, group in tqdm(journal_df[['site_id', 'created_at']].groupby('site_id')):\n",
    "    first_journal_timestamp = site_start_dict[site_id]\n",
    "    time_since_start = group.created_at - first_journal_timestamp\n",
    "    early_site_count = np.sum(time_since_start <= compute_early_site_interaction_counts.EARLY_SITE_COUNT_THRESHOLD_MS)\n",
    "    \n",
    "    last_journal_timestamp = np.max(group.created_at)\n",
    "    is_right_censored = last_journal_timestamp > right_censored_timestamp_threshold\n",
    "    site_tenure = last_journal_timestamp - first_journal_timestamp  # number of milliseconds since site start\n",
    "    \n",
    "    d = {\n",
    "        'site_id': site_id,\n",
    "        'first_journal_timestamp': first_journal_timestamp,\n",
    "        'last_journal_timestamp': last_journal_timestamp,\n",
    "        'early_journal_count': early_site_count,\n",
    "        'subsequent_journal_count': len(group) - early_site_count,\n",
    "        'site_tenure': site_tenure,\n",
    "        'is_right_censored': is_right_censored,\n",
    "    }\n",
    "    site_activity_data.append(d)\n",
    "site_df = pd.DataFrame(site_activity_data)\n",
    "len(site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_start_timestamp = datetime.fromisoformat('2021-09-01').replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "site_df = site_df[site_df.first_journal_timestamp <= study_start_timestamp].copy()\n",
    "len(site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hexbin(np.log(site_df.early_journal_count + 1), np.log(site_df.subsequent_journal_count + 1), bins='log', gridsize=20)\n",
    "plt.xlabel(f\"Early journal count (in first {compute_early_site_interaction_counts.EARLY_SITE_COUNT_THRESHOLD_MS / (1000 * 60 * 60 * 24):.1f} days) (log)\")\n",
    "plt.ylabel(\"Subsequent journal count (log)\")\n",
    "print(f\"{np.sum(site_df.subsequent_journal_count == 0) / len(site_df) * 100:.2f}% of sites have no subsequent journal updates.\")\n",
    "r, p = scipy.stats.pearsonr(site_df.early_journal_count, site_df.subsequent_journal_count)\n",
    "print(f\"Positive correlation between early and subsequent journal counts (r={r:.4f}).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the default threshold\n",
    "site_df.is_right_censored.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(site_df.isna())\n",
    "print(site_df.shape)\n",
    "site_df = site_df.join(int_count_df, on='site_id').fillna(0)\n",
    "print(site_df.shape)\n",
    "site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sites that receive interactions and author interactions specifically\n",
    "pd.crosstab(site_df.n_early_int > 0, site_df.n_early_author_int > 0, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survival time is additional time survived BEYOND the early period (e.g. first 30 days)\n",
    "site_df['survival_time_months'] = np.maximum(site_df.site_tenure - compute_early_site_interaction_counts.EARLY_SITE_COUNT_THRESHOLD_MS, 0) / (1000 * 60 * 60 * 24 * 30)\n",
    "assert np.all((site_df.subsequent_journal_count > 0) == (site_df.survival_time_months > 0))\n",
    "print(f\"{np.sum(site_df.survival_time_months == 0) / len(site_df) * 100:.2f}% of sites have 0 survival time.\")\n",
    "site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 7))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(site_df.survival_time_months / 12, log=True, bins=20)\n",
    "ax.set_title(\"Distribution of all site survival times\")\n",
    "ax.set_xlabel(\"Survival time (years)\")\n",
    "ax.set_ylabel(\"Site count\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.hist(site_df.survival_time_months * 30, bins=np.linspace(0, 30, 20), log=True)\n",
    "ax.set_title(\"Distribution of site survival times < 30 days\")\n",
    "ax.set_xlabel(\"Survival time (days)\")\n",
    "ax.set_ylabel(\"Site count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "x = site_df.n_early_int\n",
    "bins = np.arange(14)\n",
    "bins[-1] = np.max(x)\n",
    "print(bins)\n",
    "counts, bin_edges = np.histogram(x, bins=bins, density=False)\n",
    "probs, bin_edges = np.histogram(x, bins=bins, density=True)\n",
    "print(counts.shape, bin_edges.shape)\n",
    "#ax.hist(site_df.early_int_total, bins=bins, log=True)\n",
    "ax.bar(bin_edges[:-1], counts, width=1)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for i in bins[:-2]:\n",
    "    ax.text(bin_edges[i], counts[i], f\"{np.sum(x == i) / len(x) * 100:.2f}%\", ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quant in [0, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 0.95, 1]:\n",
    "    print(f\"{quant:.3f} {np.quantile(site_df.n_early_int, quant):>6.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the choice of censorship threshold on the pct of censored data and the estimated median survival time\n",
    "#censor_thresholds = [0, 30, 90, 364 / 2, 365, 500, 365*2]\n",
    "censor_thresholds = np.linspace(0, 365*2, num=50)\n",
    "pct_sites_considered_censored = []\n",
    "median_survival_times = []\n",
    "end_timestamp = np.max(journal_df.created_at)\n",
    "for right_censor_threshold_days in tqdm(censor_thresholds):\n",
    "    right_censor_threshold_ms = 1000 * 60 * 60 * 24 * right_censor_threshold_days\n",
    "    right_censor_timestamp_threshold = end_timestamp - right_censor_threshold_ms\n",
    "    #print(\"Threshold (days):\", right_censor_threshold_days)\n",
    "    \n",
    "    is_right_censored = site_df.last_journal_timestamp >= right_censor_timestamp_threshold\n",
    "    pct_sites_considered_censored.append(np.sum(is_right_censored) / len(is_right_censored))\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(site_df.survival_time_months, event_observed=~is_right_censored)\n",
    "    median_survival_time = kmf.median_survival_time_\n",
    "    #print(\"KM median survival time:\", median_survival_time)\n",
    "    median_survival_times.append(median_survival_time)\n",
    "median_survival_times = np.array(median_survival_times) * 30  # convert to days\n",
    "    \n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "axes[0].plot(censor_thresholds, pct_sites_considered_censored, marker='o')\n",
    "axes[0].set_title(\"Percent of sites considered censored\")\n",
    "axes[0].set_xlabel(\"Number of days to the end of the data-collection period\")\n",
    "axes[0].set_ylabel(\"Percent of sites considered censored\")\n",
    "\n",
    "axes[1].plot(censor_thresholds, median_survival_times, marker='o')\n",
    "axes[1].set_title(\"Estimated median survival time\")\n",
    "axes[1].set_xlabel(\"Number of days to the end of the data-collection period\")\n",
    "axes[1].set_ylabel(\"Estimated median survival time (in days, KM model)\")\n",
    "\n",
    "plt.axvline(90, color='gray', alpha=0.6, linestyle='--')\n",
    "i = np.abs(censor_thresholds-90).argmin()\n",
    "plt.text(censor_thresholds[i], median_survival_times[i], f\"{median_survival_times[i]:.1f}\", va='bottom', ha='right')\n",
    "\n",
    "plt.axvline(365, color='gray', alpha=0.6, linestyle='--')\n",
    "i = np.abs(censor_thresholds-365).argmin()\n",
    "plt.text(censor_thresholds[i], median_survival_times[i], f\"{median_survival_times[i]:.1f}\", va='bottom', ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set censor threshold to 90\n",
    "right_censor_threshold_days = 90\n",
    "right_censor_threshold_ms = 1000 * 60 * 60 * 24 * right_censor_threshold_days\n",
    "right_censor_timestamp_threshold = end_timestamp - right_censor_threshold_ms\n",
    "site_df.is_right_censored = site_df.last_journal_timestamp >= right_censor_timestamp_threshold\n",
    "site_df.is_right_censored.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_start_date = datetime.fromisoformat('2014-01-01').replace(tzinfo=pytz.UTC)\n",
    "site_start_timestamp = site_start_date.timestamp() * 1000\n",
    "sdf = site_df[site_df.first_journal_timestamp >= site_start_timestamp].copy()\n",
    "print(f\"Identified {len(sdf)} / {len(site_df)} sites that started after {site_start_date}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = scipy.stats.pearsonr(sdf.early_journal_count, sdf.subsequent_journal_count)\n",
    "print(f\"Positive correlation between early and subsequent journal counts (r={r:.4f}).\")\n",
    "print(\"Quantile   Int Count\")\n",
    "for quant in [0, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 0.95, 1]:\n",
    "    print(f\"{quant:.3f} {np.quantile(sdf.n_early_int, quant):>6.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "x = sdf.n_early_int\n",
    "bins = np.arange(14)\n",
    "bins[-1] = np.max(x)\n",
    "counts, bin_edges = np.histogram(x, bins=bins, density=False)\n",
    "probs, bin_edges = np.histogram(x, bins=bins, density=True)\n",
    "ax.bar(bin_edges[:-1], counts, width=1)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Early Int Count\")\n",
    "ax.set_ylabel(\"Site Count\")\n",
    "ax.set_title(f\"Distribution of early interactions for {len(x):,} sites\")\n",
    "for i in bins[:-2]:\n",
    "    ax.text(bin_edges[i], counts[i], f\"{np.sum(x == i) / len(x) * 100:.2f}%\", ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_early_int_cat(n_early_int):\n",
    "    if n_early_int == 0:\n",
    "        return 0\n",
    "    elif n_early_int <= 38:  # 50th percentile\n",
    "        return 1\n",
    "    elif n_early_int <= 167:  # 75th percentile\n",
    "        return 2\n",
    "    elif n_early_int <= 708:  # 95th percentile\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "sdf['early_int_cat'] = sdf.n_early_int.map(get_early_int_cat)\n",
    "sdf.early_int_cat.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['has_early_author_int'] = sdf.n_early_author_int > 0\n",
    "sdf.has_early_author_int.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(sdf.early_int_cat, sdf.has_early_author_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean survival time in months (beyond 30 days)\n",
    "pd.crosstab(sdf.early_int_cat, sdf.has_early_author_int, values=sdf.survival_time_months, aggfunc=lambda vals: f\"M={np.mean(vals):.2f} (SD={np.std(vals):.2f}) med={np.median(vals):.1f}\", margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for int_count in np.arange(0, 16):\n",
    "    ssdf = sdf[sdf.n_early_int == int_count]\n",
    "    if int_count == 0:\n",
    "        ssdf = sdf[sdf.n_early_int > 0]\n",
    "    ssdf_y = ssdf[ssdf.has_early_author_int]\n",
    "    ssdf_n = ssdf[~ssdf.has_early_author_int]\n",
    "    \n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(ssdf_y.survival_time_months, event_observed=~ssdf_y.is_right_censored)\n",
    "    auth_median_survival_time = kmf.median_survival_time_\n",
    "    \n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(ssdf_n.survival_time_months, event_observed=~ssdf_n.is_right_censored)\n",
    "    noauth_median_survival_time = kmf.median_survival_time_\n",
    "    \n",
    "    d = {\n",
    "        'n_early_ints': int_count if int_count > 0 else '>0' ,\n",
    "        'n_sites': len(ssdf),\n",
    "        'n_sites_author': len(ssdf_y),\n",
    "        '%_author_int': f'{len(ssdf_y) / len(ssdf) * 100:.2f}%',\n",
    "        'M (author)': np.mean(ssdf_y.survival_time_months),\n",
    "        'M (no author)': np.mean(ssdf_n.survival_time_months),\n",
    "        'Med (author)': np.median(ssdf_y.survival_time_months),\n",
    "        'Med (no author)': np.median(ssdf_n.survival_time_months),\n",
    "        'Med-KM (author)': auth_median_survival_time,\n",
    "        'Med-KM (no author)': noauth_median_survival_time,\n",
    "        '% > 0 (author)': np.sum(ssdf_y.survival_time_months > 0) / len(ssdf_y),\n",
    "        '% > 0 (no author)': np.sum(ssdf_n.survival_time_months > 0) / len(ssdf_n),\n",
    "        'Mean subsequent updates (author)': np.mean(ssdf_y.subsequent_journal_count),\n",
    "        'Mean subsequent updates (no author)': np.mean(ssdf_n.subsequent_journal_count),\n",
    "        'Med subsequent updates (author)': np.median(ssdf_y.subsequent_journal_count),\n",
    "        'Med subsequent updates (no author)': np.median(ssdf_n.subsequent_journal_count),\n",
    "    }\n",
    "    data.append(d)\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.plotting import add_at_risk_counts\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "title_map = {\n",
    "    0: '0 interactions',\n",
    "    1: '1-12 interactions (bottom 50%)',\n",
    "    2: '12-104 interactions (50-75%)',\n",
    "    3: '104-496 interactions (75-95%)',\n",
    "    4: '496+ interactions (top 5%)',\n",
    "}\n",
    "\n",
    "timeline = np.arange(25)\n",
    "\n",
    "for early_int_cat, ax in zip([1, 2, 3, 4], axes):\n",
    "    ssdf = sdf[sdf.early_int_cat == early_int_cat]\n",
    "    ssdf_y = ssdf[ssdf.has_early_author_int]\n",
    "    ssdf_n = ssdf[~ssdf.has_early_author_int]\n",
    "    \n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(ssdf_y.survival_time_months, event_observed=~ssdf_y.is_right_censored, timeline=timeline)\n",
    "    auth_median_survival_time = kmf.median_survival_time_\n",
    "    auth_kmf = kmf\n",
    "    kmf.plot_survival_function(at_risk_counts=False, ax=ax, label=f'Author Int ({len(ssdf_y) / len(ssdf) * 100:.1f}% of sites)')\n",
    "    \n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(ssdf_n.survival_time_months, event_observed=~ssdf_n.is_right_censored, timeline=timeline)\n",
    "    noauth_median_survival_time = kmf.median_survival_time_\n",
    "    noauth_kmf = kmf\n",
    "    kmf.plot_survival_function(at_risk_counts=False, ax=ax, label='No Author Int')\n",
    "    \n",
    "    add_at_risk_counts(auth_kmf, noauth_kmf, ax=ax, labels=['Author Int', 'No Author Int'])\n",
    "    \n",
    "    ax.set_title(title_map[early_int_cat])\n",
    "\n",
    "    auth_mean = np.mean(ssdf_y.survival_time_months)\n",
    "    y = auth_kmf.survival_function_.loc[np.floor(auth_mean)]\n",
    "    ax.scatter([auth_mean,], [y,], color='blue', marker='o', label=f'M={auth_mean:.2f} (SD={np.std(ssdf_y.survival_time_months):.2f})')\n",
    "    y = auth_kmf.survival_function_.loc[np.floor(auth_median_survival_time)]\n",
    "    ax.scatter([auth_median_survival_time,], [y,], color='blue', marker='v', label=f'Median={auth_median_survival_time:.0f}mos ({np.median(ssdf_y.survival_time_months):.0f} raw)')\n",
    "    \n",
    "    auth_mean = np.mean(ssdf_n.survival_time_months)\n",
    "    y = noauth_kmf.survival_function_.loc[np.floor(auth_mean)]\n",
    "    ax.scatter([auth_mean,], [y,], color='orange', marker='o', label=f'M={auth_mean:.2f} (SD={np.std(ssdf_n.survival_time_months):.2f})')\n",
    "    y = noauth_kmf.survival_function_.loc[np.floor(noauth_median_survival_time)]\n",
    "    ax.scatter([noauth_median_survival_time,], [y,], color='orange', marker='v', label=f'Median={noauth_median_survival_time:.0f}mos ({np.median(ssdf_n.survival_time_months):.0f} raw)')\n",
    "    \n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdf = sdf[sdf.n_early_int > 0]\n",
    "md = smf.glm(formula=\"subsequent_journal_count ~ early_journal_count + n_early_int + has_early_author_int\", data=ssdf, family=statsmodels.genmod.families.family.Poisson())\n",
    "res = md.fit(cov_type='HC0')\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(res.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.quantile(ssdf.n_early_int, q) for q in np.arange(0, 1.1, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.quantile(ssdf.early_journal_count, q) for q in np.arange(0, 1.1, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_journal_count = np.quantile(ssdf.early_journal_count, 0.5)\n",
    "n_early_int = np.quantile(ssdf.n_early_int, 0.5)\n",
    "\n",
    "early_journal_count = np.mean(ssdf.early_journal_count)\n",
    "n_early_int = np.mean(ssdf.n_early_int)\n",
    "\n",
    "X = pd.DataFrame([{'has_early_author_int': True, 'early_journal_count': early_journal_count, 'n_early_int': n_early_int}])\n",
    "pred_y = res.predict(X).iloc[0]\n",
    "X = pd.DataFrame([{'has_early_author_int': False, 'early_journal_count': early_journal_count, 'n_early_int': n_early_int}])\n",
    "pred_n = res.predict(X).iloc[0]\n",
    "print(early_journal_count, n_early_int, pred_y - pred_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "for early_journal_count_q in np.arange(0, 1, 0.1):\n",
    "    early_journal_count = np.quantile(ssdf.early_journal_count, early_journal_count_q)\n",
    "    ds.append([])\n",
    "    line = \"\"\n",
    "    for n_early_int_q in np.arange(0, 1, 0.1):\n",
    "        n_early_int = np.quantile(ssdf.n_early_int, n_early_int_q)\n",
    "        \n",
    "        X = pd.DataFrame([{'has_early_author_int': True, 'early_journal_count': early_journal_count, 'n_early_int': n_early_int}])\n",
    "        pred_y = res.predict(X).iloc[0]\n",
    "        X = pd.DataFrame([{'has_early_author_int': False, 'early_journal_count': early_journal_count, 'n_early_int': n_early_int}])\n",
    "        pred_n = res.predict(X).iloc[0]\n",
    "        #print(early_journal_count, n_early_int, pred_y - pred_n)\n",
    "        ds[-1].append(pred_y - pred_n)\n",
    "        line += f\" {pred_y - pred_n:.2f}\"\n",
    "    print(line)\n",
    "diffs = np.array(ds)\n",
    "diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing too wild here... \n",
    "# basic point: number of early interactions is much less important than number of early journal for early interactors\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.matshow(diffs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdf = sdf[sdf.n_early_int > 0]\n",
    "ssdf_y = ssdf[ssdf.has_early_author_int]\n",
    "ssdf_n = ssdf[~ssdf.has_early_author_int]\n",
    "\n",
    "print(f\"Site Count & {len(ssdf_y):,} & {len(ssdf_n):,} & {len(ssdf_y) - len(ssdf_n):,} & - \\\\\\\\\")\n",
    "\n",
    "y = ssdf_y.n_early_int\n",
    "n = ssdf_n.n_early_int\n",
    "tstat, p = scipy.stats.ttest_ind(y, n, equal_var=False)    \n",
    "ustat, up = scipy.stats.mannwhitneyu(y, n)\n",
    "cles = ustat / (len(y) * len(n)) * 100\n",
    "assert p < 0.005 and up < 0.005\n",
    "print(f\"Early Interactions (M; SD) & {y.mean():.1f} ({y.std():.1f}) & {n.mean():.1f} ({n.std():.1f}) & {y.mean() - n.mean():.1f} & {cles:.1f}\\\\% \\\\\\\\\")\n",
    "\n",
    "y = ssdf_y.early_journal_count\n",
    "n = ssdf_n.early_journal_count\n",
    "tstat, p = scipy.stats.ttest_ind(y, n, equal_var=False)    \n",
    "ustat, up = scipy.stats.mannwhitneyu(y, n)\n",
    "cles = ustat / (len(y) * len(n)) * 100\n",
    "assert p < 0.005 and up < 0.005\n",
    "print(f\"Early Journals (M; SD) & {y.mean():.1f} ({y.std():.1f}) & {n.mean():.1f} ({n.std():.1f}) & {y.mean() - n.mean():.1f} & {100 - cles:.1f}\\\\% \\\\\\\\\")\n",
    "\n",
    "y = ssdf_y.survival_time_months\n",
    "n = ssdf_n.survival_time_months\n",
    "tstat, p = scipy.stats.ttest_ind(y, n, equal_var=False)    \n",
    "ustat, up = scipy.stats.mannwhitneyu(y, n)\n",
    "cles = ustat / (len(y) * len(n)) * 100\n",
    "assert p < 0.005 and up < 0.005\n",
    "print(f\"Site Tenure (M; SD) & {y.mean():.1f} ({y.std():.1f}) & {n.mean():.1f} ({n.std():.1f}) & +{y.mean() - n.mean():.1f}mos & {100 - cles:.1f}\\\\% \\\\\\\\\")\n",
    "\n",
    "y = ssdf_y.survival_time_months\n",
    "n = ssdf_n.survival_time_months\n",
    "tstat, p = scipy.stats.ttest_ind(y, n, equal_var=False)    \n",
    "ustat, up = scipy.stats.mannwhitneyu(y, n)\n",
    "cles = ustat / (len(y) * len(n)) * 100\n",
    "assert p < 0.005 and up < 0.005\n",
    "print(f\"Site Tenure (Median) & {y.median():.1f} & {n.median():.1f} & +{y.median() - n.median():.1f}mos & - \\\\\\\\\")\n",
    "\n",
    "y = ssdf_y.subsequent_journal_count\n",
    "n = ssdf_n.subsequent_journal_count\n",
    "tstat, p = scipy.stats.ttest_ind(y, n, equal_var=False)    \n",
    "ustat, up = scipy.stats.mannwhitneyu(y, n)\n",
    "cles = ustat / (len(y) * len(n)) * 100\n",
    "assert p < 0.005 and up < 0.005\n",
    "print(f\"\\\\# Journals (M; SD) & {y.mean():.1f} ({y.std():.1f}) & {n.mean():.1f} ({n.std():.1f}) & +{y.mean() - n.mean():.1f} & {100 - cles:.1f}\\\\% \\\\\\\\\")\n",
    "\n",
    "y = ssdf_y.subsequent_journal_count\n",
    "n = ssdf_n.subsequent_journal_count\n",
    "tstat, p = scipy.stats.ttest_ind(y, n, equal_var=False)    \n",
    "ustat, up = scipy.stats.mannwhitneyu(y, n)\n",
    "cles = ustat / (len(y) * len(n)) * 100\n",
    "assert p < 0.005 and up < 0.005\n",
    "print(f\"\\\\# Journals (Median) & {y.median():.1f} & {n.median():.1f} & +{y.median() - n.median():.1f}mos & - \\\\\\\\\")\n",
    "\n",
    "\n",
    "pct_y = np.sum(ssdf_y.survival_time_months > 0) / len(ssdf_y) * 100\n",
    "pct_n = np.sum(ssdf_n.survival_time_months > 0) / len(ssdf_n) * 100\n",
    "print(f\"\\\\% 2nd Journals & {pct_y:.1f}\\\\% & {pct_n:.1f}\\\\% & +{pct_y - pct_n:.1f}pp & - \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shared-conda)",
   "language": "python",
   "name": "shared-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
