{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Participant Profile Exploration\n",
    "===\n",
    "\n",
    "Generate a summary of participant's activities on CaringBridge.\n",
    "\n",
    " - Given the obfuscated email addresses, look up user_ids and other profile info.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.family'] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import bson\n",
    "from bson.codec_options import CodecOptions\n",
    "from bson.raw_bson import RawBSONDocument\n",
    "from bson import ObjectId\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import dateutil\n",
    "import pytz\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbcore.data.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(cbcore.data.paths.raw_data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "import cbrec.genconfig\n",
    "import cbrec.text.textdb\n",
    "#import cbrec.text.embeddingdb\n",
    "from cbrec.text import textdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the profile data\n",
    "s = datetime.now()\n",
    "profile_metadata_dir = os.path.join(cbcore.data.paths.derived_data_dir, 'profile')\n",
    "profile_df = pd.read_feather(os.path.join(profile_metadata_dir, 'profile.feather'))\n",
    "print(f\"Loaded {len(profile_df)} lines in {datetime.now() - s}.\")\n",
    "profile_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the site data\n",
    "s = datetime.now()\n",
    "site_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/site_metadata\"\n",
    "site_metadata_filepath = os.path.join(site_metadata_dir, \"site_metadata.feather\")\n",
    "site_df = pd.read_feather(site_metadata_filepath)\n",
    "print(f\"Read {len(site_df)} site_df rows in {datetime.now() - s}.\")\n",
    "site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_dir = os.path.join(git_root_dir, 'data', 'survey')\n",
    "survey_files = glob(survey_data_dir + \"/CaringBridge Author Recommendations Opt-In_*.tsv\")\n",
    "if len(survey_files) > 1:\n",
    "    survey_files.sort(key = lambda fname: int(fname.split(\",\")[0][-2:].strip()))\n",
    "    survey_filepath = survey_files[-1]\n",
    "else:\n",
    "    survey_filepath = survey_files[0]\n",
    "survey_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(survey_filepath, sep='\\t', encoding='utf-16')\n",
    "# need to trim off the 2 header lines\n",
    "df = df.iloc[2:]\n",
    "# identify emails based on the survey responses\n",
    "emails = []\n",
    "for cb_email, backup_email in zip(df.caringbridge_email_1_TEXT, df.caringbridge_email_2_TEXT):\n",
    "    email = cb_email\n",
    "    if pd.isna(email):\n",
    "        email = backup_email\n",
    "    if pd.isna(email):\n",
    "        email = \"\"\n",
    "    if email == \"zwlevonian@gmail.com\" or email.endswith(\"@caringbridge.org\"):\n",
    "        email = \"\"\n",
    "    emails.append(email)\n",
    "df['email'] = emails\n",
    "fdf = df[df.email != ''].copy()\n",
    "# compute end dates from response strings\n",
    "central_time = pytz.timezone('US/Central')\n",
    "fdf['end_date'] = fdf.EndDate.map(lambda dt_str: datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S').astimezone(central_time))\n",
    "print(f\"Responses from {fdf.end_date.min()} to {fdf.end_date.max()}\")\n",
    "survey_df = fdf.sort_values(by='end_date').drop_duplicates(subset=['email',], keep='last')\n",
    "print(len(survey_df), len(fdf))\n",
    "survey_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_email_map = {}\n",
    "email_name_map = {}\n",
    "with open(os.path.join(git_root_dir, 'data/email/participant_matched_20210831.tsv'), 'r') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        if line != \"\":\n",
    "            email_address, email_secret, first_name, last_name = line.split(\"\\t\")\n",
    "            secret_email_map[email_secret] = email_address\n",
    "            email_name_map[email_address] = (first_name, last_name)\n",
    "email_secrets = set(secret_email_map.keys())\n",
    "len(secret_email_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If new / updated participant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify unmatched emails\n",
    "matched_emails = set([ea.lower() for ea in secret_email_map.values()])\n",
    "unmatched_emails = []\n",
    "n_total = 0\n",
    "all_emails = set()\n",
    "with open(os.path.join(git_root_dir, 'data/survey/participant_emails.txt'), 'r') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        if line != \"\":\n",
    "            email_address = line.lower()\n",
    "            n_total += 1\n",
    "            all_emails.add(email_address)\n",
    "            if email_address not in matched_emails:\n",
    "                unmatched_emails.append(email_address)\n",
    "print(len(unmatched_emails), len(matched_emails), len(all_emails))\n",
    "if len(all_emails) != n_total:\n",
    "    print(f\"Read {n_total} lines but {len(all_emails)} emails; duplicates!\")\n",
    "assert len(unmatched_emails) + len(matched_emails) == len(all_emails)\n",
    "unmatched_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_id_matches = {\n",
    "    0: {\n",
    "        'study_email_address': 'test@example.com',\n",
    "        'profile_email_address': '',\n",
    "        'use_profile_email_address': False,\n",
    "        'first_name': 'Ellen',\n",
    "        'last_name': 'Smith',\n",
    "    },\n",
    "}\n",
    "for profile_id, match in profile_id_matches.items():\n",
    "    email_address = match['study_email_address']\n",
    "    unmatched_emails.remove(email_address)\n",
    "    matched_emails.add(email_address)\n",
    "print(len(unmatched_emails), len(matched_emails), len(all_emails))\n",
    "\n",
    "matched_profile_ids = set(profile_id_matches.keys())\n",
    "len(matched_profile_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset profiles to only matched users\n",
    "sprofile_df = profile_df[(profile_df.email_address.isin(email_secrets))|(profile_df.user_id.isin(matched_profile_ids))]\n",
    "len(sprofile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprofile_df = sprofile_df.reset_index(drop=True)\n",
    "sprofile_df['real_email_address'] = sprofile_df.email_address.map(lambda ea: secret_email_map[ea] if ea in secret_email_map else '')\n",
    "for profile_id, match in profile_id_matches.items():\n",
    "    email_address = match['study_email_address']\n",
    "    sprofile_df.loc[sprofile_df.user_id == profile_id, 'real_email_address'] = email_address\n",
    "    email_name_map[email_address] = (match['first_name'], match['last_name'])\n",
    "sprofile_df['first_name'] = sprofile_df.real_email_address.map(lambda ea: email_name_map[ea][0])\n",
    "sprofile_df['last_name'] = sprofile_df.real_email_address.map(lambda ea: email_name_map[ea][1])\n",
    "len(sprofile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the participant data to a file\n",
    "participant_id_filepath = os.path.join(git_root_dir, 'data/email/participant_ids.tsv')\n",
    "to_save = sprofile_df[['user_id', 'real_email_address', 'first_name', 'last_name']].reset_index(drop=True)\n",
    "to_save.to_csv(participant_id_filepath, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If no new participant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get participant data\n",
    "participant_id_filepath = os.path.join(git_root_dir, 'data/email/participant_ids.tsv')\n",
    "participant_df = pd.read_csv(participant_id_filepath, sep='\\t', header=0)\n",
    "print(len(participant_df))\n",
    "participant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_profile_ids = set(participant_df.user_id)\n",
    "sprofile_df = profile_df[profile_df.user_id.isin(matched_profile_ids)]\n",
    "sprofile_df = sprofile_df.merge(participant_df, how='left', on='user_id')\n",
    "len(sprofile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_emails = set(participant_df.real_email_address)\n",
    "len(matched_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join with survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset survey responses to only matched users\n",
    "ssurvey_df = survey_df[survey_df.email.map(lambda ea: ea.strip().lower().replace(\" \", \"\")).isin(matched_emails)]\n",
    "len(ssurvey_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprofile_df[['first_name', 'last_name', 'country', 'gender', 'isPrivate', 'isPublic', 'isSecure', 'language', 'location', 'numNotifications', 'tz', 'sms', 'email_isSubscriber']].sample(50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprofile_df.country.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprofile_df.tz.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the user_id for matched users to a file\n",
    "# pretty sure this is deprecated: not sure if any other notebook or process is consuming this, and participant_ids.tsv has more info\n",
    "with open(os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant', 'matched_participant_user_ids.tsv'), 'w') as outfile:\n",
    "    for row in sprofile_df.itertuples():\n",
    "        outfile.write(f\"{row.real_email_address}\\t{row.user_id}\\n\")\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 0.5))\n",
    "\n",
    "x = sprofile_df.createdAt\n",
    "ax.scatter(x, [1 for i in range(len(x))], marker='.', color='black', alpha=0.4)\n",
    "ax.set_ylim(0, 2)\n",
    "ax.set_yticks([])\n",
    "\n",
    "use_autoloc = True\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "else:\n",
    "    locs = bins\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick / 1000).strftime('%b %Y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "#ax.set_xscale('log')\n",
    "#ax.yaxis.set_ticks_position('left')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "#ax.xaxis.set_ticks_position('bottom')\n",
    "day = 1000 * 60 * 60 * 24\n",
    "ax.set_xlim(np.min(x) - (day * 30), np.max(x) + (day * 30))\n",
    "\n",
    "ax.set_title(f\"Account creation date for {len(sprofile_df):,} opted-in participants\")\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "relative_to = ssurvey_df.end_date.max().timestamp() * 1000#datetime.strptime('2021-08-20', '%Y-%m-%d').replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "x = sprofile_df.createdAt.map(lambda ts: int(ts))\n",
    "print(f\"{np.sum(x <= datetime.strptime('2016-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC).timestamp() * 1000) / len(x) * 100:.2f}% created pre-2016\")\n",
    "x = relative_to - x\n",
    "print(np.min(x) / 1000 / 60 / 60 / 24, np.max(x) / 1000 / 60 / 60 / 24)  # minimum and max difference in days\n",
    "\n",
    "bins = [0, 1000 * 60 * 60, 1000 * 60 * 60 * 24, 1000 * 60 * 60 * 24 * 7, 1000 * 60 * 60 * 24 * 30, 1000 * 60 * 60 * 24 * 365, 1000 * 60 * 60 * 24 * 365 * 5, np.max(x) + 1]\n",
    "counts, bin_edges = np.histogram(x, bins=bins)\n",
    "\n",
    "x = np.arange(len(counts)) + 1\n",
    "ax.bar(x, counts, width=0.9, color=matplotlib.cm.viridis(0.2))\n",
    "\n",
    "#ax.bar(0, np.sum(sdf.first_journal_timestamp.isna()), width=0.9, color=matplotlib.cm.viridis(0.5), label=f'No Journal updates ({np.sum(sdf.first_journal_timestamp.isna())/len(sdf)*100:.1f}% of sites)')\n",
    "#ax.axvline(0.5, linestyle='--', color='black', alpha=0.7)\n",
    "\n",
    "#ax.legend()\n",
    "ax.set_title(f\"Time since CaringBridge account creation\")\n",
    "\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(['<1 hour', '<1 day', '<1 week', '<1 month', '<1 year', '<5 years', '>'])\n",
    "for i, count in enumerate(counts):\n",
    "    ax.text(i+1, count + 0.1, f'{count / len(sprofile_df) * 100:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "email_survey_end_date_map = {email_address.strip().lower().replace(\" \", \"\"): end_date for email_address, end_date in zip(ssurvey_df.email, ssurvey_df.end_date)}\n",
    "\n",
    "xs = []\n",
    "for row in sprofile_df.itertuples():\n",
    "    x = (email_survey_end_date_map[row.real_email_address].timestamp() * 1000) - row.createdAt\n",
    "    xs.append(x)\n",
    "xs = np.array(xs)\n",
    "assert np.all(xs > 0)\n",
    "print(f\"Median time {np.median(xs) / 1000 / 60 / 60 / 24:.2f} days ({np.quantile(xs, 0.25) / 1000 / 60 / 60 / 24:.2f} - {np.quantile(xs, 0.75) / 1000 / 60 / 60 / 24:.2f})\")\n",
    "\n",
    "bins = [0, 1000 * 60 * 60, 1000 * 60 * 60 * 24, 1000 * 60 * 60 * 24 * 7, 1000 * 60 * 60 * 24 * 30, 1000 * 60 * 60 * 24 * 365, 1000 * 60 * 60 * 24 * 365 * 5, np.max(xs) + 1]\n",
    "counts, bin_edges = np.histogram(xs, bins=bins)\n",
    "\n",
    "x = np.arange(len(counts)) + 1\n",
    "ax.bar(x, counts, width=0.9, color=matplotlib.cm.viridis(0.2))\n",
    "\n",
    "\n",
    "ax.set_title(f\"Time between CaringBridge account creation and enrollment\\n for $n$={len(sprofile_df)} eligible participants\")\n",
    "ax.set_ylabel(\"Number of participants\")\n",
    "\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(['<1 hour', '<1 day', '<1 week', '<1 month', '<1 year', '<5 years', '>5 years'])\n",
    "for i, count in enumerate(counts):\n",
    "    ax.text(i+1, count + 0.1, f'{count / len(sprofile_df) * 100:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sprofile_df.createdAt\n",
    "start_time = datetime.utcfromtimestamp(np.min(x) / 1000).replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.utcfromtimestamp(np.max(x) / 1000).replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "bins.append(int(curr_time.timestamp() * 1000))\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "print(f'(actual from {datetime.utcfromtimestamp(bins[0] / 1000)} to {datetime.utcfromtimestamp(bins[-1] / 1000)})')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 2))\n",
    "\n",
    "total_counts, bin_edges = np.histogram(x, bins=bins)\n",
    "ax.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2)\n",
    "\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick / 1000).strftime('%b %Y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_title(f\"Date of {len(x):,} profile creations by participants\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging\n",
    "\n",
    "Create a few constructs that will be useful in other investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_user_ids = set(sprofile_df.user_id)\n",
    "len(participant_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_email_map = {row.user_id: row.real_email_address for row in sprofile_df.itertuples()}\n",
    "len(user_id_to_email_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site_profile merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbcore.script.computeCollectionCounts import iterate_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify site_profiles for participants\n",
    "site_profiles = []\n",
    "input_filepath = os.path.join(cbcore.data.paths.raw_data_filepath, 'site_profile.bson.gz')\n",
    "for doc in tqdm(iterate_collection(input_filepath), desc='Processing documents', total=83000000):\n",
    "    user_id = int(doc['userId']) if 'userId' in doc else -1\n",
    "    if user_id in participant_user_ids:\n",
    "        site_profiles.append(doc)\n",
    "len(site_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, 'site_profile.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(site_profiles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant')\n",
    "with open(os.path.join(participant_dir, 'site_profile.pkl'), 'rb') as infile:\n",
    "    site_profiles = pickle.load(infile)\n",
    "len(site_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "for sp in site_profiles:\n",
    "    user_id = int(sp['userId'])\n",
    "    site_id = int(sp['siteId']) if 'siteId' in sp else -1\n",
    "    # not capturing: n, nl\n",
    "    d = {\n",
    "        'user_id': user_id,\n",
    "        'site_id': site_id,\n",
    "        'is_creator': sp['isCreator'] if 'isCreator' in sp else None,\n",
    "        'is_primary': sp['isPrimary'] if 'isPrimary' in sp else None,\n",
    "        'role': sp['role'],\n",
    "        'is_profile_deleted': sp['isProfileDeleted'] if 'isProfileDeleted' in sp else None,\n",
    "        'is_site_deleted': sp['isSiteDeleted'] if 'isSiteDeleted' in sp else None,\n",
    "        'is_stub': sp['isStub'] if 'isStub' in sp else None,\n",
    "        'created_at': sp['createdAt'].timestamp() * 1000 if 'createdAt' in sp else 0,\n",
    "        'updated_at': sp['updatedAt'].timestamp() * 1000 if 'updatedAt' in sp else 0,\n",
    "    }\n",
    "    ds.append(d)\n",
    "\n",
    "ssite_profile_df = pd.DataFrame(ds)\n",
    "ssite_profile_df.sample(n=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df.role.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df.is_site_deleted.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df.is_profile_deleted.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one \"stub\" (don't know what this means)\n",
    "# anyway, the site was also deleted, as can be seen in both the site_profile and site entry\n",
    "display(ssite_profile_df[ssite_profile_df.is_stub == '1'])\n",
    "site_id = ssite_profile_df[ssite_profile_df.is_stub == '1'].iloc[0].site_id\n",
    "display(site_df[site_df.site_id == site_id])\n",
    "ssite_profile_df.is_stub.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df.is_primary.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df.is_creator.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_primary and is_creator are perfectly redundant\n",
    "pd.crosstab(ssite_profile_df.is_primary == '1', ssite_profile_df.is_creator == '1', dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(ssite_profile_df.role, ssite_profile_df.is_creator == '1', dropna=False, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df[ssite_profile_df.role == 'Removed'].sample(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = ssite_profile_df[ssite_profile_df.created_at > 0]\n",
    "start_time = datetime.utcfromtimestamp(np.min(sdf.created_at) / 1000).replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.utcfromtimestamp(np.max(sdf.created_at) / 1000).replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "bins.append(int(curr_time.timestamp() * 1000))\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "print(f'(actual from {datetime.utcfromtimestamp(bins[0] / 1000)} to {datetime.utcfromtimestamp(bins[-1] / 1000)})')\n",
    "\n",
    "print(f\"{np.sum(ssite_profile_df.created_at < bins[0])} below, {np.sum(ssite_profile_df.created_at > bins[-1])} above the expected time range\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "total_counts, bin_edges = np.histogram(sdf.created_at, bins=bins)\n",
    "ax.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2)\n",
    "\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick / 1000).strftime('%b %Y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_title(f\"Date of {len(sdf):,} first site visits by participants\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of site vists per user\n",
    "site_counts = ssite_profile_df.user_id.value_counts()\n",
    "unmatched_users = list(participant_user_ids - set(site_counts.index))\n",
    "site_counts = site_counts.append(pd.Series(index=unmatched_users, data=0))\n",
    "print(f\"{np.sum(site_counts == 0)} participants have visited 0 sites.\")\n",
    "print(f\"{np.sum(site_counts == 1)} participants have visited 1 site.\")\n",
    "print(f\"{np.sum(site_counts == 2)} participants have visited 2 sites.\")\n",
    "print(f\"{np.sum(site_counts >= 2)} ({np.sum(site_counts >= 2) / len(site_counts):.2%}) participants have visited 2+ sites.\")\n",
    "print(f\"{np.quantile(site_counts, 0.5)} median site profiles.\")\n",
    "site_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile_df = ssite_profile_df.groupby('user_id').agg({\n",
    "    'site_id': len,\n",
    "    'role': [lambda role: np.sum(role == 'Organizer'), lambda role: np.sum(role == 'Removed'), lambda role: np.sum(role == 'Visitor')],\n",
    "})\n",
    "user_profile_df.columns = user_profile_df.columns.get_level_values(1)\n",
    "\n",
    "user_profile_df = user_profile_df.rename(columns={\n",
    "    'len': 'n_sites',\n",
    "    '<lambda_0>': 'n_organizer',\n",
    "    '<lambda_1>': 'n_removed',\n",
    "    '<lambda_2>': 'n_visitor',\n",
    "})\n",
    "#unmatched_users = list(participant_user_ids - set(user_profile_df.index))\n",
    "#user_profile_df = user_profile_df.append(pd.DataFrame(index=unmatched_users, columns=user_profile_df.columns, data=0)) #data=[[0, 0, 0, 0],]))\n",
    "user_profile_df.sort_values(by='n_organizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile_df.merge(sprofile_df.set_index('user_id')[['real_email_address', 'first_name', 'last_name']], how='left', left_index=True, right_index=True).sort_values(by='n_organizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile_df = ssite_profile_df[['user_id', 'site_id', 'is_creator', 'is_primary', 'role', 'is_site_deleted']]\\\n",
    "    .merge(sprofile_df[['user_id', 'real_email_address', 'first_name', 'last_name']], how='left')\\\n",
    "    .merge(site_df[['site_id', 'name', 'title']], how='left')\n",
    "user_profile_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_profile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal metadata\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sjournal_df = journal_df[journal_df.user_id.isin(participant_user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_counts = sjournal_df.user_id.value_counts()\n",
    "unmatched_users = list(participant_user_ids - set(journal_counts.index))\n",
    "journal_counts = journal_counts.append(pd.Series(index=unmatched_users, data=0))\n",
    "print(f\"{np.sum(journal_counts == 0)} participants have written 0 journals.\")\n",
    "print(f\"{np.sum((journal_counts > 0)&(journal_counts < 3))} participants have written 1 or 2 journals.\")\n",
    "print(f\"{np.sum(journal_counts >= 3)} participants have written 3+ journals.\")\n",
    "journal_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(journal_counts, 0.5), np.quantile(journal_counts, 0.90), np.quantile(journal_counts, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_COUNT = 200\n",
    "MANUAL_HEIGHT = 22\n",
    "color = matplotlib.cm.viridis(0.2)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "bins = np.linspace(0, MAX_COUNT)\n",
    "print(\"how many cut off?\", np.sum(journal_counts >= MAX_COUNT), np.sum(journal_counts >= MAX_COUNT) / len(journal_counts))\n",
    "x = np.minimum(journal_counts, MAX_COUNT)\n",
    "totals, _, bar_patches = ax.hist(x, bins=bins, color=color)\n",
    "ax.set_ylim(0, MANUAL_HEIGHT)\n",
    "\n",
    "ax.set_xlabel(\"Total journal updates\")\n",
    "ax.set_ylabel(\"Number of participants\")\n",
    "\n",
    "print(f\"{np.sum(x == 0) / len(x) * 100:.1f}% ({np.sum(x == 0)}) participants have written no journal updates\")\n",
    "print(f\"Median participant has {np.quantile(x, 0.5)} journal updates\")\n",
    "subset_end = np.ceil(np.quantile(x, 0.5))\n",
    "print(subset_end)\n",
    "axins = ax.inset_axes([0.5, 0.5, 0.47, 0.47])\n",
    "axins.hist(x[x <= subset_end], bins=np.linspace(0, subset_end+1, 20), color=matplotlib.cm.viridis(0.4))\n",
    "axins.text(0.75, 0.75, f\"{np.sum(x <= subset_end) / len(x) *100:.1f}%\\nin\\n[0, {int(np.ceil(subset_end))}]\", transform=axins.transAxes, ha='center', va='center')\n",
    "rec_patch, lines = ax.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "rec_patch.set_height(np.max(totals))  # correct height\n",
    "# now need to fix the line positioning\n",
    "lines[1].set_visible(False)\n",
    "line = lines[1]  # upper left corner line\n",
    "verts = line.get_path().vertices\n",
    "start_pos = verts[0,:]\n",
    "end_pos = verts[2,:]\n",
    "end_pos[1] = np.max(totals)\n",
    "new_line = matplotlib.patches.FancyArrowPatch(posA=start_pos, posB=end_pos, arrowstyle='-', linewidth=0.7)\n",
    "ax.add_patch(new_line)\n",
    "\n",
    "ax.set_yticks(np.arange(0, MANUAL_HEIGHT+1, 2))\n",
    "#ax.yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: int(x)))\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: f\">{x:.0f}\" if x == 1500 else int(x)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_journal_counts = sjournal_df[sjournal_df.published_at > 0].groupby(by=['user_id', 'site_id']).journal_oid.count().rename('n_journals').reset_index()\n",
    "user_site_journal_counts.sort_values(by='n_journals', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sites_authored = user_site_journal_counts.groupby('user_id').site_id.count()\n",
    "print(f\"{np.sum(num_sites_authored > 1)} / {len(num_sites_authored)} = {np.sum(num_sites_authored > 1) / len(num_sites_authored) * 100:.1f}% of participants have authored more than 1 site\")\n",
    "# TODO what is this number for non-participants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible_authors = set(user_site_journal_counts[user_site_journal_counts.n_journals >= 3].user_id)\n",
    "ineligible_authors = participant_user_ids - eligible_authors\n",
    "len(eligible_authors), len(ineligible_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[user_id_to_email_map[user_id] for user_id in ineligible_authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of most recent journal updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cbrec\n",
    "except:\n",
    "    sys.path.append(\"/home/lana/levon003/repos/recsys-peer-match/src\")\n",
    "from cbrec import genconfig\n",
    "config = genconfig.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_oids = []\n",
    "n_groups = 0\n",
    "n_invalid = 0\n",
    "for key, group in sjournal_df[sjournal_df.is_nontrivial].sort_values(by=['user_id', 'site_id', 'created_at']).groupby(['user_id', 'site_id']):\n",
    "    n_groups += 1\n",
    "    user_site_journals = group.journal_oid.iloc[-12:]\n",
    "    if len(user_site_journals) < 3:\n",
    "        n_invalid += 1\n",
    "    journal_oids.extend(user_site_journals)\n",
    "len(journal_oids), n_groups, n_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config.model_data_dir, 'predict_participant_journal_oids.txt'), 'a') as outfile:\n",
    "    for journal_oid in journal_oids:\n",
    "        outfile.write(journal_oid + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of participant journal updates for author role annotation\n",
    "\n",
    "Goal: generate a consistent spreadsheet for annotation of Author Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_list = []\n",
    "td = textdb.TextDatabase(config)\n",
    "text_db = td.get_text_db()\n",
    "with text_db:\n",
    "    for key, group in sjournal_df[sjournal_df.published_at > 0].merge(sprofile_df[['user_id', 'first_name', 'last_name']], on='user_id').merge(site_df[['site_id', 'title']], on='site_id').sort_values(by=['user_id', 'site_id', 'published_at']).groupby(['user_id', 'site_id']):\n",
    "        if len(group) <= 6:\n",
    "            inds = np.arange(len(group))\n",
    "        else:\n",
    "            # take the first 3 and last 3 on the site\n",
    "            inds = np.array([0, 1, 2, -3, -2, -1])\n",
    "\n",
    "        for row in group.iloc[inds].itertuples():\n",
    "            raw_title, raw_body = td.get_raw_journal_text_from_db(text_db, row.journal_oid)\n",
    "            title = cbrec.text.textdb.clean_text(raw_title)\n",
    "            body = cbrec.text.textdb.clean_text(raw_body.replace(\"</div>\", \"</div> \\n\"))\n",
    "            journal_list.append({\n",
    "                'user_id': row.user_id,\n",
    "                'site_id': row.site_id,\n",
    "                'site_title': row.title,\n",
    "                'user_name': row.first_name + \" \" + row.last_name,\n",
    "                'published_at': datetime.utcfromtimestamp(row.published_at / 1000).strftime('%Y-%m-%d %H:%M'),\n",
    "                'site_index': row.site_index,\n",
    "                'title': title,\n",
    "                'body': body,\n",
    "                'author_type': \"\",\n",
    "                'notes': \"\",\n",
    "            })\n",
    "len(journal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.DataFrame(journal_list)\n",
    "sdf.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO read from the existing participant_author_type_annotations file and consider adding to any existing recorded journal updates...\n",
    "# basically, not clear what the \"continuing analysis\" plan is as far as new updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_type_filename = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant', 'participant_author_type_annotations.tsv')\n",
    "sdf.to_csv(author_type_filename, sep='\\t', index=False)\n",
    "author_type_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation with site_profile entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for coverage of sites on which authors write journals\n",
    "# this set should be empty, which indicates that we have a site_profile for every site on which users author\n",
    "set(sjournal_df.site_id) - set(user_profile_df.site_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_user_profile_df = user_profile_df[user_profile_df.site_id.isin(set(sjournal_df.site_id))]\n",
    "author_user_profile_df = author_user_profile_df.merge(sjournal_df.groupby(['user_id', 'site_id']).journal_oid.count().reset_index().rename(columns={'journal_oid': 'n_journals'}), how='left', on=['user_id', 'site_id'])\n",
    "len(author_user_profile_df), len(user_profile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_user_profile_df.role.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_user_profile_df.is_creator.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_user_profile_df.sort_values(by=['user_id', 'n_journals']).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_user_profile_df.sort_values(by=['user_id', 'n_journals']).tail(53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_list = []\n",
    "for user_id, group in author_user_profile_df.groupby('user_id'):\n",
    "    s = group.sort_values('n_journals', ascending=False).iloc[0]\n",
    "    s_list.append(s)\n",
    "#author_user_profile_df.sort_values(by=['user_id', 'n_journals']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(s_list).sort_values(by='n_journals').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(s_list).sort_values(by='n_journals').tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible_author_user_profile_df = pd.DataFrame(s_list)\n",
    "eligible_author_user_profile_df = eligible_author_user_profile_df[eligible_author_user_profile_df.n_journals >= 3]\n",
    "len(eligible_author_user_profile_df), len(set(eligible_author_user_profile_df.user_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible_participant_user_id_filepath = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant', 'eligible_participant_user_ids.txt')\n",
    "with open(eligible_participant_user_id_filepath, 'w') as outfile:\n",
    "    for user_id in set(eligible_author_user_profile_df.user_id):\n",
    "        outfile.write(str(user_id) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints = []\n",
    "interactions_dir = os.path.join(cbcore.data.paths.derived_data_filepath, 'interactions')\n",
    "for filename in ['reaction.csv', 'amps.csv', 'comment.csv', 'guestbook.csv']:\n",
    "    input_filepath = os.path.join(interactions_dir, filename)\n",
    "    with open(input_filepath, 'r') as infile:\n",
    "        for line in tqdm(infile, desc=filename):\n",
    "            # columns: user_id, site_id, interaction_type, interaction_oid, parent_type, parent_id, ancestor_type, ancestor_id, created_at, updated_at\n",
    "            tokens = line.strip().split(\",\")\n",
    "            user_id = int(tokens[0])\n",
    "            if user_id in participant_user_ids:\n",
    "                ints.append(tokens)\n",
    "len(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user_id', 'site_id', 'interaction_type', 'interaction_oid', 'parent_type', 'parent_oid', 'ancestor_type', 'ancestor_oid', 'created_at', 'updated_at']\n",
    "sints_df = pd.DataFrame(ints, columns=cols).astype({\n",
    "    'user_id': int,\n",
    "    'site_id': int,\n",
    "    'created_at': np.int64,\n",
    "    'updated_at': str,\n",
    "})\n",
    "len(sints_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sints_df.interaction_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute user int counts\n",
    "# also add users who don't interact as zeros\n",
    "user_int_counts = sints_df.user_id.value_counts()\n",
    "unmatched_users = list(participant_user_ids - set(user_int_counts.index))\n",
    "user_int_counts = user_int_counts.append(pd.Series(index=unmatched_users, data=0))\n",
    "user_int_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "bins = np.linspace(0, 1500)\n",
    "x = np.minimum(user_int_counts, 1500)\n",
    "print(np.sum(user_int_counts > 30))\n",
    "totals, _, bar_patches = ax.hist(x, bins=bins)\n",
    "manual_height = 42\n",
    "ax.set_ylim(0, manual_height)\n",
    "\n",
    "ax.set_xlabel(\"Total interactions\")\n",
    "ax.set_ylabel(\"Number of participants\")\n",
    "\n",
    "print(f\"{np.sum(x == 0) / len(x) * 100:.1f}% ({np.sum(x == 0)}) participants have no prior interactions\")\n",
    "print(f\"Median participant has {np.quantile(x, 0.5)} interactions\")\n",
    "subset_end = np.ceil(np.quantile(x, 0.5))\n",
    "print(subset_end)\n",
    "axins = ax.inset_axes([0.5, 0.5, 0.47, 0.47])\n",
    "axins.hist(x[x <= subset_end], bins=np.linspace(0, subset_end+1, 20))\n",
    "axins.text(0.75, 0.75, f\"{np.sum(x <= subset_end) / len(x) *100:.1f}%\\nin\\n[0, {int(np.ceil(subset_end))}]\", transform=axins.transAxes, ha='center', va='center')\n",
    "rec_patch, lines = ax.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "rec_patch.set_height(np.max(totals))  # correct height\n",
    "# now need to fix the line positioning\n",
    "lines[1].set_visible(False)\n",
    "line = lines[1]  # upper left corner line\n",
    "verts = line.get_path().vertices\n",
    "start_pos = verts[0,:]\n",
    "end_pos = verts[2,:]\n",
    "end_pos[1] = np.max(totals)\n",
    "new_line = matplotlib.patches.FancyArrowPatch(posA=start_pos, posB=end_pos, arrowstyle='-', linewidth=0.7)\n",
    "ax.add_patch(new_line)\n",
    "\n",
    "ax.set_yticks(np.arange(0, manual_height+1, 2))\n",
    "#ax.yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: int(x)))\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: f\">{x:.0f}\" if x == 1500 else int(x)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.strptime('2016-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.strptime('2021-07-15', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "bins.append(int(curr_time.timestamp() * 1000))\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "print(f'(actual from {datetime.utcfromtimestamp(bins[0] / 1000)} to {datetime.utcfromtimestamp(bins[-1] / 1000)})')\n",
    "\n",
    "print(f\"{np.sum(df.timestamp < bins[0])} below, {np.sum(df.timestamp > bins[-1])} above the expected time range\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "total_counts, bin_edges = np.histogram(df.timestamp, bins=bins)\n",
    "ax.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2)\n",
    "\n",
    "first_nl_entry = np.min(df.timestamp)\n",
    "ax.axvline(first_nl_entry, color='gray', linestyle='--', alpha=0.4, label=f\"First follow on {datetime.utcfromtimestamp(first_nl_entry / 1000).strftime('%Y-%m-%d')}\")\n",
    "ax.legend()\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick / 1000).strftime('%b %Y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_title(f\"Date of {len(df):,} site_profile nl entries\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.strptime('2014-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.strptime('2021-07-15', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "bins.append(int(curr_time.timestamp() * 1000))\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "print(f'(actual from {datetime.utcfromtimestamp(bins[0] / 1000)} to {datetime.utcfromtimestamp(bins[-1] / 1000)})')\n",
    "\n",
    "print(f\"{np.sum(df.timestamp < bins[0])} below, {np.sum(df.timestamp > bins[-1])} above the expected time range\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "#total_counts, bin_edges = np.histogram(df.timestamp, bins=bins)\n",
    "#ax.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2)\n",
    "\n",
    "for context in df.context.value_counts().head(10).index:\n",
    "    counts, bin_edges = np.histogram(df[df.context == context].timestamp, bins=bins)\n",
    "    ax.plot(bin_edges[:-1], counts, linestyle='-', linewidth=2, label=f'{context}')\n",
    "\n",
    "first_nl_entry = np.min(df.timestamp)\n",
    "ax.axvline(first_nl_entry, color='gray', linestyle='--', alpha=0.4, label=f\"First follow on {datetime.utcfromtimestamp(first_nl_entry / 1000).strftime('%Y-%m-%d')}\")\n",
    "legend = ax.legend(frameon=False)\n",
    "#legend.get_frame().set_alpha(0)\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick / 1000).strftime('%b %Y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_title(f\"Date of {len(df):,} site_profile nl entries\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.strptime('2014-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.strptime('2021-07-15', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "bins.append(int(curr_time.timestamp() * 1000))\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "print(f'(actual from {datetime.utcfromtimestamp(bins[0] / 1000)} to {datetime.utcfromtimestamp(bins[-1] / 1000)})')\n",
    "\n",
    "print(f\"{np.sum(df.timestamp < bins[0])} below, {np.sum(df.timestamp > bins[-1])} above the expected time range\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "ax = axes[0]\n",
    "sdf = df[df.is_subscription]\n",
    "for context in sdf.context.value_counts().head(10).index:\n",
    "    counts, bin_edges = np.histogram(sdf[sdf.context == context].timestamp, bins=bins)\n",
    "    ax.plot(bin_edges[:-1], counts, linestyle='-', linewidth=2, label=f'{context} (n={np.sum(counts):,})')\n",
    "\n",
    "first_nl_entry = np.min(df.timestamp)\n",
    "ax.axvline(first_nl_entry, color='gray', linestyle='--', alpha=0.4, label=f\"First follow on {datetime.utcfromtimestamp(first_nl_entry / 1000).strftime('%Y-%m-%d')}\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick / 1000).strftime('%b %Y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title(f\"Date of {len(sdf):,} site_profile nl subscribe entries\")\n",
    "\n",
    "ax = axes[1]\n",
    "sdf = df[~df.is_subscription]\n",
    "for context in sdf.context.value_counts().head(10).index:\n",
    "    counts, bin_edges = np.histogram(sdf[sdf.context == context].timestamp, bins=bins)\n",
    "    ax.plot(bin_edges[:-1], counts, linestyle='-', linewidth=2, label=f'{context} (n={np.sum(counts):,})')\n",
    "\n",
    "first_nl_entry = np.min(df.timestamp)\n",
    "ax.axvline(first_nl_entry, color='gray', linestyle='--', alpha=0.4, label=f\"First follow on {datetime.utcfromtimestamp(first_nl_entry / 1000).strftime('%Y-%m-%d')}\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick / 1000).strftime('%b %Y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title(f\"Date of {len(sdf):,} site_profile nl unsubscribe entries\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_profile_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_nl_entry = np.min(df.timestamp)\n",
    "sdf = site_profile_df[(site_profile_df.role == 'Removed')&(site_profile_df.created_at >= first_nl_entry)].sample(n=10, random_state=1)\n",
    "for user_id, site_id in zip(sdf.user_id, sdf.site_id):\n",
    "    tdf = df[(df.user_id == user_id)&(df.site_id == site_id)]\n",
    "    print(len(tdf))\n",
    "    if len(tdf) == 2:\n",
    "        print(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site_profile analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_time = datetime.strptime('2019-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "site_profiles = []\n",
    "input_filepath = os.path.join(cbcore.data.paths.raw_data_filepath, 'site_profile.bson.gz')\n",
    "for doc in tqdm(iterate_collection(input_filepath), desc='Processing documents', total=81769812):\n",
    "    if 'n' not in doc:\n",
    "        continue\n",
    "    elif doc['role'] == 'Visitor' and 'createdAt' in doc and doc['createdAt'] > valid_time:\n",
    "        site_profiles.append(convert_to_dict(doc))\n",
    "        if len(site_profiles) > 1000:\n",
    "            break\n",
    "len(site_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(site_profiles)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.sample(n=20, random_state=0).itertuples():\n",
    "    print(row.Index)\n",
    "    pprint(row.n)\n",
    "    if not np.all(pd.isna(row.nl)):\n",
    "        for item in row.nl:\n",
    "            pprint(convert_to_dict(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_profiles[144]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_site_follower(n):\n",
    "    \"\"\"\n",
    "    From `bi-etl/etl/dim_site_profile/dim_site_profile_tmp.ktr`:\n",
    "        // n - notifications\n",
    "        var n = json.match(/\"n\" : \\{.+\"j\" : (.+)\\].*\\}/);\n",
    "        var journalNotifications = n &amp;&amp; n[1].length > 0 ? n[1] : \"\";\n",
    "        var isSiteFollower = journalNotifications.indexOf(\"email\") > 0 ? \"Yes\" : \"No\";\n",
    "\n",
    "    \"\"\"\n",
    "    if 'j' in n:\n",
    "        if 'email' in n['j']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "site_profiles = []\n",
    "input_filepath = os.path.join(cbcore.data.paths.raw_data_filepath, 'site_profile.bson.gz')\n",
    "processed_count = 0\n",
    "batch_size = 20000000\n",
    "for doc in tqdm(iterate_collection(input_filepath), desc='Processing documents', total=81769812):\n",
    "    #site_profile = convert_to_dict(doc)\n",
    "    d = {\n",
    "        'site_profile_oid': str(doc['_id']),\n",
    "        'created_at': int(doc['createdAt'].timestamp() * 1000) if 'createdAt' in doc else 0,\n",
    "        'updated_at': int(doc['updatedAt'].timestamp() * 1000) if 'updatedAt' in doc else 0,\n",
    "        'ref_at': int(doc['refAt'].timestamp() * 1000) if 'refAt' in doc else 0,\n",
    "        'is_creator': str(doc['isCreator']) if 'isCreator' in doc else '',\n",
    "        'is_primary': str(doc['isPrimary']) if 'isPrimary' in doc else '',\n",
    "        'is_profile_deleted': str(doc['isProfileDeleted']) if 'isProfileDeleted' in doc else '',\n",
    "        'is_site_deleted': str(doc['isSiteDeleted']) if 'isSiteDeleted' in doc else '',\n",
    "        'is_stub': str(doc['isStub']) if 'isStub' in doc else '',\n",
    "        'role': str(doc['role']) if 'role' in doc else '',\n",
    "        'site_id': int(doc['siteId']) if 'siteId' in doc else -1,\n",
    "        'user_id': int(doc['userId']) if 'userId' in doc else -1,\n",
    "        'is_site_follower': get_is_site_follower(doc['n']) if 'n' in doc else False,\n",
    "    }\n",
    "    site_profiles.append(d)\n",
    "    processed_count += 1\n",
    "    if processed_count % batch_size == 0:\n",
    "        if len(site_profiles) == 0:\n",
    "            print(f\"Warning: no site profiles available after processing {processed_count} documents.\")\n",
    "            continue\n",
    "        s = datetime.now()\n",
    "        site_profile_df = pd.DataFrame(site_profiles)\n",
    "        print(f\"Created dataframe with {len(site_profile_df)} rows in {datetime.now() - s} (processed = {processed_count})\")\n",
    "        output_filepath = os.path.join(cbcore.data.paths.derived_data_filepath, 'profile', f'site_profile_{processed_count}.feather')\n",
    "        s = datetime.now()\n",
    "        site_profile_df.to_feather(output_filepath)\n",
    "        print(f\"Saved dataframe to {output_filepath} in {datetime.now() - s} (processed = {processed_count})\")\n",
    "        del site_profile_df\n",
    "        site_profiles = []\n",
    "if len(site_profiles) > 0:\n",
    "    s = datetime.now()\n",
    "    site_profile_df = pd.DataFrame(site_profiles)\n",
    "    print(f\"Created dataframe with {len(site_profile_df)} rows in {datetime.now() - s} (processed = {processed_count})\")\n",
    "    output_filepath = os.path.join(cbcore.data.paths.derived_data_filepath, 'profile', f'site_profile_{processed_count}.feather')\n",
    "    s = datetime.now()\n",
    "    site_profile_df.to_feather(output_filepath)\n",
    "    print(f\"Saved dataframe to {output_filepath} in {datetime.now() - s} (processed = {processed_count})\")\n",
    "    site_profiles = []\n",
    "len(site_profile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "dfs = []\n",
    "for filepath in glob(os.path.join(cbcore.data.paths.derived_data_filepath, 'profile', 'site_profile_*.feather')):\n",
    "    tdf = pd.read_feather(filepath)\n",
    "    print(filepath, len(tdf))\n",
    "    dfs.append(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_info_df = pd.concat(dfs, axis=0)\n",
    "len(site_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "site_info_df.sort_values(by='created_at', inplace=True)\n",
    "print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "site_info_df.reset_index(drop=True).to_feather(os.path.join(cbcore.data.paths.derived_data_filepath, 'profile', 'site_profile.feather'))\n",
    "print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze site_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the site_profile_df\n",
    "s = datetime.now()\n",
    "site_profile_filepath = os.path.join(cbcore.data.paths.derived_data_filepath, 'profile', 'site_profile.feather')\n",
    "site_profile_df = pd.read_feather(site_profile_filepath)\n",
    "print(len(site_profile_df), datetime.now() - s)\n",
    "site_profile_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.strptime('2004-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.strptime('2021-07-16', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "print(f\"{np.sum((site_profile_df.created_at < bins[0])&(site_profile_df.created_at > bins[-1])) / len(site_profile_df) * 100:.2f}% ({np.sum(site_profile_df.created_at < bins[0])} below, {np.sum(site_profile_df.created_at > bins[-1])} above) of site_profile entries lie outsite the expected time range\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "total_counts, bin_edges = np.histogram(site_profile_df.created_at, bins=bins)\n",
    "ax.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2)\n",
    "\n",
    "# start of analysis period\n",
    "ax.axvline(datetime.fromisoformat(\"2014-01-01\").timestamp() * 1000, color='gray', linestyle='--', alpha=0.4)\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick / 1000).strftime('%b %Y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_title(f\"Creation date of {len(site_profile_df):,} site_profile documents\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(site_profile_df.ref_at > 0).rename('has_ref_at').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(site_profile_df.updated_at > 0).rename('has_updated_at').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(site_profile_df.created_at > 0).rename('has_created_at').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_profile_df.is_creator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_profile_df.is_primary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_profile_df.is_profile_deleted.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_profile_df.is_site_deleted.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_profile_df.is_stub.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_profile_df.role.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(site_info_df.role, site_info_df.is_site_follower, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68% of users have a site_profile at only a single site\n",
    "vc = site_profile_df.user_id.value_counts()\n",
    "print(len(vc), len(vc) / len(site_profile_df))\n",
    "print(np.sum(vc == 1) / len(vc))\n",
    "vc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33% of sites have only a single site_profile\n",
    "vc = site_profile_df.site_id.value_counts()\n",
    "print(len(vc), len(vc) / len(site_profile_df))\n",
    "print(np.sum(vc == 1) / len(vc))\n",
    "vc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are duplicate site_id/user_id pairs in the dataframe, but very few\n",
    "vc = site_profile_df[['site_id', 'user_id']].value_counts()\n",
    "print(len(vc), len(vc) / len(site_profile_df))\n",
    "print(np.sum(vc == 1) / len(vc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~1K duplicate entries\n",
    "np.sum(vc > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_profile_df[site_profile_df.is_creator == '1'].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(site_profile_df.is_creator, site_profile_df.is_primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify no entries without site ids\n",
    "assert np.sum(site_profile_df.site_id == -1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify no entries without user ids\n",
    "assert np.sum(site_profile_df.user_id == -1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
