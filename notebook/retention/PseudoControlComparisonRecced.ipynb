{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-Control Comparison\n",
    "===\n",
    "\n",
    "Relevant Google Doc: https://docs.google.com/document/d/1_VjjJkdvUD_YsIjGMYGISpJg5CGC_mRzFgYpuBqKliA/edit?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.family'] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import bson\n",
    "from bson.codec_options import CodecOptions\n",
    "from bson.raw_bson import RawBSONDocument\n",
    "from bson import ObjectId\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import dateutil\n",
    "import pytz\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir\n",
    "\n",
    "analysis_dir = os.path.join(git_root_dir, 'analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbcore.data.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(cbcore.data.paths.raw_data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caringbridge_core_path = \"/home/lana/levon003/repos/recsys-peer-match/src\"\n",
    "sys.path.append(caringbridge_core_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading previous batch recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_data_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant')\n",
    "!wc -l {participant_data_dir}/*.ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in recommendations from previous rounds\n",
    "d = []\n",
    "for batch_id in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    participant_data_filepath = os.path.join(participant_data_dir, f'participant_rec_data_b{batch_id}.ndjson')\n",
    "    with open(participant_data_filepath, 'r') as infile:\n",
    "        for line in infile:\n",
    "            participant = json.loads(line)\n",
    "            del participant['site_scores']\n",
    "            participant['batch_id'] = batch_id\n",
    "            d.append(participant)\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = pd.DataFrame(d)\n",
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(batch_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.sse_site_list.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_recced_site_map = {}\n",
    "for participant_id, group in batch_df.groupby('participant_id'):\n",
    "    recced_site_ids = []\n",
    "    for sse_site_list in group.sse_site_list:\n",
    "        recced_site_ids.extend([site['site_id'] for site in sse_site_list])\n",
    "    assert len(recced_site_ids) == len(set(recced_site_ids)), \"Duplicate rec was given.\"\n",
    "    recced_site_ids = list(set(recced_site_ids))\n",
    "    participant_recced_site_map[participant_id] = recced_site_ids\n",
    "len(participant_recced_site_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recced_usps = [(row.participant_id, site['site_id']) for row in batch_df.itertuples() for site in row.sse_site_list]\n",
    "len(recced_usps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(recced_usps)) == len(recced_usps), \"Duplicate rec given.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rec_df\n",
    "rec_df = []\n",
    "for row in batch_df.itertuples(index=False):\n",
    "    for i, site in enumerate(row.sse_site_list):\n",
    "        rec = row._asdict()\n",
    "        del rec['sse_site_list']\n",
    "        if 'journal_body' in site:\n",
    "            # some of the data were written with different key names for cleaned_journal_{body,title}\n",
    "            # this code normalizes the key names\n",
    "            site = dict(site)\n",
    "            site['cleaned_journal_body'] = site['journal_body']\n",
    "            del site['journal_body']\n",
    "            site['cleaned_journal_title'] = site['journal_title']\n",
    "            del site['journal_title']\n",
    "        rec.update(site)\n",
    "        rec['rank'] = i\n",
    "        rec_df.append(rec)\n",
    "rec_df = pd.DataFrame(rec_df)\n",
    "len(rec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add alias for participant_id\n",
    "rec_df['user_id'] = rec_df['participant_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get participant data\n",
    "participant_id_filepath = os.path.join(git_root_dir, 'data/email/participant_ids.tsv')\n",
    "participant_df = pd.read_csv(participant_id_filepath, sep='\\t', header=0)\n",
    "print(len(participant_df))\n",
    "participant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_batch_count_map = batch_df.groupby('participant_id').batch_id.nunique().to_dict()\n",
    "participant_df['n_total_recs'] = participant_df.user_id.map(lambda user_id: participant_batch_count_map[user_id] * 5 if user_id in participant_batch_count_map else 0)\n",
    "participant_df.n_total_recs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_first_sse_map = batch_df.groupby('participant_id').sse_sent_timestamp.min()\n",
    "participant_df['first_sse_timestamp'] = participant_df.user_id.map(lambda user_id: participant_first_sse_map[user_id] if user_id in participant_first_sse_map else -1)\n",
    "participant_df.first_sse_timestamp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_user_ids = set(participant_df[participant_df.n_total_recs > 0].user_id)\n",
    "print(f\"{len(set(participant_df.user_id))} participants were matched to an email\")\n",
    "print(f\"{len(set(participant_df[participant_df.n_total_recs > 0].user_id))} participants received 1+ recommendations\")\n",
    "len(participant_user_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recced-site + pseudo-control site data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_sites_df = pd.read_csv(os.path.join(analysis_dir, \"controlSites.csv\")) \n",
    "control_site_ids = set(control_sites_df.site_id.unique())\n",
    "print(len(control_site_ids))\n",
    "\n",
    "actual_sites_df = pd.read_csv(os.path.join(analysis_dir, \"actualSites.csv\")) \n",
    "actual_site_ids = set(actual_sites_df.site_id.unique())\n",
    "print(len(actual_site_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site, Profile, Journal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the site metadata dataframe\n",
    "# this is created in caringbridge_core from the new data\n",
    "site_metadata_working_dir = \"/home/lana/shared/caringbridge/data/derived/site_metadata\"\n",
    "s = datetime.now()\n",
    "site_metadata_filepath = os.path.join(site_metadata_working_dir, \"site_metadata.feather\")\n",
    "site_info_df = pd.read_feather(site_metadata_filepath)\n",
    "assert np.sum(site_info_df.site_id.value_counts() > 1) == 0, \"Site ids are not globally unique.\"\n",
    "print(datetime.now() - s)\n",
    "len(site_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the profile data\n",
    "profile_metadata_dir = '/home/lana/shared/caringbridge/data/derived/profile'\n",
    "s = datetime.now()\n",
    "profile_df = pd.read_feather(os.path.join(profile_metadata_dir, 'profile.feather'))\n",
    "print(f\"Loaded {len(profile_df)} rows in {datetime.now() - s}.\")\n",
    "profile_df.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal metadata\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df['usp'] = [(user_id, site_id) for user_id, site_id in zip(journal_df.user_id, journal_df.site_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read interactions dataframe\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "ints_df = pd.read_feather(os.path.join(model_data_dir, 'ints_df.feather'))\n",
    "print(f\"Read {len(ints_df)} rows ({len(set(ints_df.user_id))} unique users) in {datetime.now() - s}.\")\n",
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df['usp'] = [(user_id, site_id) for user_id, site_id in zip(ints_df.user_id, ints_df.site_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the site profile diff\n",
    "# rows should be >= 37M+\n",
    "s = datetime.now()\n",
    "site_profile_diff_filepath = os.path.join(cbcore.data.paths.projects_data_dir, 'caringbridge_core', 'site_profile_diff', 'site_profile_diff.tsv')\n",
    "site_profile_diff_df = pd.read_csv(site_profile_diff_filepath, sep='\\t', header=0)\n",
    "print(f\"Read {len(site_profile_diff_df)} rows in {datetime.now() - s}.\")\n",
    "site_profile_diff_df['usp'] = [(row.user_id, row.site_id) for row in tqdm(site_profile_diff_df.itertuples(), total=len(site_profile_diff_df), desc=\"Creating USPs\")]\n",
    "site_profile_diff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also need to load the participant and non-participant site profile data\n",
    "\n",
    "nonparticipant_data_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'nonparticipant')\n",
    "with open(os.path.join(nonparticipant_data_dir, 'site_profile.pkl'), 'rb') as infile:\n",
    "    nonp_site_profiles = pickle.load(infile)\n",
    "print(len(nonp_site_profiles))\n",
    "\n",
    "with open(os.path.join(participant_data_dir, 'site_profile.pkl'), 'rb') as infile:\n",
    "    p_site_profiles = pickle.load(infile)\n",
    "print(len(p_site_profiles))\n",
    "\n",
    "site_profiles = nonp_site_profiles + p_site_profiles\n",
    "\n",
    "# create a dataframe from the site profile entires\n",
    "ds = []\n",
    "for sp in site_profiles:\n",
    "    user_id = int(sp['userId'])\n",
    "    site_id = int(sp['siteId']) if 'siteId' in sp else -1\n",
    "    # not capturing: nl\n",
    "    d = {\n",
    "        'user_id': user_id,\n",
    "        'site_id': site_id,\n",
    "        'is_creator': sp['isCreator'] if 'isCreator' in sp else None,\n",
    "        'is_primary': sp['isPrimary'] if 'isPrimary' in sp else None,\n",
    "        'role': sp['role'],\n",
    "        'is_profile_deleted': sp['isProfileDeleted'] if 'isProfileDeleted' in sp else None,\n",
    "        'is_site_deleted': sp['isSiteDeleted'] if 'isSiteDeleted' in sp else None,\n",
    "        'is_stub': sp['isStub'] if 'isStub' in sp else None,\n",
    "        'created_at': sp['createdAt'].timestamp() * 1000 if 'createdAt' in sp else 0,\n",
    "        'updated_at': sp['updatedAt'].timestamp() * 1000 if 'updatedAt' in sp else 0,\n",
    "        'n': dict(sp['n']) if 'n' in sp and sp['n'] is not None else {},\n",
    "    }\n",
    "    ds.append(d)\n",
    "\n",
    "ssite_profile_df = pd.DataFrame(ds)\n",
    "ssite_profile_df['is_recced'] = ssite_profile_df.site_id.isin(actual_site_ids)\n",
    "ssite_profile_df['is_control'] = ssite_profile_df.site_id.isin(control_site_ids)\n",
    "ssite_profile_df['usp'] = [(row.user_id, row.site_id) for row in ssite_profile_df.itertuples()]\n",
    "ssite_profile_df.sample(n=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df.is_creator.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df.is_primary.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df['is_self_author'] = (ssite_profile_df.is_creator == 1)|(ssite_profile_df.is_primary == 1)|(ssite_profile_df.role == 'Organizer')\n",
    "ssite_profile_df.is_self_author.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sjournal_df = journal_df[journal_df.user_id.isin(set(ssite_profile_df.user_id))]\n",
    "len(sjournal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_usp_set = set([(row.user_id, row.site_id) for row in sjournal_df.itertuples()])\n",
    "len(journal_usp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a small number of USPs where this user has authored a journal on that site but is not marked as an author in the site_profile record\n",
    "pd.crosstab(ssite_profile_df.is_self_author, ssite_profile_df.usp.isin(journal_usp_set).rename(\"is_journal_author\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssite_profile_df.loc[ssite_profile_df.usp.isin(journal_usp_set), 'is_self_author'] = True\n",
    "ssite_profile_df.is_self_author.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the first_visit_df for others' sites only\n",
    "first_visit_df = ssite_profile_df[~ssite_profile_df.is_self_author]\n",
    "len(first_visit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_usp_set = set(ssite_profile_df[ssite_profile_df.is_self_author].usp) | set(journal_df.usp)\n",
    "len(author_usp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_user_id_set = set(ssite_profile_df[ssite_profile_df.is_self_author].user_id) | set(journal_df.user_id)\n",
    "len(author_user_id_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author-to-author site visits\n",
    "# excludes all non-authors\n",
    "# excludes all self-visits\n",
    "site_visits = site_profile_diff_df[(site_profile_diff_df.key == 'updatedAt')&(site_profile_diff_df.user_id.isin(author_user_id_set)&(~site_profile_diff_df.usp.isin(author_usp_set)))]\n",
    "len(site_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_interactions = {\n",
    "    (row.user_id, row.site_id): [row.created_at,] for row in first_visit_df.itertuples()\n",
    "}\n",
    "len(user_site_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOLERANCE = 1000 * 60 * 60 * 7  # 7 hours, chosen so that if there's a bug with UTC (5 hours) and DST (1 hour) we still have an hour to treat them as essentially the same time\n",
    "\n",
    "n_missing_site_profiles = 0\n",
    "n_potential_missed_visits = 0\n",
    "n_empty_curr_values = 0\n",
    "for row in tqdm(site_visits.itertuples(), total=len(site_visits)):\n",
    "    usp = (row.user_id, row.site_id)\n",
    "    if usp not in user_site_interactions:\n",
    "        # these are author interactions, but the author in question is not \"eligible\" i.e. not in the participant group or the pseudo-control group\n",
    "        # the assertion below works as expected, although it requires running cells out of order\n",
    "        # assert row.user_id not in target_user_ids\n",
    "        n_missing_site_profiles += 1\n",
    "        user_site_interactions[usp] = [float(row.old_value) * 1000,]\n",
    "    visit_list = user_site_interactions[usp]\n",
    "    last_visit = float(row.old_value) * 1000\n",
    "    curr_visit = float(row.new_value) * 1000\n",
    "    assert curr_visit > 0\n",
    "    if last_visit == 0:\n",
    "        n_empty_curr_values += 1\n",
    "    elif last_visit < visit_list[-1] - TOLERANCE:\n",
    "        logging.warning(\"updatedAt's old value was before the creation date of the site_profile or before the value from the previous snapshot.\")\n",
    "        break\n",
    "    elif last_visit > visit_list[-1] + 5000:\n",
    "        n_potential_missed_visits += 1\n",
    "        visit_list.append(last_visit)\n",
    "    assert curr_visit > last_visit\n",
    "    visit_list.append(curr_visit)\n",
    "n_missing_site_profiles, n_potential_missed_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_df = pd.DataFrame([{'usp': usp, 'visit_timestamp': visit_timestamp} for usp, visit_list in user_site_interactions.items() for visit_timestamp in visit_list])\n",
    "visits_df['user_id'] = visits_df.usp.map(lambda usp: usp[0])\n",
    "visits_df['site_id'] = visits_df.usp.map(lambda usp: usp[1])\n",
    "len(visits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_df['visit_date'] = visits_df.visit_timestamp.map(lambda ts: int(datetime.utcfromtimestamp(int(ts / 1000)).strftime('%Y%m%d')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_time = pytz.timezone('US/Central')\n",
    "banner_live_time = datetime.fromisoformat('2021-08-02 12:11:00').astimezone(central_time)\n",
    "banner_end_time = datetime.fromisoformat('2021-08-23 11:59:59').astimezone(central_time)\n",
    "print(f\"Banner live: {banner_live_time}\")\n",
    "print(f\"Banner end: {banner_end_time}\")\n",
    "\n",
    "first_sse_timestamp = batch_df.sse_sent_timestamp.min()\n",
    "first_sse_time = datetime.utcfromtimestamp(first_sse_timestamp / 1000)\n",
    "print(f\"First SSE sent: {first_sse_time}\")\n",
    "\n",
    "last_sse_timestamp = batch_df.sse_sent_timestamp.max()\n",
    "last_sse_time = datetime.utcfromtimestamp(last_sse_timestamp / 1000)\n",
    "print(f\"Last SSE sent: {last_sse_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Click data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the rec_df with associated click data\n",
    "participant_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/participant'\n",
    "click_rec_df = pd.read_feather(os.path.join(participant_data_dir, 'click_rec_df.feather'))\n",
    "len(click_rec_df), click_rec_df.was_clicked.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_rec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click_rec_df = click_rec_df[[\"participant_id\", \"site_id\", \"batch_id\", \"first_click_timestamp\", \"was_clicked\"]]\n",
    "click_rec_df['was_clicked'] = click_rec_df['was_clicked'].astype(int)\n",
    "click_rec_df[click_rec_df.was_clicked == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_timestamps_df = click_rec_df[click_rec_df.was_clicked == 1].groupby('batch_id').first_click_timestamp.unique()\n",
    "clicked_timestamps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by site_id, was_clicked and first_click_timestamp = max(min(first_click_timestamp where was_clicked == 1), min(first_click_timestamp))\n",
    "click_rec_sites_df = click_rec_df.groupby('site_id').apply(lambda x: pd.Series({'batch_id': min(x.batch_id),\\\n",
    "                                                                               'first_click_timestamp': max([x.first_click_timestamp.min(), x[x.was_clicked == 1].first_click_timestamp.min()]),\\\n",
    "                                                                               'was_clicked': x.was_clicked.max()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Site First Click data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "#click_rec_df[~click_rec_df.was_clicked].first_click_timestamp = random.choice(clicked_timestamps_df[click_rec_df.batch_id])\n",
    "click_rec_sites_df.first_click_timestamp = click_rec_sites_df[['batch_id','first_click_timestamp']].apply(lambda x: x.first_click_timestamp if x.first_click_timestamp != -1000 else random.choice(clicked_timestamps_df[x.batch_id]), axis = 1)\n",
    "click_rec_sites_df.sort_values(by=['batch_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_control_sites_df = control_sites_df.groupby('site_id').apply(lambda x: pd.Series({'batch_id': min(x.first_batch),\\\n",
    "                                                                                              'first_click_timestamp': random.choice(clicked_timestamps_df[min(x.first_batch)]),\\\n",
    "                                                                                              'was_clicked': 0}))\n",
    "click_control_sites_df.sort_values(by=['batch_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By USP Click data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "#click_rec_df[~click_rec_df.was_clicked].first_click_timestamp = random.choice(clicked_timestamps_df[click_rec_df.batch_id])\n",
    "click_rec_df.first_click_timestamp = click_rec_df[['batch_id','first_click_timestamp']].apply(lambda x: x.first_click_timestamp if x.first_click_timestamp != -1000 else random.choice(clicked_timestamps_df[x.batch_id]), axis = 1)\n",
    "click_rec_df.sort_values(by=['batch_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_control_sites = pd.read_csv(os.path.join(analysis_dir, \"allControlSites.csv\")).astype(int)\n",
    "print(len(all_control_sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "all_control_sites['was_clicked'] = 0\n",
    "all_control_sites['first_click_timestamp'] = all_control_sites[['batch_id']].apply(lambda x: random.choice(clicked_timestamps_df[x.batch_id]), axis = 1)\n",
    "all_control_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_control_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click_rec_df = click_rec_df.set_index(['site_id','participant_id']) #uncomment me for preclick descriptive stats\n",
    "all_control_sites = all_control_sites.set_index(['site_id','participant_id'])\n",
    "all_control_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click_rec_df = click_rec_df.set_index(['site_id','participant_id'])\n",
    "click_rec_df = click_rec_df.set_index(['site_id','participant_id'])\n",
    "click_rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use in post pres reward analysis\n",
    "#click_rec_df = click_rec_df.rename(columns={\"journal_oid\": \"rec_journal_oid\", \"user_id\": \"rec_user_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_site_ids = actual_site_ids | control_site_ids\n",
    "len(target_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df = pd.concat([control_sites_df, actual_sites_df])\n",
    "len(sites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_sites_df = pd.concat([click_control_sites_df, click_rec_sites_df[click_rec_sites_df.was_clicked==1]])\n",
    "len(click_sites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = all_control_sites.groupby(['site_id','participant_id']).nunique()\n",
    "print(test[(test.batch_id > 1)])\n",
    "\n",
    "test = click_rec_df.groupby(['site_id','participant_id']).nunique()\n",
    "print(test[(test.batch_id > 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_control_sites.groupby(['site_id','participant_id']).nunique())\n",
    "print(all_control_sites)\n",
    "print(click_rec_df.groupby(['site_id','participant_id']).nunique())\n",
    "print(click_rec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click_rec_df = Clicked vs Non-clicked group\n",
    "print(f\"Comparison 1(click_rec_df): {len(click_rec_df)}\")\n",
    "click_control_df = pd.concat([click_rec_df[click_rec_df.was_clicked == 1], all_control_sites])\n",
    "print(f\"Comparison 2(click_control_df): {len(click_control_df)}\")\n",
    "click_rec_df[\"was_recced\"] = 1\n",
    "rec_control_df = pd.concat([click_rec_df, all_control_sites])\n",
    "rec_control_df = rec_control_df.fillna(value=0)\n",
    "print(f\"Comparison 3(rec_control_df): {len(rec_control_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trim down the available profile data\n",
    "# profile_df = profile_df[profile_df.user_id.isin(target_user_ids)].copy()\n",
    "# account_creation_time_map = {row.user_id: row.createdAt for row in profile_df.itertuples()}\n",
    "# len(profile_df), len(account_creation_time_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recced_usps = set([(row.participant_id, row.site_id) for row in rec_df.itertuples()])\n",
    "recced_sites = set(rec_df.site_id)\n",
    "len(recced_sites), len(recced_usps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modeling\n",
    "\n",
    "Useful docs: https://www.statsmodels.org/stable/api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_rec_sites_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates/Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_window = 35 * 1000 * 60 * 60 * 24\n",
    "front_window = 35 * 1000 * 60 * 60 * 24\n",
    "\n",
    "sjournal_df = click_rec_df.merge(journal_df[['site_id','published_at','user_id','journal_oid']], how='left', on='site_id')\n",
    "\n",
    "sjournal_df_pre = sjournal_df[(sjournal_df.sse_sent_timestamp - sjournal_df.published_at >= 0)&(sjournal_df.sse_sent_timestamp - sjournal_df.published_at <= back_window)]\n",
    "sjournal_df_post = sjournal_df[(sjournal_df.published_at - sjournal_df.sse_sent_timestamp >= 0)&(sjournal_df.published_at - sjournal_df.sse_sent_timestamp <= front_window)]\n",
    "\n",
    "print(sjournal_df_pre)\n",
    "n_updates_pre = sjournal_df_pre.groupby('site_id').journal_oid.nunique().rename(\"n_updates_pre\")\n",
    "n_updates_post = sjournal_df_post.groupby('site_id').journal_oid.nunique().rename(\"n_updates_post\")\n",
    "n_authors_pre = sjournal_df_pre.groupby('site_id').user_id.nunique().rename(\"n_authors_pre\")\n",
    "n_authors_post = sjournal_df_post.groupby('site_id').user_id.nunique().rename(\"n_authors_post\")\n",
    "\n",
    "\n",
    "print(n_updates_pre)\n",
    "print(n_updates_post)\n",
    "print(n_authors_pre)\n",
    "print(n_authors_post)\n",
    "# n_updates_pre = click_rec_sites_df.apply(lambda x: journal_df[(journal_df.published_at >= x.first_click_timestamp - back_window)&\n",
    "#                                                               (journal_df.published_at <= x.first_click_timestamp)&\n",
    "#                                                               (journal_df.site_id == x.name)].journal_oid.nunique(), axis = 1).rename(\"n_updates_pre\")\n",
    "\n",
    "# n_updates_post = click_rec_sites_df.apply(lambda x: journal_df[(journal_df.published_at >= x.first_click_timestamp)&\n",
    "#                                                               (journal_df.published_at <= x.first_click_timestamp + front_window)&\n",
    "#                                                               (journal_df.site_id == x.name)].journal_oid.nunique(), axis = 1).rename(\"n_updates_post\")\n",
    "# print(n_updates_pre)\n",
    "# print(n_updates_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time since first journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_since_first_journal_update = click_rec_sites_df.apply(lambda x: (x.first_click_timestamp - journal_df[(journal_df.site_id == x.name)].created_at.min()) / 1000 / 60 / 60 / 24, axis = 1).rename(\"time_since_first_journal_update\")\n",
    "\n",
    "print(time_since_first_journal_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sitewide interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_usps_pre = sints_df_pre[['user_id','site_id','interaction_oid']].merge(journal_df[['user_id','site_id']].drop_duplicates().rename(columns={'site_id': 'source_site_id'}), how='left', on='user_id')\n",
    "target_usps_post = sints_df_post[['user_id','site_id','interaction_oid']].merge(journal_df[['user_id','site_id']].drop_duplicates().rename(columns={'site_id': 'source_site_id'}), how='left', on='user_id')\n",
    "\n",
    "n_sitewide_interactionswith_pre = target_usps_pre[target_usps_pre.site_id != target_usps_pre.source_site_id]\\\n",
    "    .groupby(['source_site_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "n_sitewide_interactionswith_post = target_usps_post[target_usps_post.site_id != target_usps_post.source_site_id]\\\n",
    "    .groupby(['source_site_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "n_sitewide_interactionswith_self_pre = target_usps_pre[target_usps_pre.site_id == target_usps_pre.source_site_id]\\\n",
    ".groupby(['source_site_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "n_sitewide_interactionswith_self_post = target_usps_post[target_usps_post.site_id == target_usps_post.source_site_id]\\\n",
    ".groupby(['source_site_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "\n",
    "n_sitewide_interactions_pre = n_sitewide_interactionswith_pre.groupby('source_site_id').sum().rename(\"n_sitewide_interactions_pre\")\n",
    "n_sitewide_interactions_post = n_sitewide_interactionswith_post.groupby('source_site_id').sum().rename(\"n_sitewide_interactions_post\")\n",
    "n_sitewide_sites_intereactedwith_pre = n_sitewide_interactionswith_pre.groupby('source_site_id').count().rename(\"n_sitewide_sites_intereactedwith_pre\")\n",
    "n_sitewide_sites_intereactedwith_post = n_sitewide_interactionswith_post.groupby('source_site_id').count().rename(\"n_sitewide_sites_intereactedwith_post\")\n",
    "n_sitewide_self_interactions_pre = n_sitewide_interactionswith_self_pre.groupby('source_site_id').sum().rename(\"n_sitewide_self_interactions_pre\")\n",
    "n_sitewide_self_interactions_post = n_sitewide_interactionswith_self_post.groupby('source_site_id').sum().rename(\"n_sitewide_self_interactions_post\")\n",
    "\n",
    "print(n_sitewide_self_interactions_pre)\n",
    "print(n_sitewide_self_interactions_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_participants = True\n",
    "sints_df = click_rec_sites_df.merge(ints_df[['site_id','usp','created_at','interaction_oid','user_id']], how='left', on='site_id')\n",
    "sints_df_pre = sints_df[(sints_df.first_click_timestamp - sints_df.created_at >= 0)&(sints_df.first_click_timestamp - sints_df.created_at <= back_window)]\n",
    "sints_df_post = sints_df[(sints_df.created_at - sints_df.first_click_timestamp >= 0)&(sints_df.created_at - sints_df.first_click_timestamp  <= front_window)]\n",
    "\n",
    "\n",
    "if exclude_participants:\n",
    "    sints_df_pre = sints_df_pre[~sints_df_pre.usp.isin(recced_usps)]\n",
    "    sints_df_post = sints_df_post[~sints_df_post.usp.isin(recced_usps)]\n",
    "is_self_interaction_pre = sints_df_pre.usp.isin(author_usp_set)\n",
    "is_self_interaction_post = sints_df_post.usp.isin(author_usp_set)\n",
    "\n",
    "interactionswith_pre = sints_df_pre[~is_self_interaction_pre].groupby(['site_id','usp']).interaction_oid.nunique()\n",
    "interactionswith_post = sints_df_post[~is_self_interaction_post].groupby(['site_id','usp']).interaction_oid.nunique()\n",
    "n_interactions_pre = interactionswith_pre.groupby('site_id').sum().rename(\"n_interactions_pre\")\n",
    "n_interactions_post = interactionswith_post.groupby('site_id').sum().rename(\"n_interactions_post\")\n",
    "n_users_intereactedwith_pre = interactionswith_pre.groupby('site_id').count().rename(\"n_users_interactedwith_pre\")\n",
    "n_users_intereactedwith_post = interactionswith_post.groupby('site_id').count().rename(\"n_users_intereactedwith_post\")\n",
    "print(n_interactions_pre)\n",
    "print(n_interactions_post)\n",
    "print(n_users_intereactedwith_pre)\n",
    "print(n_users_intereactedwith_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_participants = True\n",
    "\n",
    "sfirst_vist_df = click_rec_sites_df.merge(first_visit_df[['site_id','user_id', 'usp', 'created_at']], how='left', on='site_id')\n",
    "sfirst_vist_df_pre = sfirst_vist_df[(sfirst_vist_df.first_click_timestamp - sfirst_vist_df.created_at >= 0)&(sfirst_vist_df.first_click_timestamp - sfirst_vist_df.created_at <= back_window)]\n",
    "sfirst_vist_df_post = sfirst_vist_df[(sfirst_vist_df.created_at - sfirst_vist_df.first_click_timestamp >= 0)&(sfirst_vist_df.created_at - sfirst_vist_df.first_click_timestamp <= front_window)]\n",
    "\n",
    "if exclude_participants:\n",
    "    sfirst_vist_df_pre = sfirst_vist_df_pre[~sfirst_vist_df_pre.usp.isin(recced_usps)]\n",
    "    sfirst_vist_df_post = sfirst_vist_df_post[~sfirst_vist_df_post.usp.isin(recced_usps)]\n",
    "\n",
    "n_first_visits_pre = sfirst_vist_df_pre.groupby('site_id').created_at.count().rename(\"n_first_visits_pre\")\n",
    "n_first_visits_post = sfirst_vist_df_post.groupby('site_id').created_at.count().rename(\"n_first_visits_post\")\n",
    "    \n",
    "print(n_first_visits_pre)\n",
    "print(n_first_visits_post)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat Visitsvisits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svisits_df = click_rec_sites_df.merge(visits_df, how='left', on='site_id')\n",
    "svisits_df_pre = svisits_df[(svisits_df.first_click_timestamp - svisits_df.visit_timestamp >= 0)&(svisits_df.first_click_timestamp - svisits_df.visit_timestamp <= back_window)]\n",
    "svisits_df_post = svisits_df[(svisits_df.visit_timestamp - svisits_df.first_click_timestamp >= 0)&(svisits_df.visit_timestamp - svisits_df.first_click_timestamp <= front_window)]\n",
    "\n",
    "if exclude_participants:\n",
    "    svisits_df_pre = svisits_df_pre[~svisits_df_pre.usp.isin(recced_usps)]\n",
    "    svisits_df_post = svisits_df_post[~svisits_df_post.usp.isin(recced_usps)]\n",
    "    \n",
    "\n",
    "n_days_visited_pre = svisits_df_pre.groupby('site_id').visit_date.nunique().rename(\"n_days_visited_pre\")\n",
    "n_days_visited_post = svisits_df_post.groupby('site_id').visit_date.nunique().rename(\"n_days_visited_post\")\n",
    "n_repeat_visits_pre = svisits_df_pre.groupby(['user_id', 'site_id']).visit_timestamp.count() - 1\n",
    "n_repeat_visits_post = svisits_df_post.groupby(['user_id', 'site_id']).visit_timestamp.count() - 1\n",
    "n_users_repeat_visited_pre = n_repeat_visits_pre[n_repeat_visits_pre > 0].groupby('site_id').count().rename(\"n_users_repeat_visited_pre\")\n",
    "n_users_repeat_visited_post = n_repeat_visits_post[n_repeat_visits_post > 0].groupby('site_id').count().rename(\"n_users_repeat_visited_post\")\n",
    "\n",
    "print(n_users_repeat_visited_pre)\n",
    "print(n_users_repeat_visited_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_window_features(back_window, front_window, target_sites_df, exclude_participants=True):\n",
    "    \n",
    "    sjournal_df = target_sites_df.merge(journal_df[['site_id','published_at','user_id','journal_oid']], how='left', on='site_id')\n",
    "\n",
    "    sjournal_df_pre = sjournal_df[(sjournal_df.first_click_timestamp - sjournal_df.published_at >= 0)&(sjournal_df.first_click_timestamp - sjournal_df.published_at <= back_window)]\n",
    "    sjournal_df_post = sjournal_df[(sjournal_df.published_at - sjournal_df.first_click_timestamp >= 0)&(sjournal_df.published_at - sjournal_df.first_click_timestamp <= front_window)]\n",
    "\n",
    "    n_updates_pre = sjournal_df_pre.groupby('site_id').journal_oid.nunique().rename(\"n_updates_pre\")\n",
    "    n_updates_post = sjournal_df_post.groupby('site_id').journal_oid.nunique().rename(\"n_updates_post\")\n",
    "    n_authors_pre = sjournal_df_pre.groupby('site_id').user_id.nunique().rename(\"n_authors_pre\")\n",
    "    n_authors_post = sjournal_df_post.groupby('site_id').user_id.nunique().rename(\"n_authors_post\")\n",
    "    \n",
    "    #n_authors_total = journal_df[(journal_df.published_at <= end_timestamp)].groupby('site_id').user_id.nunique().rename(\"n_authors_total\" + postfix) #Doesn't make sense for this analysis\n",
    "    \n",
    "    time_since_first_journal_update = target_sites_df.apply(lambda x: (x.first_click_timestamp - journal_df[(journal_df.site_id == x.name)].created_at.min()) / 1000 / 60 / 60 / 24, axis = 1).rename(\"time_since_first_journal_update\")\n",
    "    \n",
    "    sints_df = target_sites_df.merge(ints_df[['site_id','usp','created_at','interaction_oid','user_id']], how='left', on='site_id')\n",
    "    sints_df_pre = sints_df[(sints_df.first_click_timestamp - sints_df.created_at >= 0)&(sints_df.first_click_timestamp - sints_df.created_at <= back_window)]\n",
    "    sints_df_post = sints_df[(sints_df.created_at - sints_df.first_click_timestamp >= 0)&(sints_df.created_at - sints_df.first_click_timestamp  <= front_window)]\n",
    "\n",
    "    if exclude_participants:\n",
    "        sints_df_pre = sints_df_pre[~sints_df_pre.usp.isin(recced_usps)]\n",
    "        sints_df_post = sints_df_post[~sints_df_post.usp.isin(recced_usps)]\n",
    "    is_self_interaction_pre = sints_df_pre.usp.isin(author_usp_set)\n",
    "    is_self_interaction_post = sints_df_post.usp.isin(author_usp_set)\n",
    "\n",
    "    interactionswith_pre = sints_df_pre[~is_self_interaction_pre].groupby(['site_id','usp']).interaction_oid.nunique()\n",
    "    interactionswith_post = sints_df_post[~is_self_interaction_post].groupby(['site_id','usp']).interaction_oid.nunique()\n",
    "    n_interactions_pre = interactionswith_pre.groupby('site_id').sum().rename(\"n_interactions_pre\")\n",
    "    n_interactions_post = interactionswith_post.groupby('site_id').sum().rename(\"n_interactions_post\")\n",
    "    n_users_interactedwith_pre = interactionswith_pre.groupby('site_id').count().rename(\"n_users_interactedwith_pre\")\n",
    "    n_users_interactedwith_post = interactionswith_post.groupby('site_id').count().rename(\"n_users_interactedwith_post\")\n",
    "    \n",
    "    \n",
    "    target_usps_pre = sints_df_pre[['user_id','site_id','interaction_oid']].merge(journal_df[['user_id','site_id']].drop_duplicates().rename(columns={'site_id': 'source_site_id'}), how='left', on='user_id')\n",
    "    target_usps_post = sints_df_post[['user_id','site_id','interaction_oid']].merge(journal_df[['user_id','site_id']].drop_duplicates().rename(columns={'site_id': 'source_site_id'}), how='left', on='user_id')\n",
    "\n",
    "    n_sitewide_interactionswith_pre = target_usps_pre[target_usps_pre.site_id != target_usps_pre.source_site_id]\\\n",
    "        .groupby(['source_site_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "    n_sitewide_interactionswith_post = target_usps_post[target_usps_post.site_id != target_usps_post.source_site_id]\\\n",
    "        .groupby(['source_site_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "    n_sitewide_interactionswith_self_pre = target_usps_pre[target_usps_pre.site_id == target_usps_pre.source_site_id]\\\n",
    "        .groupby(['source_site_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "    n_sitewide_interactionswith_self_post = target_usps_post[target_usps_post.site_id == target_usps_post.source_site_id]\\\n",
    "        .groupby(['source_site_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "\n",
    "    n_sitewide_interactions_pre = n_sitewide_interactionswith_pre.groupby('source_site_id').sum().rename(\"n_sitewide_interactions_pre\")\n",
    "    n_sitewide_interactions_post = n_sitewide_interactionswith_post.groupby('source_site_id').sum().rename(\"n_sitewide_interactions_post\")\n",
    "    n_sitewide_sites_intereactedwith_pre = n_sitewide_interactionswith_pre.groupby('source_site_id').count().rename(\"n_sitewide_sites_intereactedwith_pre\")\n",
    "    n_sitewide_sites_intereactedwith_post = n_sitewide_interactionswith_post.groupby('source_site_id').count().rename(\"n_sitewide_sites_intereactedwith_post\")\n",
    "    n_sitewide_self_interactions_pre = n_sitewide_interactionswith_self_pre.groupby('source_site_id').sum().rename(\"n_sitewide_self_interactions_pre\")\n",
    "    n_sitewide_self_interactions_post = n_sitewide_interactionswith_self_post.groupby('source_site_id').sum().rename(\"n_sitewide_self_interactions_post\")\n",
    "    \n",
    "    \n",
    "    sfirst_vist_df = target_sites_df.merge(first_visit_df[['site_id','user_id', 'usp', 'created_at']], how='left', on='site_id')\n",
    "    sfirst_vist_df_pre = sfirst_vist_df[(sfirst_vist_df.first_click_timestamp - sfirst_vist_df.created_at >= 0)&(sfirst_vist_df.first_click_timestamp - sfirst_vist_df.created_at <= back_window)]\n",
    "    sfirst_vist_df_post = sfirst_vist_df[(sfirst_vist_df.created_at - sfirst_vist_df.first_click_timestamp >= 0)&(sfirst_vist_df.created_at - sfirst_vist_df.first_click_timestamp <= front_window)]\n",
    "\n",
    "    if exclude_participants:\n",
    "        sfirst_vist_df_pre = sfirst_vist_df_pre[~sfirst_vist_df_pre.usp.isin(recced_usps)]\n",
    "        sfirst_vist_df_post = sfirst_vist_df_post[~sfirst_vist_df_post.usp.isin(recced_usps)]\n",
    "\n",
    "    n_first_visits_pre = sfirst_vist_df_pre.groupby('site_id').created_at.count().rename(\"n_first_visits_pre\")\n",
    "    n_first_visits_post = sfirst_vist_df_post.groupby('site_id').created_at.count().rename(\"n_first_visits_post\")\n",
    "    \n",
    "    svisits_df = target_sites_df.merge(visits_df, how='left', on='site_id')\n",
    "    svisits_df_pre = svisits_df[(svisits_df.first_click_timestamp - svisits_df.visit_timestamp >= 0)&(svisits_df.first_click_timestamp - svisits_df.visit_timestamp <= back_window)]\n",
    "    svisits_df_post = svisits_df[(svisits_df.visit_timestamp - svisits_df.first_click_timestamp >= 0)&(svisits_df.visit_timestamp - svisits_df.first_click_timestamp <= front_window)]\n",
    "\n",
    "    if exclude_participants:\n",
    "        svisits_df_pre = svisits_df_pre[~svisits_df_pre.usp.isin(recced_usps)]\n",
    "        svisits_df_post = svisits_df_post[~svisits_df_post.usp.isin(recced_usps)]\n",
    "\n",
    "\n",
    "    n_days_visited_pre = svisits_df_pre.groupby('site_id').visit_date.nunique().rename(\"n_days_visited_pre\")\n",
    "    n_days_visited_post = svisits_df_post.groupby('site_id').visit_date.nunique().rename(\"n_days_visited_post\")\n",
    "    n_repeat_visits_pre = svisits_df_pre.groupby(['user_id', 'site_id']).visit_timestamp.count() - 1\n",
    "    n_repeat_visits_post = svisits_df_post.groupby(['user_id', 'site_id']).visit_timestamp.count() - 1\n",
    "    n_users_repeat_visited_pre = n_repeat_visits_pre[n_repeat_visits_pre > 0].groupby('site_id').count().rename(\"n_users_repeat_visited_pre\")\n",
    "    n_users_repeat_visited_post = n_repeat_visits_post[n_repeat_visits_post > 0].groupby('site_id').count().rename(\"n_users_repeat_visited_post\")\n",
    "    \n",
    "    target_sites_df = target_sites_df.join([time_since_first_journal_update,\n",
    "                  n_updates_pre,\n",
    "                  n_updates_post,\n",
    "                  n_authors_pre,\n",
    "                  n_authors_post,\n",
    "                  n_interactions_pre,\n",
    "                  n_interactions_post,\n",
    "                  n_users_interactedwith_pre,\n",
    "                  n_users_interactedwith_post,\n",
    "                  n_sitewide_interactions_pre,\n",
    "                  n_sitewide_interactions_post,\n",
    "                  n_sitewide_sites_intereactedwith_pre,\n",
    "                  n_sitewide_sites_intereactedwith_post,\n",
    "                  n_sitewide_self_interactions_pre,\n",
    "                  n_sitewide_self_interactions_post,\n",
    "                  n_first_visits_pre,\n",
    "                  n_first_visits_post,\n",
    "                  n_days_visited_pre,\n",
    "                  n_days_visited_post,\n",
    "                  n_users_repeat_visited_pre,\n",
    "                  n_users_repeat_visited_post\n",
    "    ])\n",
    "    \n",
    "    target_sites_df = target_sites_df.fillna(value=0)\n",
    "\n",
    "    return target_sites_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_preclick_features(back_window, target_sites_df, exclude_participants=True):\n",
    "    \n",
    "    sjournal_df = target_sites_df.reset_index(level='participant_id').merge(journal_df[['site_id','published_at','user_id','journal_oid']], how='left', on='site_id')\n",
    "    sjournal_df_pre = sjournal_df[(sjournal_df.first_click_timestamp - sjournal_df.published_at >= 0)&(sjournal_df.first_click_timestamp - sjournal_df.published_at <= back_window)]\n",
    "    n_updates_pre = sjournal_df_pre.groupby(['site_id','participant_id']).journal_oid.nunique().rename(\"n_updates_pre\")\n",
    "    n_authors_pre = sjournal_df_pre.groupby(['site_id','participant_id']).user_id.nunique().rename(\"n_authors_pre\")\n",
    "    \n",
    "    sjournal_df_total = sjournal_df[(sjournal_df.first_click_timestamp - sjournal_df.published_at >= 0)]\n",
    "    n_authors_total = sjournal_df_total.groupby(['site_id','participant_id']).user_id.nunique().rename(\"n_authors_total\")\n",
    "    \n",
    "    time_since_first_journal_update = target_sites_df.apply(lambda x: (x.first_click_timestamp - journal_df[(journal_df.site_id == x.name[0])].created_at.min()) / 1000 / 60 / 60 / 24, axis = 1).rename(\"time_since_first_journal_update\")\n",
    "    \n",
    "    sints_df = target_sites_df.reset_index(level='participant_id').merge(ints_df[['site_id','usp','created_at','interaction_oid','user_id']], how='left', on='site_id')\n",
    "    sints_df_pre = sints_df[(sints_df.first_click_timestamp - sints_df.created_at >= 0)&(sints_df.first_click_timestamp - sints_df.created_at <= back_window)]\n",
    "\n",
    "    if exclude_participants:\n",
    "        sints_df_pre = sints_df_pre[~sints_df_pre.usp.isin(recced_usps)]\n",
    "    is_self_interaction_pre = sints_df_pre.usp.isin(author_usp_set)\n",
    "    \n",
    "    interactionswith_pre = sints_df_pre[~is_self_interaction_pre].groupby(['site_id','participant_id','usp']).interaction_oid.nunique()\n",
    "    n_interactions_pre = interactionswith_pre.groupby(['site_id','participant_id']).sum().rename(\"n_interactions_pre\")\n",
    "    n_users_interactedwith_pre = interactionswith_pre.groupby(['site_id','participant_id']).count().rename(\"n_users_interactedwith_pre\")\n",
    "    \n",
    "    \n",
    "    target_usps_pre = sints_df_pre[['user_id','site_id','interaction_oid','participant_id']].merge(journal_df[['user_id','site_id']].drop_duplicates().rename(columns={'site_id': 'source_site_id'}), how='left', on='user_id')\n",
    "\n",
    "    n_sitewide_interactionswith_pre = target_usps_pre[target_usps_pre.site_id != target_usps_pre.source_site_id]\\\n",
    "        .groupby(['source_site_id','participant_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "    n_sitewide_interactionswith_self_pre = target_usps_pre[target_usps_pre.site_id == target_usps_pre.source_site_id]\\\n",
    "        .groupby(['source_site_id','participant_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "\n",
    "    n_sitewide_interactions_pre = n_sitewide_interactionswith_pre.groupby(['source_site_id','participant_id']).sum().rename(\"n_sitewide_interactions_pre\")\n",
    "    n_sitewide_sites_intereactedwith_pre = n_sitewide_interactionswith_pre.groupby(['source_site_id','participant_id']).count().rename(\"n_sitewide_sites_intereactedwith_pre\")\n",
    "    n_sitewide_self_interactions_pre = n_sitewide_interactionswith_self_pre.groupby(['source_site_id','participant_id']).sum().rename(\"n_sitewide_self_interactions_pre\")\n",
    "    \n",
    "    sfirst_vist_df = target_sites_df.reset_index(level='participant_id').merge(first_visit_df[['site_id','user_id', 'usp', 'created_at']], how='left', on='site_id')\n",
    "    sfirst_vist_df_pre = sfirst_vist_df[(sfirst_vist_df.first_click_timestamp - sfirst_vist_df.created_at >= 0)&(sfirst_vist_df.first_click_timestamp - sfirst_vist_df.created_at <= back_window)]\n",
    "\n",
    "    if exclude_participants:\n",
    "        sfirst_vist_df_pre = sfirst_vist_df_pre[~sfirst_vist_df_pre.usp.isin(recced_usps)]\n",
    "\n",
    "    n_first_visits_pre = sfirst_vist_df_pre.groupby(['site_id','participant_id']).created_at.count().rename(\"n_first_visits_pre\")\n",
    "    \n",
    "    svisits_df = target_sites_df.reset_index(level='participant_id').merge(visits_df, how='left', on='site_id')\n",
    "    svisits_df_pre = svisits_df[(svisits_df.first_click_timestamp - svisits_df.visit_timestamp >= 0)&(svisits_df.first_click_timestamp - svisits_df.visit_timestamp <= back_window)]\n",
    "\n",
    "    if exclude_participants:\n",
    "        svisits_df_pre = svisits_df_pre[~svisits_df_pre.usp.isin(recced_usps)]\n",
    "\n",
    "    n_days_visited_pre = svisits_df_pre.groupby(['site_id','participant_id']).visit_date.nunique().rename(\"n_days_visited_pre\")\n",
    "    n_repeat_visits_pre = svisits_df_pre.groupby(['user_id', 'site_id','participant_id']).visit_timestamp.count() - 1\n",
    "    n_users_repeat_visited_pre = n_repeat_visits_pre[n_repeat_visits_pre > 0].groupby(['site_id','participant_id']).count().rename(\"n_users_repeat_visited_pre\")\n",
    "    \n",
    "    print(target_sites_df)\n",
    "    print(len(time_since_first_journal_update))\n",
    "    print(len(n_updates_pre))\n",
    "    print(len(n_authors_total))\n",
    "    print(len(n_interactions_pre))\n",
    "    print(len(n_users_interactedwith_pre))\n",
    "    print(len(n_sitewide_interactions_pre))\n",
    "    print(len(n_sitewide_sites_intereactedwith_pre))\n",
    "    print(len(n_sitewide_self_interactions_pre))\n",
    "    print(len(n_first_visits_pre))\n",
    "    print(len(n_days_visited_pre))\n",
    "    print(len(n_users_repeat_visited_pre))\n",
    "    target_sites_df = target_sites_df.join([time_since_first_journal_update,\n",
    "                                            n_updates_pre,\n",
    "                                            n_authors_total,\n",
    "                                            n_authors_pre,\n",
    "                                            n_interactions_pre,\n",
    "                                            n_users_interactedwith_pre,\n",
    "                                            n_sitewide_interactions_pre,\n",
    "                                            n_sitewide_sites_intereactedwith_pre,\n",
    "                                            n_sitewide_self_interactions_pre,\n",
    "                                            n_first_visits_pre,\n",
    "                                            n_days_visited_pre,\n",
    "                                            n_users_repeat_visited_pre\n",
    "    ])\n",
    "    \n",
    "    target_sites_df = target_sites_df.fillna(value=0)\n",
    "\n",
    "    return target_sites_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prerec_features(back_window, target_sites_df, exclude_participants=True):\n",
    "    \n",
    "    sjournal_df = target_sites_df.reset_index(level='participant_id').merge(journal_df[['site_id','published_at','user_id','journal_oid']], how='left', on='site_id')\n",
    "    sjournal_df_pre = sjournal_df[(sjournal_df.sse_sent_timestamp - sjournal_df.published_at >= 0)&(sjournal_df.sse_sent_timestamp - sjournal_df.published_at <= back_window)]\n",
    "    n_updates_pre = sjournal_df_pre.groupby(['site_id','participant_id']).journal_oid.nunique().rename(\"n_updates_pre\")\n",
    "    n_authors_pre = sjournal_df_pre.groupby(['site_id','participant_id']).user_id.nunique().rename(\"n_authors_pre\")\n",
    "    \n",
    "    sjournal_df_total = sjournal_df[(sjournal_df.sse_sent_timestamp - sjournal_df.published_at >= 0)]\n",
    "    n_updates_total = sjournal_df_pre.groupby(['site_id','participant_id']).journal_oid.nunique().rename(\"n_updates_total\")\n",
    "    n_authors_total = sjournal_df_total.groupby(['site_id','participant_id']).user_id.nunique().rename(\"n_authors_total\")\n",
    "    \n",
    "    time_since_first_journal_update = target_sites_df.apply(lambda x: (x.sse_sent_timestamp - journal_df[(journal_df.site_id == x.name[0])].created_at.min()) / 1000 / 60 / 60 / 24, axis = 1).rename(\"time_since_first_journal_update\")\n",
    "    \n",
    "    sints_df = target_sites_df.reset_index(level='participant_id').merge(ints_df[['site_id','usp','created_at','interaction_oid','user_id']], how='left', on='site_id')\n",
    "    sints_df_pre = sints_df[(sints_df.sse_sent_timestamp - sints_df.created_at >= 0)&(sints_df.sse_sent_timestamp - sints_df.created_at <= back_window)]\n",
    "    sints_df_total = sints_df[(sints_df.sse_sent_timestamp - sints_df.created_at >= 0)]\n",
    "\n",
    "    if exclude_participants:\n",
    "        sints_df_pre = sints_df_pre[~sints_df_pre.usp.isin(recced_usps)]\n",
    "        sints_df_total = sints_df_total[~sints_df_total.usp.isin(recced_usps)]\n",
    "    is_self_interaction_pre = sints_df_pre.usp.isin(author_usp_set)\n",
    "    is_self_interaction_total = sints_df_total.usp.isin(author_usp_set)\n",
    "    \n",
    "    interactionswith_pre = sints_df_pre[~is_self_interaction_pre].groupby(['site_id','participant_id','usp']).interaction_oid.nunique()\n",
    "    interactionswith_total = sints_df_total[~is_self_interaction_total].groupby(['site_id','participant_id','usp']).interaction_oid.nunique()\n",
    "    n_interactions_pre = interactionswith_pre.groupby(['site_id','participant_id']).sum().rename(\"n_interactions_pre\")\n",
    "    n_interactions_total = interactionswith_total.groupby(['site_id','participant_id']).sum().rename(\"n_interactions_total\")\n",
    "    n_users_interactedwith_pre = interactionswith_pre.groupby(['site_id','participant_id']).count().rename(\"n_users_interactedwith_pre\")\n",
    "    n_users_interactedwith_total = interactionswith_total.groupby(['site_id','participant_id']).count().rename(\"n_users_interactedwith_total\")\n",
    "    \n",
    "    \n",
    "    target_usps_pre = sints_df_pre[['user_id','site_id','interaction_oid','participant_id']].merge(journal_df[['user_id','site_id']].drop_duplicates().rename(columns={'site_id': 'source_site_id'}), how='left', on='user_id')\n",
    "    target_usps_total = sints_df_total[['user_id','site_id','interaction_oid','participant_id']].merge(journal_df[['user_id','site_id']].drop_duplicates().rename(columns={'site_id': 'source_site_id'}), how='left', on='user_id')\n",
    "\n",
    "    n_sitewide_interactionswith_pre = target_usps_pre[target_usps_pre.site_id != target_usps_pre.source_site_id]\\\n",
    "        .groupby(['source_site_id','participant_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "    n_sitewide_interactionswith_total = target_usps_total[target_usps_total.site_id != target_usps_total.source_site_id]\\\n",
    "        .groupby(['source_site_id','participant_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "    n_sitewide_interactionswith_self_pre = target_usps_pre[target_usps_pre.site_id == target_usps_pre.source_site_id]\\\n",
    "        .groupby(['source_site_id','participant_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "    n_sitewide_interactionswith_self_total = target_usps_total[target_usps_total.site_id == target_usps_total.source_site_id]\\\n",
    "        .groupby(['source_site_id','participant_id', 'user_id', 'site_id']).interaction_oid.nunique()\n",
    "\n",
    "    n_sitewide_interactions_pre = n_sitewide_interactionswith_pre.groupby(['source_site_id','participant_id']).sum().rename(\"n_sitewide_interactions_pre\")\n",
    "    n_sitewide_interactions_total = n_sitewide_interactionswith_total.groupby(['source_site_id','participant_id']).sum().rename(\"n_sitewide_interactions_total\")\n",
    "    n_sitewide_sites_intereactedwith_pre = n_sitewide_interactionswith_pre.groupby(['source_site_id','participant_id']).count().rename(\"n_sitewide_sites_intereactedwith_pre\")\n",
    "    n_sitewide_sites_intereactedwith_total = n_sitewide_interactionswith_total.groupby(['source_site_id','participant_id']).count().rename(\"n_sitewide_sites_intereactedwith_total\")\n",
    "    n_sitewide_self_interactions_pre = n_sitewide_interactionswith_self_pre.groupby(['source_site_id','participant_id']).sum().rename(\"n_sitewide_self_interactions_pre\")\n",
    "    n_sitewide_self_interactions_total = n_sitewide_interactionswith_self_total.groupby(['source_site_id','participant_id']).sum().rename(\"n_sitewide_self_interactions_total\")\n",
    "    \n",
    "    sfirst_vist_df = target_sites_df.reset_index(level='participant_id').merge(first_visit_df[['site_id','user_id', 'usp', 'created_at']], how='left', on='site_id')\n",
    "    sfirst_vist_df_pre = sfirst_vist_df[(sfirst_vist_df.sse_sent_timestamp - sfirst_vist_df.created_at >= 0)&(sfirst_vist_df.sse_sent_timestamp - sfirst_vist_df.created_at <= back_window)]\n",
    "    sfirst_vist_df_total = sfirst_vist_df[(sfirst_vist_df.sse_sent_timestamp - sfirst_vist_df.created_at >= 0)]\n",
    "\n",
    "    if exclude_participants:\n",
    "        sfirst_vist_df_pre = sfirst_vist_df_pre[~sfirst_vist_df_pre.usp.isin(recced_usps)]\n",
    "        sfirst_vist_df_total = sfirst_vist_df_total[~sfirst_vist_df_total.usp.isin(recced_usps)]\n",
    "\n",
    "    n_first_visits_pre = sfirst_vist_df_pre.groupby(['site_id','participant_id']).created_at.count().rename(\"n_first_visits_pre\")\n",
    "    n_first_visits_total = sfirst_vist_df_total.groupby(['site_id','participant_id']).created_at.count().rename(\"n_first_visits_total\")\n",
    "    \n",
    "    svisits_df = target_sites_df.reset_index(level='participant_id').merge(visits_df, how='left', on='site_id')\n",
    "    svisits_df_pre = svisits_df[(svisits_df.sse_sent_timestamp - svisits_df.visit_timestamp >= 0)&(svisits_df.sse_sent_timestamp - svisits_df.visit_timestamp <= back_window)]\n",
    "    svisits_df_total = svisits_df[(svisits_df.sse_sent_timestamp - svisits_df.visit_timestamp >= 0)]\n",
    "\n",
    "    if exclude_participants:\n",
    "        svisits_df_pre = svisits_df_pre[~svisits_df_pre.usp.isin(recced_usps)]\n",
    "        svisits_df_total = svisits_df_total[~svisits_df_total.usp.isin(recced_usps)]\n",
    "\n",
    "    n_days_visited_pre = svisits_df_pre.groupby(['site_id','participant_id']).visit_date.nunique().rename(\"n_days_visited_pre\")\n",
    "    n_days_visited_total = svisits_df_total.groupby(['site_id','participant_id']).visit_date.nunique().rename(\"n_days_visited_total\")\n",
    "    n_repeat_visits_pre = svisits_df_pre.groupby(['user_id', 'site_id','participant_id']).visit_timestamp.count() - 1\n",
    "    n_repeat_visits_total = svisits_df_total.groupby(['user_id', 'site_id','participant_id']).visit_timestamp.count() - 1\n",
    "    n_users_repeat_visited_pre = n_repeat_visits_pre[n_repeat_visits_pre > 0].groupby(['site_id','participant_id']).count().rename(\"n_users_repeat_visited_pre\")\n",
    "    n_users_repeat_visited_total = n_repeat_visits_total[n_repeat_visits_total > 0].groupby(['site_id','participant_id']).count().rename(\"n_users_repeat_visited_total\")\n",
    "    \n",
    "    target_sites_df = target_sites_df.join([time_since_first_journal_update,\n",
    "                                            n_updates_pre,\n",
    "                                            n_authors_pre,\n",
    "                                            n_interactions_pre,\n",
    "                                            n_users_interactedwith_pre,\n",
    "                                            n_sitewide_interactions_pre,\n",
    "                                            n_sitewide_sites_intereactedwith_pre,\n",
    "                                            n_sitewide_self_interactions_pre,\n",
    "                                            n_first_visits_pre,\n",
    "                                            n_days_visited_pre,\n",
    "                                            n_users_repeat_visited_pre,\n",
    "                                            n_updates_total,\n",
    "                                            n_authors_total,\n",
    "                                            n_interactions_total,\n",
    "                                            n_users_interactedwith_total,\n",
    "                                            n_sitewide_interactions_total,\n",
    "                                            n_sitewide_sites_intereactedwith_total,\n",
    "                                            n_sitewide_self_interactions_total,\n",
    "                                            n_first_visits_total,\n",
    "                                            n_days_visited_total,\n",
    "                                            n_users_repeat_visited_total\n",
    "    ])\n",
    "    \n",
    "    target_sites_df = target_sites_df.fillna(value=0)\n",
    "\n",
    "    return target_sites_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_control_df[['first_click_timestamp', 'was_clicked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sjournal_df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = 1000 * 60 * 60 * 24\n",
    "\n",
    "pre_rec_df = click_rec_df\n",
    "\n",
    "pre_rec_total_df = compute_prerec_features(35 * one_day, pre_rec_df)\n",
    "pre_rec_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_rec_total_df = pre_rec_total_df.reset_index()\n",
    "pre_rec_total_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35 days back\n",
    "pre_rec_total_df.to_feather(\"pre_rec_total_df_20220608.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison 1(click_rec_df): 4190\n",
    "# Comparison 2(click_control_df): 4410\n",
    "# Comparison 3(rec_control_df): 8380\n",
    "\n",
    "one_day = 1000 * 60 * 60 * 24\n",
    "\n",
    "click_rec_target_site_df = click_rec_df[['first_click_timestamp', 'was_clicked']] # Recced group, clicked vs non-clicked\n",
    "\n",
    "click_rec_total_df = compute_preclick_features(35 * one_day, click_rec_target_site_df)\n",
    "\n",
    "\n",
    "click_control_target_site_df = click_control_df[['first_click_timestamp', 'was_clicked']] # clicked vs control\n",
    "\n",
    "click_control_total_df = compute_preclick_features(35 * one_day, click_control_target_site_df)\n",
    "\n",
    "\n",
    "rec_control_target_site_df = rec_control_df[['first_click_timestamp', 'was_recced']] # rec vs control\n",
    "\n",
    "rec_control_total_df = compute_preclick_features(35 * one_day, rec_control_target_site_df)\n",
    "\n",
    "\n",
    "\n",
    "# target_site_df = click_sites_df # Clicked vs non-recced psuedo control\n",
    "\n",
    "# total_df = compute_window_features(start_timestamp, end_timestamp, target_site_df, \"_preclick\")\n",
    "# len(total_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_rec_df[click_rec_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_rec_df[~(click_rec_df.was_clicked == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_df['average_daily_updates'] = total_df.n_updates / total_df.time_since_first_journal_update\n",
    "#total_df['is_participant'] = total_df.index.isin(participant_user_ids).astype(int)\n",
    "print(total_df.was_clicked.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_rec_total_df.groupby('was_clicked').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_control_total_df.groupby('was_clicked').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_control_total_df.groupby('was_recced').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.groupby('was_clicked').agg(['median', 'mean', 'std', 'min', 'max']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretty_name_map = {\n",
    "    'time_since_first_journal_update': \"Site tenure (days)\",\n",
    "    'n_updates_pre': \"Journal updates\",\n",
    "    'n_authors_pre': \"\\# of authors\",\n",
    "    'n_authors_total': \"Total \\# of authors\",\n",
    "    'n_first_visits_pre': \"Peer visits\",\n",
    "    'n_users_repeat_visited_pre': \"Repeat user visits\",\n",
    "    'n_users_interactedwith_pre': \"Peer initiations\", \n",
    "    'n_interactions_pre': \"Peer interactions\", \n",
    "    'n_days_visited_pre': \"\\# days visiting peers\",\n",
    "    'n_sitewide_interactions_pre': \"Site author interactions\",\n",
    "    'n_sitewide_sites_intereactedwith_pre': \"Site author initiations\",\n",
    "    'n_sitewide_self_interactions_pre': \"Site author self interactions\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = pretty_name_map.keys()\n",
    "print(len(click_rec_total_df.loc[click_rec_total_df.was_clicked == 1]))\n",
    "print(len(click_rec_total_df.loc[click_rec_total_df.was_clicked == 0]))\n",
    "for col in cols:\n",
    "    t = click_rec_total_df.loc[click_rec_total_df.was_clicked == 1, col]\n",
    "    c = click_rec_total_df.loc[click_rec_total_df.was_clicked == 0, col]\n",
    "    if col != \"time_since_first_journal_update\" and col != \"n_days_visited_pre\" and col != \"n_authors_total\" and col != \"n_authors_pre\":\n",
    "        t = t / 35 * 7\n",
    "        c = c / 35 * 7\n",
    "    \n",
    "    tstat, p = scipy.stats.ttest_ind(t, c, equal_var=False)\n",
    "    diff = t.mean() - c.mean()\n",
    "    #p *= len(cols)  # bonferroni correction\n",
    "    \n",
    "    ustat, up = scipy.stats.mannwhitneyu(t, c)\n",
    "    #up *= len(cols)\n",
    "    \n",
    "    threshold = 0.005\n",
    "    \n",
    "    print(f\"{pretty_name_map[col]:>25} & {t.median():.0f} & {t.mean():.1f} ({t.std():.1f}) & {c.median():.0f} & {c.mean():.1f} ({c.std():.1f}) & {diff:.1f}{'*' if p < threshold else ''} & {ustat / (len(t)*len(c)) * 100:.1f}\\\\%{'*' if up < threshold else ''} \\\\\\\\\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pretty_name_map.keys()\n",
    "print(len(click_control_total_df.loc[click_control_total_df.was_clicked == 1]))\n",
    "print(len(click_control_total_df.loc[click_control_total_df.was_clicked == 0]))\n",
    "for col in cols:\n",
    "    t = click_control_total_df.loc[click_control_total_df.was_clicked == 1, col]\n",
    "    c = click_control_total_df.loc[click_control_total_df.was_clicked == 0, col]\n",
    "    if col != \"time_since_first_journal_update\" and col != \"n_days_visited_pre\" and col != \"n_authors_total\" and col != \"n_authors_pre\":\n",
    "        t = t / 35 * 7\n",
    "        c = c / 35 * 7\n",
    "    \n",
    "    tstat, p = scipy.stats.ttest_ind(t, c, equal_var=False)\n",
    "    diff = t.mean() - c.mean()\n",
    "    #p *= len(cols)  # bonferroni correction\n",
    "    \n",
    "    ustat, up = scipy.stats.mannwhitneyu(t, c)\n",
    "    #up *= len(cols)\n",
    "    \n",
    "    threshold = 0.005\n",
    "    \n",
    "    print(f\"{pretty_name_map[col]:>25} & {t.median():.0f} & {t.mean():.1f} ({t.std():.1f}) & {c.median():.0f} & {c.mean():.1f} ({c.std():.1f}) & {diff:.1f}{'*' if p < threshold else ''} & {ustat / (len(t)*len(c)) * 100:.1f}\\\\%{'*' if up < threshold else ''} \\\\\\\\\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pretty_name_map.keys()\n",
    "print(len(rec_control_total_df.loc[rec_control_total_df.was_recced == 1]))\n",
    "print(len(rec_control_total_df.loc[rec_control_total_df.was_recced == 0]))\n",
    "for col in cols:\n",
    "    t = rec_control_total_df.loc[rec_control_total_df.was_recced == 1, col]\n",
    "    c = rec_control_total_df.loc[rec_control_total_df.was_recced == 0, col]\n",
    "    if col != \"time_since_first_journal_update\" and col != \"n_days_visited_pre\" and col != \"n_authors_total\" and col != \"n_authors_pre\":\n",
    "        t = t / 35 * 7\n",
    "        c = c / 35 * 7\n",
    "    \n",
    "    tstat, p = scipy.stats.ttest_ind(t, c, equal_var=False)\n",
    "    diff = t.mean() - c.mean()\n",
    "    #p *= len(cols)  # bonferroni correction\n",
    "    \n",
    "    ustat, up = scipy.stats.mannwhitneyu(t, c)\n",
    "    #up *= len(cols)\n",
    "    \n",
    "    threshold = 0.005\n",
    "    \n",
    "    print(f\"{pretty_name_map[col]:>25} & {t.median():.0f} & {t.mean():.1f} ({t.std():.1f}) & {c.median():.0f} & {c.mean():.1f} ({c.std():.1f}) & {diff:.1f}{'*' if p < threshold else ''} & {ustat / (len(t)*len(c)) * 100:.1f}\\\\%{'*' if up < threshold else ''} \\\\\\\\\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pretty_name_map.keys()\n",
    "for col in cols:\n",
    "    t = total_df.loc[total_df.was_clicked == 1, col]\n",
    "    c = total_df.loc[(total_df.is_recced == 1)&(total_df.was_clicked == 0), col]\n",
    "    \n",
    "    tstat, p = scipy.stats.ttest_ind(t, c, equal_var=False)\n",
    "    diff = t.mean() - c.mean()\n",
    "    #p *= len(cols)  # bonferroni correction\n",
    "    \n",
    "    ustat, up = scipy.stats.mannwhitneyu(t, c)\n",
    "    #up *= len(cols)\n",
    "    \n",
    "    threshold = 0.005\n",
    "    \n",
    "    print(f\"{pretty_name_map[col]:>25} & {t.median():.0f} & {t.mean():.1f} ({t.std():.1f}) & {c.median():.0f} & {c.mean():.1f} ({c.std():.1f}) & {diff:.1f}{'*' if p < threshold else ''} & {up:.0e}{'*' if up < threshold else ''} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make little histograms\n",
    "# inspired from: https://github.com/levon003/icwsm-cancer-journeys/blob/master/identify_candidate_sites/ClassificationCandidateSites.ipynb\n",
    "\n",
    "cols = pretty_name_map.keys()\n",
    "for col in cols:\n",
    "    t = total_df.loc[total_df.was_clicked == 1, col]\n",
    "    c = total_df.loc[total_df.was_clicked == 0, col]\n",
    "    \n",
    "    d = t\n",
    "    fig, ax = plt.subplots(figsize=(2, 1), squeeze=True)\n",
    "    nunique = d[d < np.quantile(d, 0.9)].nunique()\n",
    "    if nunique < 30:\n",
    "        bins = np.arange(0, 30)\n",
    "        p = d\n",
    "    else:\n",
    "        bins=30\n",
    "        p = d[d < np.quantile(d, 0.9)]\n",
    "    _, bins, _ = ax.hist(p, bins=bins, align=\"left\", color=\"black\", density=True)\n",
    "    ax.hist(c, bins=bins, align=\"left\", color=\"gray\", alpha=0.9, density=True)\n",
    "    plt.tight_layout()\n",
    "    print(col, nunique)\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.margins(0,0)\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    \n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.subplots_adjust(top = 0.4, bottom = 0, right = 1, left = 0, \n",
    "                hspace = 0, wspace = 0)\n",
    "\n",
    "    bbox = matplotlib.transforms.Bbox.from_bounds(0,0,1,0.2)\n",
    "    #image_shortfilename = f\"{col}_hist_small.pdf\"\n",
    "    #image_filename = os.path.join(figures_dir, image_shortfilename)\n",
    "    #plt.savefig(image_filename, format='pdf', dpi=200, pad_inches=0, bbox_inches=bbox) #, transparent=True)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make little histograms\n",
    "# inspired from: https://github.com/levon003/icwsm-cancer-journeys/blob/master/identify_candidate_sites/ClassificationCandidateSites.ipynb\n",
    "\n",
    "cols = pretty_name_map.keys()\n",
    "for col in cols:\n",
    "    t = total_df.loc[total_df.is_recced == 1, col]\n",
    "    c = total_df.loc[total_df.is_recced == 0, col]\n",
    "    \n",
    "    d = t\n",
    "    fig, ax = plt.subplots(figsize=(2, 1), squeeze=True)\n",
    "    nunique = d[d < np.quantile(d, 0.9)].nunique()\n",
    "    if nunique < 30:\n",
    "        bins = np.arange(0, 30)\n",
    "        p = d\n",
    "    else:\n",
    "        bins=30\n",
    "        p = d[d < np.quantile(d, 0.9)]\n",
    "    _, bins, _ = ax.hist(p, bins=bins, align=\"left\", color=\"black\", density=True)\n",
    "    ax.hist(c, bins=bins, align=\"left\", color=\"gray\", alpha=0.9, density=True)\n",
    "    plt.tight_layout()\n",
    "    print(col, nunique)\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.margins(0,0)\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    \n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.subplots_adjust(top = 0.4, bottom = 0, right = 1, left = 0, \n",
    "                hspace = 0, wspace = 0)\n",
    "\n",
    "    bbox = matplotlib.transforms.Bbox.from_bounds(0,0,1,0.2)\n",
    "    #image_shortfilename = f\"{col}_hist_small.pdf\"\n",
    "    #image_filename = os.path.join(figures_dir, image_shortfilename)\n",
    "    #plt.savefig(image_filename, format='pdf', dpi=200, pad_inches=0, bbox_inches=bbox) #, transparent=True)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.loc[:,['time_since_first_journal_update']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = pd.plotting.scatter_matrix(total_df.loc[:,['time_since_first_journal_update']], alpha=0.3)\n",
    "#for ax in axes.flatten():\n",
    "#    ax.set_yscale('log')\n",
    "#    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.logit(formula=\"is_participant ~ n_updates + n_first_visits + n_interactions + np.log(time_since_first_journal_update)\", data=total_df)\n",
    "res = model.fit(disp=0)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre- vs Post- modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the correlation matrix\n",
    "corr = total_df.corr()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ms = ax.matshow(corr)\n",
    "\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(corr.shape[1]):\n",
    "        ax.text(i, j, f\"{corr.iloc[i, j]:.2f}\", ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.xticks(range(total_df.select_dtypes(['number']).shape[1]), total_df.select_dtypes(['number']).columns, fontsize=8, rotation=15, ha='left')\n",
    "plt.yticks(range(total_df.select_dtypes(['number']).shape[1]), total_df.select_dtypes(['number']).columns, fontsize=8)\n",
    "cb = fig.colorbar(ms, ax=ax, shrink=0.9)\n",
    "cb.ax.tick_params(labelsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of zero-counts...\n",
    "(df == 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stddev is larger than means for all variables, which suggests over-dispersion\n",
    "# https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/\n",
    "df.groupby('is_recced').agg(['mean', 'std', 'min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'n_updates_post' \n",
    "formula = outcome + \"\"\"\n",
    "        ~ was_clicked +\n",
    "        np.log(time_since_first_journal_update) +\n",
    "        n_updates_pre + \n",
    "        n_authors_pre +\n",
    "        n_interactions_pre +\n",
    "        n_users_interactedwith_pre + \n",
    "        n_sitewide_interactions_pre +\n",
    "        n_sitewide_self_interactions_pre +\n",
    "        n_sitewide_sites_intereactedwith_pre +\n",
    "        n_first_visits_pre +\n",
    "        n_days_visited_pre +\n",
    "        n_users_repeat_visited_pre\n",
    "    \"\"\"\n",
    "    \n",
    "# basic regression estimates\n",
    "# that \"adjust for\" confounders\n",
    "# plus standardization\n",
    "md = smf.ols(formula=formula, data=total_df)\n",
    "res = md.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# participants have fewer post-study updates compared to pre-study updates\n",
    "sdf = df[df.is_participant == 1]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "# could optionally add some jitter:\n",
    "# + (np.random.random(len(sdf)) / 10)\n",
    "#ax.scatter(sdf.n_updates_prestudy + 1, sdf.n_updates_poststudy + 1, alpha=0.2, color='black')\n",
    "#hb = ax.hexbin(sdf.n_updates_prestudy, sdf.n_updates_poststudy, gridsize=10, bins='log', mincnt=0, extent=(0, 10, 0, 10))\n",
    "#bins = np.arange()\n",
    "counts, hbins, vbins, hb = ax.hist2d(sdf.n_updates_prestudy, sdf.n_updates_poststudy, \n",
    "    bins=[np.arange(0, np.max(sdf.n_updates_prestudy)+1), np.arange(0, np.max(sdf.n_updates_poststudy)+1)],\n",
    "    cmin=1,  norm=matplotlib.colors.LogNorm(), alpha=0.4)\n",
    "steps = np.arange(0, min(np.max(sdf.n_updates_prestudy)+1, np.max(sdf.n_updates_poststudy)+1))\n",
    "plt.step(steps, steps, color='darkgray')\n",
    "plt.step(steps, steps - 1, color='darkgray')\n",
    "for i in range(counts.shape[0]):\n",
    "    for j in range(counts.shape[1]):\n",
    "        if counts[i, j] > 0:\n",
    "            ax.text(hbins[i] + ((hbins[1] - hbins[0]) / 2), vbins[j] + ((vbins[1] - vbins[0]) / 2), \n",
    "                    f\"{counts[i, j]:.0f}\", \n",
    "                    ha='center', va='center', fontsize=8)\n",
    "#fig.colorbar(hb, ax=ax)\n",
    "#ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xlabel(\"# pre-study updates\")\n",
    "ax.set_ylabel(\"# post-study updates\")\n",
    "ax.set_title(\"Participant pre- and post-study Journal update counts\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between pre- and post-study updates for authors who had at least 1 update in the measurement period\n",
    "# participants had fewer updates in 80% of cases... compared to only 70% among control authors\n",
    "sdf = df[(df.n_updates_prestudy > 0)|(df.n_updates_poststudy > 0)]\n",
    "pd.crosstab(\n",
    "    sdf.is_participant, \n",
    "    (sdf.n_updates_poststudy - sdf.n_updates_prestudy)\\\n",
    "        .map(lambda diff: 'fewer' if diff < 0 else 'equal' if diff == 0 else 'more')\\\n",
    "        .rename(\"post - pre n_updates\"),\n",
    "    margins=True,\n",
    "    normalize='index',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/\n",
    "# n_authors,\n",
    "# n_interactions,\n",
    "# n_users_intereactedwith,\n",
    "# n_first_visits,\n",
    "# n_days_visited,\n",
    "# n_users_repeat_visited,\n",
    "# n_sitewide_interactions,\n",
    "# n_sitewide_sites_intereactedwith,\n",
    "# time_since_first_journal_update\n",
    "formula = \"\"\"n_updates_poststudy ~ \n",
    "                        is_recced +\n",
    "                        np.log(time_since_first_journal_update_prestudy) +\n",
    "                        n_recs +\n",
    "                        n_updates_prestudy + \n",
    "                        n_authors_prestudy +\n",
    "                        n_users_interactedwith_prestudy + \n",
    "                        n_first_visits_prestudy +\n",
    "                        n_days_visited_prestudy +\n",
    "                        n_users_repeat_visited_prestudy +\n",
    "                        n_sitewide_sites_intereactedwith_prestudy\"\"\"\n",
    "\n",
    "# md = smf.poisson(formula=formula, data=df)\n",
    "# res = md.fit()\n",
    "# res.summary()\n",
    "\n",
    "md = smf.glm(formula=formula, data=df, family=statsmodels.genmod.families.family.Poisson())\n",
    "\n",
    "res = md.fit()\n",
    "print(\"default\")\n",
    "print(res.summary().tables[1])\n",
    "\n",
    "res = md.fit(cov_type='HC0')\n",
    "print(\"hc0\")\n",
    "print(res.summary().tables[1])\n",
    "\n",
    "res = md.fit(cov_type='HC1')\n",
    "print(\"hc1\")\n",
    "print(res.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = smf.glm(formula=formula, data=df, family=statsmodels.genmod.families.family.Poisson())\n",
    "res = md.fit(cov_type='HC0')\n",
    "for line in res.summary().tables[0].as_csv().split(\"\\n\"):\n",
    "    if \"Pearson chi2\" in line:\n",
    "        chi2 = float(line.split(\",\")[-1])\n",
    "        print(chi2)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_ip_f(df, use_I=False):\n",
    "    \"\"\"\n",
    "    Create the f(y|X) part of IP weights using logistic regression\n",
    "    \n",
    "    Adapted from https://github.com/jrfiedler/causal_inference_python_code/blob/master/chapter12.ipynb\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Numpy array of IP weights\n",
    "    \n",
    "    \"\"\"\n",
    "    formula = \"\"\"\n",
    "        was_clicked ~ \n",
    "        np.log(time_since_first_journal_update) +\n",
    "        n_updates_pre + \n",
    "        n_authors_pre +\n",
    "        n_interactions_pre +\n",
    "        n_users_interactedwith_pre + \n",
    "        n_sitewide_interactions_pre +\n",
    "        n_sitewide_self_interactions_pre +\n",
    "        n_sitewide_sites_intereactedwith_pre +\n",
    "        n_first_visits_pre +\n",
    "        n_days_visited_pre +\n",
    "        n_users_repeat_visited_pre\n",
    "    \"\"\"\n",
    "    model = smf.logit(formula=formula, data=df)\n",
    "    res = model.fit(disp=0)\n",
    "    #print(res.summary())\n",
    "    weights = np.zeros(len(df))\n",
    "    weights[df.was_clicked == 1] = res.predict(df[df.was_clicked == 1])\n",
    "    weights[df.was_clicked == 0] = (1 - res.predict(df[df.was_clicked == 0]))\n",
    "    return weights\n",
    "\n",
    "def produce_ci_estimates(df, outcome):\n",
    "    block2 = df.copy()\n",
    "    block2.was_clicked = 0\n",
    "    block3 = df.copy()\n",
    "    block3.was_clicked = 1\n",
    "    \n",
    "    formula = outcome + \"\"\"\n",
    "        ~ was_clicked +\n",
    "        np.log(time_since_first_journal_update) +\n",
    "        n_updates_pre + \n",
    "        n_authors_pre +\n",
    "        n_interactions_pre +\n",
    "        n_users_interactedwith_pre + \n",
    "        n_sitewide_interactions_pre +\n",
    "        n_sitewide_self_interactions_pre +\n",
    "        n_sitewide_sites_intereactedwith_pre +\n",
    "        n_first_visits_pre +\n",
    "        n_days_visited_pre +\n",
    "        n_users_repeat_visited_pre\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_effect = df.loc[df.was_clicked==1, outcome].mean() - df.loc[df.was_clicked==0, outcome].mean()\n",
    "    \n",
    "    poisson_effect = -1\n",
    "    poisson_ci = [-1, -1]\n",
    "    if False:\n",
    "        try:\n",
    "            md = smf.glm(formula=formula, data=df, family=statsmodels.genmod.families.family.Poisson())\n",
    "            res = md.fit(cov_type='HC0')\n",
    "            if not res.mle_retvals['converged']:\n",
    "                raise ValueError(\"Poisson model failed to converge.\")\n",
    "            poisson_effect = res.params.was_clicked\n",
    "            poisson_ci = list(res.conf_int().loc['was_clicked'])\n",
    "        except:\n",
    "            poisson_effect = -1\n",
    "            poisson_ci = [-1, -1]\n",
    "    \n",
    "    # basic regression estimates\n",
    "    # that \"adjust for\" confounders\n",
    "    # plus standardization\n",
    "    md = smf.ols(formula=formula, data=df)\n",
    "    res = md.fit()\n",
    "    modeled_observational_effect = res.params.was_clicked\n",
    "    modeled_observational_ci = list(res.conf_int().loc['was_clicked'])\n",
    "    block2 = df.copy()\n",
    "    block2.was_clicked = 0\n",
    "    block3 = df.copy()\n",
    "    block3.was_clicked = 1\n",
    "    block2_pred = res.predict(block2)\n",
    "    block3_pred = res.predict(block3)\n",
    "    standardized_effect = block3_pred.mean() - block2_pred.mean()\n",
    "    \n",
    "    # IP weighting and the Bang-Robins doubly robust (DR) estimator\n",
    "    weights = logit_ip_f(df)\n",
    "    weights = 1 / weights\n",
    "    wls = smf.wls(formula=f'{outcome} ~ was_clicked', data=df, weights=weights)\n",
    "    res = wls.fit(disp=0)\n",
    "    ip_weighted_effect = res.params.was_clicked\n",
    "    \n",
    "    block1 = df.copy()\n",
    "    block1['R'] = weights\n",
    "    block1.loc[block1.was_clicked == 0, 'R'] *= -1\n",
    "    md = smf.ols(formula=formula + \"+ R\", data=block1)\n",
    "    res = md.fit()\n",
    "    block2 = block1.copy()\n",
    "    block2.was_clicked = 0\n",
    "    block3 = block1.copy()\n",
    "    block3.was_clicked = 1\n",
    "    block2_pred = res.predict(block2)\n",
    "    block3_pred = res.predict(block3)\n",
    "    dr_effect = block3_pred.mean() - block2_pred.mean()\n",
    "    \n",
    "    return {\n",
    "        'raw_diff': raw_effect,\n",
    "        'poisson_diff': poisson_effect,\n",
    "        'poisson_ci': poisson_ci,\n",
    "        'modeled_observational_diff': modeled_observational_effect,\n",
    "        'modeled_observational_ci': modeled_observational_ci,\n",
    "        'standardized_diff': standardized_effect,\n",
    "        'ip_weighted_diff': ip_weighted_effect,\n",
    "        'dr_diff': dr_effect,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = click_rec_sites_df[['first_click_timestamp', 'was_clicked']] \n",
    "total_df = compute_window_features(35 * one_day, 7 * one_day, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df[\"n_updates_post\"]\n",
    "raw_effect = total_df.loc[total_df.was_clicked==1, \"n_updates_post\"].mean() - total_df.loc[total_df.was_clicked==0, \"n_updates_post\"].mean()\n",
    "raw_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = total_df.sample(frac=1, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_ci_estimates(sdf, \"n_updates_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_ci_estimates(total_df, \"n_users_interactedwith_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_ci_estimates(total_df, \"n_users_repeat_visited_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_ci_estimates(total_df, \"n_first_visits_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_ci_estimates(total_df, \"n_interactions_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_ci_estimates(total_df, \"n_days_visited_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_ci_estimates(total_df, \"n_sitewide_sites_intereactedwith_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_sites_df[['first_click_timestamp', 'was_clicked']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time window sensitivity analysis: Putting it all together\n",
    "\n",
    "From the first SSE on August 2nd, can go at most 35 days (5 weeks) back and still have the diff features.\n",
    "\n",
    "Can go until Feb 23rd \"forward\" i.e. 91 days (13 weeks) from the last SSE timestamp (on Nov 24th).\n",
    "\n",
    "Time interval from first to last study interval is 82 days:\n",
    "\n",
    "    >datetime.utcfromtimestamp(last_sse_timestamp / 1000) - datetime.utcfromtimestamp(first_sse_timestamp / 1000)\n",
    "    datetime.timedelta(days=82, seconds=79186, microseconds=174000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_dir = os.path.join(git_root_dir, 'figures')\n",
    "os.makedirs(figures_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = 1000 * 60 * 60 * 24\n",
    "seven_days = one_day * 7\n",
    "ninety_days = one_day * 90\n",
    "time_window = ninety_days\n",
    "np.arange(0, 7 + 1, 7), np.arange(0, 91 + 1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_window_days = 35 # min(time_window_days, 35)\n",
    "front_window_days = 91\n",
    "\n",
    "# recced clicked vs non-clicked\n",
    "recced_site_df = click_rec_sites_df[['first_click_timestamp', 'was_clicked']] \n",
    "recced_df = compute_window_features(back_window_days * one_day, front_window_days * one_day, recced_site_df)\n",
    "\n",
    "# recced clicked vs pseudo-control \n",
    "clicked_site_df = click_sites_df[['first_click_timestamp', 'was_clicked']]\n",
    "clicked_df = compute_window_features(back_window_days * one_day, front_window_days * one_day, clicked_site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_columns = [\n",
    "    'n_updates_post', \n",
    "    'n_first_visits_post', \n",
    "    #'n_users_repeat_visited_post', \n",
    "    #'n_users_interactedwith_post', \n",
    "    'n_interactions_post', \n",
    "    #'n_days_visited_post',\n",
    "    'n_sitewide_interactions_post',\n",
    "    #'n_sitewide_sites_intereactedwith_post',\n",
    "    'n_sitewide_self_interactions_post'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "errors = 0\n",
    "for i in tqdm(range(1000)):\n",
    "    sdf = recced_df.sample(frac=1, replace=True)\n",
    "    for col in outcome_columns:\n",
    "        try:\n",
    "            ests = produce_ci_estimates(sdf, col)\n",
    "        except Exception as e:\n",
    "            errors +=1\n",
    "            continue\n",
    "        diff = {}\n",
    "        diff['outcome'] = col\n",
    "        diff['diff_raw'] = sdf.loc[sdf.was_clicked==1, col].mean() - sdf.loc[sdf.was_clicked==0, col].mean()\n",
    "        diff['diff_ols'] = ests['modeled_observational_diff']\n",
    "        diff['diff_dr'] = ests['dr_diff']\n",
    "        diffs.append(diff)\n",
    "rec_diff_df = pd.DataFrame(diffs)\n",
    "print(f\"Len: {len(rec_diff_df)}\")\n",
    "print(f\"Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_columns = [\n",
    "    'n_updates_post', \n",
    "    'n_first_visits_post', \n",
    "    #'n_users_repeat_visited_post', \n",
    "    #'n_users_interactedwith_post', \n",
    "    'n_interactions_post', \n",
    "    #'n_days_visited_post',\n",
    "    'n_sitewide_interactions_post',\n",
    "    #'n_sitewide_sites_intereactedwith_post',\n",
    "    'n_sitewide_self_interactions_post'\n",
    "]\n",
    "errors = 0\n",
    "diffs = []\n",
    "for i in tqdm(range(1000)):\n",
    "    sdf = clicked_df.sample(frac=1, replace=True)\n",
    "    for col in outcome_columns:\n",
    "        try:\n",
    "            ests = produce_ci_estimates(sdf, col)\n",
    "        except:\n",
    "            errors += 1\n",
    "            continue\n",
    "        diff = {}\n",
    "        diff['outcome'] = col\n",
    "        diff['diff_raw'] = sdf.loc[sdf.was_clicked==1, col].mean() - sdf.loc[sdf.was_clicked==0, col].mean()\n",
    "        diff['diff_ols'] = ests['modeled_observational_diff']\n",
    "        diff['diff_dr'] = ests['dr_diff']\n",
    "        diffs.append(diff)\n",
    "clicked_diff_df = pd.DataFrame(diffs)\n",
    "print(f\"Len: {len(clicked_diff_df)}\")\n",
    "print(f\"Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_diffs = []\n",
    "for col in outcome_columns:\n",
    "    try:\n",
    "        ests = produce_ci_estimates(clicked_df, col)\n",
    "    except:\n",
    "        continue\n",
    "    diff = {}\n",
    "    diff['outcome'] = col\n",
    "    diff['diff_raw'] = clicked_df.loc[clicked_df.was_clicked==1, col].mean() - clicked_df.loc[clicked_df.was_clicked==0, col].mean()\n",
    "    diff['diff_ols'] = ests['modeled_observational_diff']\n",
    "    diff['diff_dr'] = ests['dr_diff']\n",
    "    true_diffs.append(diff)\n",
    "true_clicked_diff_df = pd.DataFrame(true_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_diffs = []\n",
    "for col in outcome_columns:\n",
    "    try:\n",
    "        ests = produce_ci_estimates(recced_df, col)\n",
    "    except:\n",
    "        continue\n",
    "    diff = {}\n",
    "    diff['outcome'] = col\n",
    "    diff['diff_raw'] = recced_df.loc[recced_df.was_clicked==1, col].mean() - recced_df.loc[recced_df.was_clicked==0, col].mean()\n",
    "    diff['diff_ols'] = ests['modeled_observational_diff']\n",
    "    diff['diff_dr'] = ests['dr_diff']\n",
    "    true_diffs.append(diff)\n",
    "true_rec_diff_df = pd.DataFrame(true_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_diff_df = pd.read_feather(\"clicked_diff_df_20220602.feather\")\n",
    "rec_diff_df = pd.read_feather(\"rec_diff_df_20220602.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "outcomes1 = [\n",
    "    'n_first_visits_post', \n",
    "    #'n_users_repeat_visited_post', \n",
    "    #'n_users_interactedwith_post', \n",
    "    #'n_interactions_post', \n",
    "    #'n_days_visited_post',\n",
    "    'n_sitewide_interactions_post',\n",
    "    #'n_updates_post', \n",
    "    #'n_sitewide_sites_intereactedwith_post',\n",
    "    #'n_sitewide_self_interactions_post'\n",
    "]\n",
    "outcomes2 = [\n",
    "    'n_updates_post', \n",
    "    #'n_first_visits_post', \n",
    "    #'n_users_repeat_visited_post', \n",
    "    #'n_users_interactedwith_post', \n",
    "    'n_interactions_post', \n",
    "    #'n_days_visited_post',\n",
    "    #'n_sitewide_interactions_post',\n",
    "    #'n_sitewide_sites_intereactedwith_post',\n",
    "    'n_sitewide_self_interactions_post'\n",
    "]\n",
    "\n",
    "outcomes = [outcomes1, outcomes2]\n",
    "pretty_name_map = {\n",
    "    'n_updates_post': \"Journal updates\",\n",
    "    'n_first_visits_post': \"Peer visits\",\n",
    "    'n_users_repeat_visited_post': \"Repeat user visits\",\n",
    "    'n_users_interactedwith_post': \"Peer initiations\", \n",
    "    'n_interactions_post': \"Peer interactions\", \n",
    "    'n_days_visited_post': \"# days visiting peers\",\n",
    "    'n_sitewide_interactions_post': \"Recommended\\nsite author\\noutward interactions\",\n",
    "    'n_sitewide_sites_intereactedwith_post': \"Site author initiations\",\n",
    "    'n_sitewide_self_interactions_post': \"Recommended\\n site author\\nself interactions\"\n",
    "}\n",
    "\n",
    "#fig, axes = plt.subplots(2, 2, gridspec_kw={'width_ratios': [2, 1]}, )\n",
    "fig, axes = plt.subplots(figsize=(5.4, 2))\n",
    "#fig = plt.figure(figsize=(5.4, 4))\n",
    "\n",
    "#gs = GridSpec(2, 3, figure=fig)\n",
    "#Xax1 = fig.add_subplot(gs[0, :])\n",
    "# identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))\n",
    "#ax2 = fig.add_subplot(gs[1, :-1])\n",
    "#ax3 = fig.add_subplot(gs[1, -1])\n",
    "#axes = [ax1, ax2, ax3]\n",
    "lowerq = 0.025\n",
    "upperq = 0.975\n",
    "\n",
    "diff_df = [rec_diff_df, clicked_diff_df]\n",
    "true_diff_df = [true_rec_diff_df, true_clicked_diff_df, ]\n",
    "\n",
    "#plt_lims = [[-0.15, 0.15], [-2.3, 7.3]]\n",
    "plt_lims = [[-0.1, 0.1], [-4, 4]]\n",
    "\n",
    "xticks = []\n",
    "xticklabels = []\n",
    "i = 0\n",
    "\n",
    "for plt_col in range(2):\n",
    "    clipped = False\n",
    "    plt_lim = plt_lims[plt_col]\n",
    "    if plt_col == 0:\n",
    "        ax = axes\n",
    "    else:\n",
    "        ax = axes.twinx()\n",
    "    #ax.axhline(0, color='gray', alpha=0.5, zorder=-1, linestyle=\"--\")\n",
    "    ax.axhline(0, color='black', alpha=1, zorder=-1, linestyle=\"-\", linewidth=0.75)\n",
    "    ax.axvline(6.6, color='black', alpha=1, zorder=-1, linestyle=\"-\", linewidth=0.75)\n",
    "    for col in outcomes[plt_col]:\n",
    "        #if col == \"n_interactions_poststudy\" or col == \"n_days_visited_poststudy\":\n",
    "        #    continue\n",
    "        #xticks.append(i + 1)\n",
    "        #xticklabels.append(f\"{pretty_name_map[col]}\")\n",
    "        xticks.extend([i, i+1, i+2])\n",
    "        xticklabels.extend([\"Raw\", f\"OLS\\n{pretty_name_map[col]}\", \"DR\"])\n",
    "\n",
    "        for df_i, df in enumerate(diff_df):\n",
    "            diffs = df[df.outcome == col]\n",
    "            if df_i == 0:\n",
    "                i_offset = -0.1\n",
    "                df_color = 'darkgray'\n",
    "                df_label = 'Clicked vs Pseudo-Control'\n",
    "            else:\n",
    "                df_color = 'lightgray'\n",
    "                df_label = 'Clicked vs Non-Clicked'\n",
    "\n",
    "                i_offset = 0.1\n",
    "\n",
    "            for j, diff_col in enumerate(['diff_raw', 'diff_ols', 'diff_dr']):\n",
    "                ds = diffs[diff_col]\n",
    "                estimate = true_diff_df[df_i].loc[true_diff_df[df_i].outcome == col, diff_col].iloc[0] / 91 * 7\n",
    "                m = ds.median() / 91 * 7\n",
    "                u = ds.quantile(upperq) / 91 * 7\n",
    "                l = ds.quantile(lowerq) / 91 * 7\n",
    "                uerr = np.abs(u - estimate)\n",
    "                lerr = np.abs(l - estimate)\n",
    "                print(f\"{col:>40} {diff_col} {i+j}, true={estimate:.2f}; bs={m:.2f} [{l:.2f},{u:.2f}], {uerr:.2f}, {lerr:.2f} {estimate - m:.3f}\")\n",
    "                err_bars = ax.errorbar(i+j+i_offset, estimate, yerr=[[lerr,],[uerr,]], color=df_color, capsize=3, zorder=1)\n",
    "                for b in err_bars[1]:\n",
    "                    b.set_clip_on(True)\n",
    "                dot = ax.scatter(i+j+i_offset, estimate, color='black', zorder=2, marker='s' if df_i == 0 else 'o', s=8, label=df_label)\n",
    "                if df_i == 0:\n",
    "                    dot0 = dot\n",
    "                else:\n",
    "                    dot1 = dot\n",
    "\n",
    "        i += 3.4\n",
    "\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=7)\n",
    "    \n",
    "    ax.set_ylabel(\"Excess weekly actions\", fontsize=7)\n",
    "    if plt_col == 0:\n",
    "        #ax.set_yticks([-0.1, -0.05, 0, 0.05, 0.1])\n",
    "        \n",
    "        ax.set_yticks([-0.1, -0.05, 0, 0.05, 0.1])\n",
    "        ax.legend([dot0, dot1],['Non-Clicked', 'Pseudo-Control'], fontsize=6, loc='upper center', bbox_to_anchor=(0.55,0.97))\n",
    "        clipped_offset_x = 0.6\n",
    "#     elif plt_col == 1:\n",
    "        \n",
    "#         ax.set_ylabel(\"Excess weekly actions\", fontsize=7)\n",
    "#         ax.set_yticks([-1, 0, 1, 2])\n",
    "#         clipped_offset_x = 0.5\n",
    "    else:\n",
    "        ax.set_yticks([-4, -2, 0, 2, 4])\n",
    "        clipped_offset_x = 0.4\n",
    "\n",
    "    ax.set_ylim(plt_lim)\n",
    "    ax.set_xlim(-0.4, i - 1)\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "#     if plt_col == 1:\n",
    "#         # ADD THIS LINE\n",
    "#         ax.grid(None)\n",
    "    i += 1\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "#plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "#plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=0.5)\n",
    "#ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n",
    "#plt.subplots_adjust(top = 0.4, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)\n",
    "\n",
    "#bbox = matplotlib.transforms.Bbox.from_bounds(0,0,1,0.2)\n",
    "image_shortfilename = f\"recced_outcome_estimates.pdf\"\n",
    "image_filename = os.path.join(figures_dir, image_shortfilename)\n",
    "plt.savefig(image_filename, format='pdf', dpi=200, pad_inches=0) #, bbox_inches=bbox) #, transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "outcomes1 = [\n",
    "    'n_first_visits_post', \n",
    "    #'n_users_repeat_visited_post', \n",
    "    #'n_users_interactedwith_post', \n",
    "    #'n_interactions_post', \n",
    "    #'n_days_visited_post',\n",
    "    'n_sitewide_interactions_post',\n",
    "    #'n_sitewide_sites_intereactedwith_post',\n",
    "    #'n_sitewide_self_interactions_post'\n",
    "]\n",
    "outcomes2 = [\n",
    "    'n_updates_post', \n",
    "    #'n_first_visits_post', \n",
    "    #'n_users_repeat_visited_post', \n",
    "    #'n_users_interactedwith_post', \n",
    "    'n_interactions_post', \n",
    "    #'n_days_visited_post',\n",
    "    #'n_sitewide_interactions_post',\n",
    "    #'n_sitewide_sites_intereactedwith_post',\n",
    "    'n_sitewide_self_interactions_post'\n",
    "]\n",
    "\n",
    "outcomes = [outcomes1, outcomes2]\n",
    "pretty_name_map = {\n",
    "    'n_updates_post': \"Journal updates\",\n",
    "    'n_first_visits_post': \"Peer visits\",\n",
    "    'n_users_repeat_visited_post': \"Repeat user visits\",\n",
    "    'n_users_interactedwith_post': \"Peer initiations\", \n",
    "    'n_interactions_post': \"Peer interactions\", \n",
    "    'n_days_visited_post': \"# days visiting peers\",\n",
    "    'n_sitewide_interactions_post': \"Recommended\\nsite author\\noutward interactions\",\n",
    "    'n_sitewide_sites_intereactedwith_post': \"Site author initiations\",\n",
    "    'n_sitewide_self_interactions_post': \"Recommended\\n site author\\nself interactions\"\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, gridspec_kw={'width_ratios': [2, 3]}, figsize=(5.4, 2))\n",
    "fig.subplots_adjust(wspace=0.001)\n",
    "\n",
    "# turn off spines\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "axes[1].spines['left'].set_visible(False)\n",
    "\n",
    "lowerq = 0.025\n",
    "upperq = 0.975\n",
    "\n",
    "diff_df = [rec_diff_df, clicked_diff_df]\n",
    "true_diff_df = [true_rec_diff_df, true_clicked_diff_df, ]\n",
    "\n",
    "#plt_lims = [[-0.15, 0.15], [-2.3, 7.3]]\n",
    "plt_lims = [[-0.1, 0.1], [-4, 4]]\n",
    "\n",
    "\n",
    "for plt_col in range(2):\n",
    "    i = 0\n",
    "    xticks = []\n",
    "    xticklabels = []\n",
    "    clipped = False\n",
    "    plt_lim = plt_lims[plt_col]\n",
    "    #ax.axhline(0, color='gray', alpha=0.5, zorder=-1, linestyle=\"--\")\n",
    "    axes[plt_col].axhline(0, color='black', alpha=1, zorder=-1, linestyle=\"-\", linewidth=0.75)\n",
    "    for col in outcomes[plt_col]:\n",
    "        #if col == \"n_interactions_poststudy\" or col == \"n_days_visited_poststudy\":\n",
    "        #    continue\n",
    "        #xticks.append(i + 1)\n",
    "        #xticklabels.append(f\"{pretty_name_map[col]}\")\n",
    "        xticks.extend([i, i+1, i+2])\n",
    "        xticklabels.extend([\"Raw\", f\"OLS\\n{pretty_name_map[col]}\", \"DR\"])\n",
    "\n",
    "        for df_i, df in enumerate(diff_df):\n",
    "            diffs = df[df.outcome == col]\n",
    "            if df_i == 0:\n",
    "                i_offset = -0.1\n",
    "                df_color = 'darkgray'\n",
    "                df_label = 'Clicked vs Pseudo-Control'\n",
    "            else:\n",
    "                df_color = 'lightgray'\n",
    "                df_label = 'Clicked vs Non-Clicked'\n",
    "\n",
    "                i_offset = 0.1\n",
    "\n",
    "            for j, diff_col in enumerate(['diff_raw', 'diff_ols', 'diff_dr']):\n",
    "                ds = diffs[diff_col]\n",
    "                estimate = true_diff_df[df_i].loc[true_diff_df[df_i].outcome == col, diff_col].iloc[0] / 91 * 7\n",
    "                m = ds.median() / 91 * 7\n",
    "                u = ds.quantile(upperq) / 91 * 7\n",
    "                l = ds.quantile(lowerq) / 91 * 7\n",
    "                uerr = np.abs(u - estimate)\n",
    "                lerr = np.abs(l - estimate)\n",
    "                print(f\"{col:>40} {diff_col} {i+j}, true={estimate:.2f}; bs={m:.2f} [{l:.2f},{u:.2f}], {uerr:.2f}, {lerr:.2f} {estimate - m:.3f}\")\n",
    "                err_bars = axes[plt_col].errorbar(i+j+i_offset, estimate, yerr=[[lerr,],[uerr,]], color=df_color, capsize=3, zorder=1)\n",
    "                for b in err_bars[1]:\n",
    "                    b.set_clip_on(True)\n",
    "                dot = axes[plt_col].scatter(i+j+i_offset, estimate, color='black', zorder=2, marker='s' if df_i == 0 else 'o', s=8, label=df_label)\n",
    "                if df_i == 0:\n",
    "                    dot0 = dot\n",
    "                else:\n",
    "                    dot1 = dot\n",
    "\n",
    "        i += 3.4\n",
    "\n",
    "    axes[plt_col].set_xticks(xticks)\n",
    "    axes[plt_col].set_xticklabels(xticklabels)\n",
    "    axes[plt_col].tick_params(axis='both', which='major', labelsize=7)\n",
    "    if plt_col == 0:\n",
    "        axes[plt_col].set_ylabel(\"Excess weekly actions\", fontsize=7)\n",
    "        #ax.set_yticks([-0.1, -0.05, 0, 0.05, 0.1])\n",
    "        axes[plt_col].set_yticks([-0.1, -0.05, 0, 0.05, 0.1])\n",
    "        clipped_offset_x = 0.6\n",
    "#     elif plt_col == 1:\n",
    "\n",
    "#         ax.set_ylabel(\"Excess weekly actions\", fontsize=7)\n",
    "#         ax.set_yticks([-1, 0, 1, 2])\n",
    "#         clipped_offset_x = 0.5\n",
    "    elif plt_col == 1:\n",
    "        axes[plt_col].legend([dot0, dot1],['Non-Clicked', 'Pseudo-Control'], fontsize=6, loc='upper left')\n",
    "        axes[plt_col].set_ylabel(\"Excess weekly actions\", fontsize=7)\n",
    "        axes[plt_col].set_yticks([-4, -2, 0, 2, 4])\n",
    "        clipped_offset_x = 0.4\n",
    "        axes[plt_col].yaxis.tick_right()\n",
    "        axes[plt_col].yaxis.set_label_position(\"right\")\n",
    "        \n",
    "    axes[plt_col].set_ylim(plt_lim)\n",
    "    axes[plt_col].set_xlim(-0.4, i - 1)\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "#     if plt_col == 1:\n",
    "#         # ADD THIS LINE\n",
    "#         ax.grid(None)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "d = .5  # proportion of vertical to horizontal extent of the slanted line\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
    "              linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "axes[0].plot([1, 1], [1, 0], transform=axes[0].transAxes, **kwargs)\n",
    "axes[1].plot([0, 0], [1, 0], transform=axes[1].transAxes, **kwargs)\n",
    "\n",
    "# axes[0].plot([1, 1.03], [0.499, 0.52], clip_on=False, transform=axes[0].transAxes, color='black', alpha=1, zorder=-1, linestyle=\"-\", linewidth=0.75)\n",
    "# axes[0].plot([1.03, 1.03], [0.48, 0.52], clip_on=False, transform=axes[0].transAxes, color='black', alpha=1, zorder=-1, linestyle=\"-\", linewidth=0.75)\n",
    "# axes[0].plot([1.03, 1.06], [0.48, 0.499], clip_on=False, transform=axes[0].transAxes, color='black', alpha=1, zorder=-1, linestyle=\"-\", linewidth=0.75)\n",
    "\n",
    "# axes[1].plot([-0.02, 0], [0.48, 0.499], clip_on=False, transform=axes[1].transAxes, color='black', alpha=1, zorder=-1, linestyle=\"-\", linewidth=0.75)\n",
    "# axes[1].plot([-0.02, -0.02], [0.48, 0.499], clip_on=False, transform=axes[1].transAxes, color='black', alpha=1, zorder=-1, linestyle=\"-\", linewidth=0.75)\n",
    "\n",
    "#plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "#plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=0.5)\n",
    "#ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n",
    "#plt.subplots_adjust(top = 0.4, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)\n",
    "\n",
    "#bbox = matplotlib.transforms.Bbox.from_bounds(0,0,1,0.2)\n",
    "image_shortfilename = f\"recced_outcome_estimates.pdf\"\n",
    "image_filename = os.path.join(figures_dir, image_shortfilename)\n",
    "plt.savefig(image_filename, format='pdf', dpi=200, pad_inches=0) #, bbox_inches=bbox) #, transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_study_dataframes():\n",
    "    one_day = 1000 * 60 * 60 * 24\n",
    "    for time_window_days in tqdm(np.arange(7, 7 + 1, 7), desc='Weekly frame data'):#91\n",
    "#         if time_window_days > 35:\n",
    "#             continue\n",
    "        back_window_days = 35 # min(time_window_days, 35)\n",
    "        front_window_days = time_window_days\n",
    "\n",
    "        # recced clicked vs non-clicked\n",
    "        recced_site_df = click_rec_sites_df[['first_click_timestamp', 'was_clicked']]\n",
    "        recced_df = compute_window_features(back_window_days * one_day, front_window_days * one_day, recced_site_df)\n",
    "\n",
    "        # recced clicked vs pseudo-control \n",
    "        clicked_site_df = click_sites_df[['first_click_timestamp', 'was_clicked']]\n",
    "        clicked_df = compute_window_features(back_window_days * one_day, front_window_days * one_day, clicked_site_df)\n",
    "            \n",
    "        metadata = {\n",
    "            'back_window_days': back_window_days,\n",
    "            'front_window_days': front_window_days,\n",
    "        }\n",
    "        yield recced_df, clicked_df, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(7, 7 + 1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def logit_ip_f(df, use_I=False):\n",
    "    \"\"\"\n",
    "    Create the f(y|X) part of IP weights using logistic regression\n",
    "    \n",
    "    Adapted from https://github.com/jrfiedler/causal_inference_python_code/blob/master/chapter12.ipynb\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Numpy array of IP weights\n",
    "    \n",
    "    \"\"\"\n",
    "    formula = \"\"\"\n",
    "        was_clicked ~ \n",
    "        np.log(time_since_first_journal_update) +\n",
    "        n_updates_pre + \n",
    "        n_authors_pre +\n",
    "        n_interactions_pre +\n",
    "        n_users_interactedwith_pre + \n",
    "        n_sitewide_interactions_pre +\n",
    "        n_sitewide_self_interactions_pre +\n",
    "        n_sitewide_sites_intereactedwith_pre +\n",
    "        n_first_visits_pre +\n",
    "        n_days_visited_pre +\n",
    "        n_users_repeat_visited_pre\n",
    "    \"\"\"\n",
    "    model = smf.logit(formula=formula, data=df)\n",
    "    res = model.fit(disp=0)\n",
    "#     print(res.summary())\n",
    "    weights = np.zeros(len(df))\n",
    "    weights[df.was_clicked == 1] = res.predict(df[df.was_clicked == 1])\n",
    "    weights[df.was_clicked == 0] = (1 - res.predict(df[df.was_clicked == 0]))\n",
    "    return weights\n",
    "\n",
    "def produce_ci_estimates(df, outcome):\n",
    "    block2 = df.copy()\n",
    "    block2.was_clicked = 0\n",
    "    block3 = df.copy()\n",
    "    block3.was_clicked = 1\n",
    "    \n",
    "    formula = outcome + \"\"\"\n",
    "        ~ was_clicked +\n",
    "        np.log(time_since_first_journal_update) +\n",
    "        n_updates_pre + \n",
    "        n_authors_pre +\n",
    "        n_interactions_pre +\n",
    "        n_users_interactedwith_pre + \n",
    "        n_sitewide_interactions_pre +\n",
    "        n_sitewide_self_interactions_pre +\n",
    "        n_sitewide_sites_intereactedwith_pre +\n",
    "        n_first_visits_pre +\n",
    "        n_days_visited_pre +\n",
    "        n_users_repeat_visited_pre\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_effect = df.loc[df.was_clicked==1, outcome].mean() - df.loc[df.was_clicked==0, outcome].mean()\n",
    "    \n",
    "    poisson_effect = -1\n",
    "    poisson_ci = [-1, -1]\n",
    "    if False:\n",
    "        try:\n",
    "            md = smf.glm(formula=formula, data=df, family=statsmodels.genmod.families.family.Poisson())\n",
    "            res = md.fit(cov_type='HC0')\n",
    "            if not res.mle_retvals['converged']:\n",
    "                raise ValueError(\"Poisson model failed to converge.\")\n",
    "            poisson_effect = res.params.was_clicked\n",
    "            poisson_ci = list(res.conf_int().loc['was_clicked'])\n",
    "        except:\n",
    "            poisson_effect = -1\n",
    "            poisson_ci = [-1, -1]\n",
    "    \n",
    "    # basic regression estimates\n",
    "    # that \"adjust for\" confounders\n",
    "    # plus standardization\n",
    "    md = smf.ols(formula=formula, data=df)\n",
    "    res = md.fit()\n",
    "    modeled_observational_effect = res.params.was_clicked\n",
    "    modeled_observational_ci = list(res.conf_int().loc['was_clicked'])\n",
    "    block2 = df.copy()\n",
    "    block2.was_clicked = 0\n",
    "    block3 = df.copy()\n",
    "    block3.was_clicked = 1\n",
    "    block2_pred = res.predict(block2)\n",
    "    block3_pred = res.predict(block3)\n",
    "    standardized_effect = block3_pred.mean() - block2_pred.mean()\n",
    "    \n",
    "    # IP weighting and the Bang-Robins doubly robust (DR) estimator\n",
    "    weights = logit_ip_f(df)\n",
    "    weights = 1 / weights\n",
    "    wls = smf.wls(formula=f'{outcome} ~ was_clicked', data=df, weights=weights)\n",
    "    res = wls.fit(disp=0)\n",
    "    ip_weighted_effect = res.params.was_clicked\n",
    "    \n",
    "    block1 = df.copy()\n",
    "    block1['R'] = weights\n",
    "    block1.loc[block1.was_clicked == 0, 'R'] *= -1\n",
    "    md = smf.ols(formula=formula + \"+ R\", data=block1)\n",
    "    res = md.fit()\n",
    "    block2 = block1.copy()\n",
    "    block2.was_clicked = 0\n",
    "    block3 = block1.copy()\n",
    "    block3.was_clicked = 1\n",
    "    block2_pred = res.predict(block2)\n",
    "    block3_pred = res.predict(block3)\n",
    "    dr_effect = block3_pred.mean() - block2_pred.mean()\n",
    "    \n",
    "    return {\n",
    "        'raw_diff': raw_effect,\n",
    "        'poisson_diff': poisson_effect,\n",
    "        'poisson_ci': poisson_ci,\n",
    "        'modeled_observational_diff': modeled_observational_effect,\n",
    "        'modeled_observational_ci': modeled_observational_ci,\n",
    "        'standardized_diff': standardized_effect,\n",
    "        'ip_weighted_diff': ip_weighted_effect,\n",
    "        'dr_diff': dr_effect,\n",
    "    }\n",
    "\n",
    "def compute_diff(df, outcome, bootstrap_iters=1000):\n",
    "    ests = produce_ci_estimates(df, outcome)\n",
    "    diff = {\n",
    "        'outcome': outcome,\n",
    "        'diff_raw': ests['raw_diff'],\n",
    "        'diff_ols': ests['modeled_observational_diff'],\n",
    "        'diff_ols_lower': ests['modeled_observational_ci'][0],\n",
    "        'diff_ols_upper': ests['modeled_observational_ci'][1],\n",
    "        'diff_poisson': ests['poisson_diff'],\n",
    "        'diff_poisson_lower': ests['poisson_ci'][0],\n",
    "        'diff_poisson_upper': ests['poisson_ci'][1],\n",
    "        'diff_dr': ests['dr_diff'],\n",
    "    }\n",
    "\n",
    "    # bootstrapping\n",
    "    bs_diffs = []\n",
    "    for i in tqdm(range(bootstrap_iters), desc=f'Bootstrapping {outcome}', disable=True):\n",
    "        sdf = df.sample(frac=1, replace=True)\n",
    "        # TODO move this try/catch block into bsdiff, so that e.g. the raw and the OLS samples can still be computed\n",
    "        try:\n",
    "            ests = produce_ci_estimates(sdf, outcome)\n",
    "        except Exception as e:\n",
    "            \n",
    "            continue\n",
    "        bsdiff = {\n",
    "            'diff_raw': ests['raw_diff'],\n",
    "            'diff_ols': ests['modeled_observational_diff'],\n",
    "            'diff_poisson': ests['poisson_diff'],\n",
    "            'diff_dr': ests['dr_diff'],\n",
    "        }\n",
    "        bs_diffs.append(bsdiff)\n",
    "    bsdiff_df = pd.DataFrame(bs_diffs)\n",
    "    diff['n_bootstraps'] = len(bsdiff_df)\n",
    "    for diff_col in ['diff_raw', 'diff_ols', 'diff_poisson', 'diff_dr']:\n",
    "        means = bsdiff_df[diff_col]\n",
    "        lower = means.quantile(0.025)\n",
    "        upper = means.quantile(0.975)\n",
    "        diff[diff_col + \"_lower\"] = lower\n",
    "        diff[diff_col + \"_upper\"] = upper\n",
    "        diff[diff_col + \"_bs_means\"] = list(means)\n",
    "    return diff\n",
    "\n",
    "def compute_effects():\n",
    "    outcomes = [\n",
    "        'n_updates_post', \n",
    "        'n_first_visits_post', \n",
    "        'n_users_repeat_visited_post', \n",
    "        'n_users_interactedwith_post', \n",
    "        'n_interactions_post', \n",
    "        'n_days_visited_post',\n",
    "        'n_sitewide_interactions_post',\n",
    "        'n_sitewide_sites_intereactedwith_post',\n",
    "        'n_sitewide_self_interactions_post'\n",
    "    ]\n",
    "    diffs = []\n",
    "    for recced_df, clicked_df, metadata in generate_study_dataframes():\n",
    "        for time_period, df in (('recced', recced_df), ('clicked', clicked_df)):\n",
    "            for outcome in tqdm(outcomes, desc='Outcomes'):\n",
    "                diff = compute_diff(df, outcome)\n",
    "                diff['time_period'] = time_period\n",
    "                diff.update(metadata)\n",
    "                diffs.append(diff)\n",
    "    diff_df = pd.DataFrame(diffs)\n",
    "    return diff_df\n",
    "\n",
    "def compute_effects_test():\n",
    "    outcomes = [\n",
    "        'n_updates_post'\n",
    "    ]\n",
    "    diffs = []\n",
    "    for recced_df, clicked_df, metadata in generate_study_dataframes():\n",
    "        for time_period, df in (('recced', recced_df), ('clicked', clicked_df)):\n",
    "            for outcome in tqdm(outcomes, desc='Outcomes'):\n",
    "                diff = compute_diff(df, outcome)\n",
    "                diff['time_period'] = time_period\n",
    "                diff.update(metadata)\n",
    "                diffs.append(diff)\n",
    "    diff_df = pd.DataFrame(diffs)\n",
    "    return diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('error')\n",
    "    compute_effects_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_effects_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially, with 13 weeks x {poststudy, study} x 9 outcomes: ??? runtime\n",
    "import datetime;\n",
    "\n",
    "ct = datetime.datetime.now()\n",
    "print(ct)\n",
    "\n",
    "diff_df = compute_effects()\n",
    "print(len(diff_df))\n",
    "\n",
    "ct = datetime.datetime.now()\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only the first 5 weeks, with full back_window_days (i.e. 35)\n",
    "diff_df.to_feather(\"diff_df_20220522.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.read_feather(\"diff_df_20220528.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_dir = os.path.join(git_root_dir, 'figures')\n",
    "os.makedirs(figures_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "outcomes = [\n",
    "    'n_updates_post', \n",
    "    'n_first_visits_post', \n",
    "    'n_users_repeat_visited_post', \n",
    "    'n_users_interactedwith_post', \n",
    "    'n_interactions_post', \n",
    "    'n_days_visited_post',\n",
    "    'n_sitewide_interactions_post',\n",
    "    'n_sitewide_sites_intereactedwith_post',\n",
    "    'n_sitewide_self_interactions_post'\n",
    "]\n",
    "pretty_name_map = {\n",
    "    'n_updates_post': \"Journal updates\",\n",
    "    'n_first_visits_post': \"Peer visits\",\n",
    "    'n_users_repeat_visited_post': \"Repeat user visits\",\n",
    "    'n_users_interactedwith_post': \"Peer initiations\", \n",
    "    'n_interactions_post': \"Peer interactions\", \n",
    "    'n_days_visited_post': \"# days visiting peers\",\n",
    "    'n_sitewide_interactions_post': \"Site author interactions\",\n",
    "    'n_sitewide_sites_intereactedwith_post': \"Site author initiations\",\n",
    "    'n_sitewide_self_interactions_post': \"Site author self interactions\"\n",
    "}\n",
    "fig, axes = plt.subplots(len(outcomes), 2, figsize=(10, 44))\n",
    "\n",
    "for time_period, col in zip(['recced', 'clicked'], [0, 1]):\n",
    "    for row, outcome in enumerate(outcomes):\n",
    "        ax = axes[row, col]\n",
    "        sdf = diff_df[(diff_df.outcome == outcome)&(diff_df.time_period==time_period)]\n",
    "\n",
    "        ax.axhline(0.0, color='black', linestyle='--')\n",
    "        ax.axvline(5, color='gray', linestyle='-', alpha=0.5)\n",
    "\n",
    "        fill_alpha = 0.05\n",
    "        ax.plot(sdf.front_window_days / 7, sdf.diff_raw / sdf.front_window_days * 7, marker='.', label='Raw', color='blue')\n",
    "        ax.fill_between(sdf.front_window_days / 7, sdf.diff_raw_lower / sdf.front_window_days * 7, sdf.diff_raw_upper / sdf.front_window_days * 7, color='blue', alpha=fill_alpha)\n",
    "\n",
    "        ax.plot(sdf.front_window_days / 7, sdf.diff_ols / sdf.front_window_days * 7, marker='.', label='OLS', color='orange')\n",
    "        ax.fill_between(sdf.front_window_days / 7, sdf.diff_ols_lower / sdf.front_window_days * 7, sdf.diff_ols_upper / sdf.front_window_days * 7, color='orange', alpha=fill_alpha)\n",
    "\n",
    "        ax.plot(sdf.front_window_days / 7, sdf.diff_dr / sdf.front_window_days * 7, marker='.', label='DR', color='green')\n",
    "        ax.fill_between(sdf.front_window_days / 7, sdf.diff_dr_lower / sdf.front_window_days * 7, sdf.diff_dr_upper / sdf.front_window_days * 7, color='green', alpha=fill_alpha)\n",
    "\n",
    "        ax.set_xlabel(f\"Time since clicked (weeks)\")\n",
    "        ax.set_ylabel(\"Excess weekly actions\")\n",
    "        ax.set_title(\"\\n\".join(wrap(f\"{pretty_name_map[outcome]} after click ({'Clicked vs Psuedo Control' if time_period == 'clicked' else 'Clicked vs Non-Clicked'})\", 30)))\n",
    "        ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "# image_shortfilename = f\"recced_site_outcomes_all.pdf\"\n",
    "# image_filename = os.path.join(figures_dir, image_shortfilename)\n",
    "# fig.savefig(image_filename, format='pdf', dpi=200, pad_inches=0, bbox_inches='tight') #, transparent=True)\n",
    "plt.show()\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = [\n",
    "    'n_updates_post', \n",
    "    'n_first_visits_post', \n",
    "    'n_users_repeat_visited_post', \n",
    "    'n_users_interactedwith_post', \n",
    "    'n_interactions_post', \n",
    "    #'n_days_visited_post',\n",
    "    'n_sitewide_interactions_post',\n",
    "    'n_sitewide_sites_intereactedwith_post',\n",
    "    'n_sitewide_self_interactions_post'\n",
    "]\n",
    "pretty_name_map = {\n",
    "    'n_updates_post': \"Journal updates\",\n",
    "    'n_first_visits_post': \"Peer visits\",\n",
    "    'n_users_repeat_visited_post': \"Repeat user visits\",\n",
    "    'n_users_interactedwith_post': \"Peer initiations\", \n",
    "    'n_interactions_post': \"Peer interactions\", \n",
    "    'n_days_visited_post': \"# days visiting peers\",\n",
    "    'n_sitewide_interactions_post': \"Site interactions\",\n",
    "    'n_sitewide_sites_intereactedwith_post': \"Site initiations\\n\",\n",
    "    'n_sitewide_self_interactions_post': \"Site self interactions\"\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(len(outcomes), 2, figsize=(5.6, 7))\n",
    "cm = matplotlib.cm.tab10\n",
    "\n",
    "for time_period, col in zip(['recced', 'clicked'], [0, 1]):\n",
    "    for row, outcome in enumerate(outcomes):\n",
    "        ax = axes[row, col]\n",
    "        sdf = diff_df[(diff_df.outcome == outcome)&(diff_df.time_period==time_period)]\n",
    "\n",
    "        ax.axhline(0.0, color='black', linestyle='--')\n",
    "        if time_period == 'study':\n",
    "            ax.axvline(82 / 7, color='darkgray', linestyle=':', alpha=0.5, label='End of study')\n",
    "\n",
    "        fill_alpha = 0.07\n",
    "        ax.plot(sdf.front_window_days / 7, sdf.diff_raw / sdf.front_window_days * 7, marker='.', label='Raw', color=cm(0))\n",
    "        ax.fill_between(sdf.front_window_days / 7, sdf.diff_raw_lower / sdf.front_window_days * 7, sdf.diff_raw_upper / sdf.front_window_days * 7, color=cm(0), alpha=fill_alpha)\n",
    "\n",
    "        ax.plot(sdf.front_window_days / 7, sdf.diff_ols / sdf.front_window_days * 7, marker='.', label='OLS', color=cm(1), linestyle='--')\n",
    "        ax.fill_between(sdf.front_window_days / 7, sdf.diff_ols_lower / sdf.front_window_days * 7, sdf.diff_ols_upper / sdf.front_window_days * 7, color=cm(1), alpha=fill_alpha)\n",
    "\n",
    "        ax.plot(sdf.front_window_days / 7, sdf.diff_dr / sdf.front_window_days * 7, marker='.', label='DR', color=cm(2), linestyle=':')\n",
    "        ax.fill_between(sdf.front_window_days / 7, sdf.diff_dr_lower / sdf.front_window_days * 7, sdf.diff_dr_upper / sdf.front_window_days * 7, color=cm(2), alpha=fill_alpha)\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "        if row == len(outcomes) - 1:\n",
    "            ax.set_xlabel(f\"Weeks since clicked ({'vs Pseudo-Control' if time_period == 'clicked' else 'vs Non-Clicked'})\", fontsize=8)\n",
    "            \n",
    "            ax.set_xticks(np.arange(1, 14))\n",
    "            #plt.axis('off')\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "            ax.margins(0,0)\n",
    "            ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        \n",
    "        if col == 0:\n",
    "            ax.set_ylabel(f\"Excess weekly\\n{pretty_name_map[outcome]}\", fontsize=7)\n",
    "#         if row == 0:\n",
    "#             ax.set_title(f\"{'Clicked vs Pseudo-Control' if time_period == 'clicked' else 'Clicked vs Non-Clicked'}\")\n",
    "\n",
    "        #ax.set_title(f\"{pretty_name_map[outcome]} {'after the study' if time_period == 'poststudy' else 'during the study'}\", fontsize=8)\n",
    "        #ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.2, hspace=0.05)\n",
    "\n",
    "bbox = matplotlib.transforms.Bbox.from_bounds(0,0,5.6,8)\n",
    "image_shortfilename = f\"recced_site_outcomes_all.pdf\"\n",
    "image_filename = os.path.join(figures_dir, image_shortfilename)\n",
    "print(image_filename)\n",
    "fig.savefig(image_filename, format='pdf', dpi=200, pad_inches=0, bbox_inches='tight') #, transparent=True)\n",
    "plt.show()\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = [\n",
    "    'n_updates_poststudy', \n",
    "    'n_first_visits_poststudy', \n",
    "    'n_users_repeat_visited_poststudy', \n",
    "    'n_users_interactedwith_poststudy', \n",
    "    'n_interactions_poststudy', \n",
    "    'n_days_visited_poststudy',\n",
    "    'n_sitewide_sites_intereactedwith_poststudy'\n",
    "]\n",
    "pretty_name_map = {\n",
    "    'n_updates_poststudy': \"Journal updates\",\n",
    "    'n_first_visits_poststudy': \"Peer site visits\",\n",
    "    'n_users_repeat_visited_poststudy': \"Repeat user visits\",\n",
    "    'n_users_interactedwith_poststudy': \"Peer initiations\", \n",
    "    'n_interactions_poststudy': \"Peer site interactions\", \n",
    "    'n_days_visited_poststudy': \"# days visiting peers\",\n",
    "    'n_sitewide_sites_intereactedwith_poststudy': \"Site author site initiations\",\n",
    "}\n",
    "fig, axes = plt.subplots(len(outcomes), 2, figsize=(10, 22))\n",
    "\n",
    "for time_period, col in zip(['study', 'poststudy'], [0, 1]):\n",
    "    for row, outcome in enumerate(outcomes):\n",
    "        ax = axes[row, col]\n",
    "        sdf = diff_df[(diff_df.outcome == outcome)&(diff_df.time_period==time_period)]\n",
    "\n",
    "        ax.axhline(0.0, color='black', linestyle='--')\n",
    "        ax.axvline(5, color='gray', linestyle='-', alpha=0.5)\n",
    "        if time_period == 'study':\n",
    "            ax.axvline(82 / 7, color='darkgray', linestyle=':', alpha=0.5, label='End of study')\n",
    "\n",
    "        fill_alpha = 0.05\n",
    "        ax.plot(sdf.front_window_days / 7, sdf.diff_raw / sdf.front_window_days * 7, marker='.', label='Raw', color='blue')\n",
    "        ax.fill_between(sdf.front_window_days / 7, sdf.diff_raw_lower / sdf.front_window_days * 7, sdf.diff_raw_upper / sdf.front_window_days * 7, color='blue', alpha=fill_alpha)\n",
    "\n",
    "        ax.plot(sdf.front_window_days / 7, sdf.diff_ols / sdf.front_window_days * 7, marker='.', label='OLS', color='orange')\n",
    "        ax.fill_between(sdf.front_window_days / 7, sdf.diff_ols_lower / sdf.front_window_days * 7, sdf.diff_ols_upper / sdf.front_window_days * 7, color='orange', alpha=fill_alpha)\n",
    "\n",
    "        ax.plot(sdf.front_window_days / 7, sdf.diff_dr / sdf.front_window_days * 7, marker='.', label='DR', color='green')\n",
    "        ax.fill_between(sdf.front_window_days / 7, sdf.diff_dr_lower / sdf.front_window_days * 7, sdf.diff_dr_upper / sdf.front_window_days * 7, color='green', alpha=fill_alpha)\n",
    "\n",
    "        ax.set_xlabel(f\"Time since {'end' if time_period == 'poststudy' else 'start'} of study (weeks)\")\n",
    "        ax.set_ylabel(\"Excess weekly actions\")\n",
    "        ax.set_title(f\"{pretty_name_map[outcome]} {'after the study' if time_period == 'poststudy' else 'during the study'} \")\n",
    "        ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the first version, with 13 weeks and equal matching \n",
    "diff_df.to_feather(\"rec_diff_df_20220524.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf[sdf.front_window_days > 35].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the rec_df with associated click data\n",
    "participant_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/participant'\n",
    "click_rec_df = pd.read_feather(os.path.join(participant_data_dir, 'click_rec_df.feather'))\n",
    "len(click_rec_df), click_rec_df.was_clicked.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shared-conda)",
   "language": "python",
   "name": "shared-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
