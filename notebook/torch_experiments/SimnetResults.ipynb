{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simnet Results\n",
    "===\n",
    "\n",
    "A copy of the ZachEval notebook with the simnet results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import dateutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging; only run this cell once\n",
    "import logging\n",
    "use_cbrec_logging = True\n",
    "if not use_cbrec_logging:\n",
    "    # this is a demo of how to set up logging\n",
    "    # since we use cbrec logging below, this will be done for us when we call set_up_logging.\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.DEBUG)\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    root.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cbrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.genconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config, which is needed by lots of the components for resolving paths, etc.\n",
    "config = cbrec.genconfig.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.evaluation\n",
    "import cbrec.reccontext\n",
    "import cbrec.featuredb\n",
    "import cbrec.torchmodel\n",
    "import cbrec.utils\n",
    "import cbrec.logutils\n",
    "import cbrec.feature_loader\n",
    "import cbrec.modeling\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbrec.logutils.set_up_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off matplotlib logging\n",
    "# which can be quite verbose and usually is not useful\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/simnet_20220608203014/outputs/\"\n",
    "assert os.path.exists(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_model_filepaths(model_dir):\n",
    "    logger = logging.getLogger(\"cbrec.modeling.submitEvalFromDirectory.identify_model_filepaths\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        raise ValueError(f\"Dir '{model_dir}' does not exist.\")\n",
    "    model_filepaths = []\n",
    "    for model_filepath in glob(os.path.join(model_dir, '*.json')):\n",
    "        model_filepaths.append(model_filepath)\n",
    "    if len(model_filepaths) == 0:\n",
    "        raise ValueError(f\"No .json files in dir '{model_dir}'.\")\n",
    "    logger.info(f\"Identified {len(model_filepaths)} model filepaths in dir {model_dir}.\")\n",
    "    return model_filepaths\n",
    "\n",
    "\n",
    "class ModelEval:\n",
    "    def __init__(self, model_output_dir):\n",
    "        self.logger = logging.getLogger('eval.ModelEval')\n",
    "        self.model_output_dir = model_output_dir\n",
    "        self.model_filepaths = self.identify_model_filepaths()\n",
    "        \n",
    "        self.models = {}\n",
    "        \n",
    "        \n",
    "    def identify_model_filepaths(self):\n",
    "        if not os.path.exists(self.model_output_dir):\n",
    "            raise ValueError(f\"Dir '{self.model_output_dir}' does not exist.\")\n",
    "        model_filepaths = []\n",
    "        for model_filepath in glob(os.path.join(self.model_output_dir, '*.json')):\n",
    "            model_filepaths.append(model_filepath)\n",
    "        if len(model_filepaths) == 0:\n",
    "            raise ValueError(f\"No .json files in dir '{model_dir}'.\")\n",
    "        self.logger.info(f\"Identified {len(model_filepaths)} model filepaths in dir {self.model_output_dir}.\")\n",
    "        return model_filepaths\n",
    "\n",
    "        \n",
    "    def create_managers(self):\n",
    "        self.managers = []\n",
    "        for model_filepath in model_filepaths:\n",
    "            manager = cbrec.modeling.manager.ModelManager.load_from_filepath(model_filepath)\n",
    "            self.managers.append(manager)\n",
    "            \n",
    "            self.models[manager.model_config.output_name] = {}\n",
    "            \n",
    "            \n",
    "    def create_test_metrics(self):\n",
    "        for manager in self.managers:\n",
    "            manager.load_model(load_preprocessor=False, load_model_state_dict=False, load_training_metrics=True)\n",
    "            self.models[manager.model_config.output_name]['train_metrics'] = manager.model_trainer.train_metrics\n",
    "            self.models[manager.model_config.output_name]['test_metrics'] = manager.model_trainer.test_metrics\n",
    "\n",
    "    def get_scores(self):\n",
    "        for manager in self.managers:\n",
    "            #metadata_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_test_metadata.ndjson')\n",
    "            scores_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_coverage_scores.pkl')\n",
    "            #assert os.path.exists(metadata_filepath)\n",
    "            assert os.path.exists(scores_filepath)\n",
    "            \n",
    "            with open(scores_filepath, 'rb') as scores_infile:\n",
    "                scores = pickle.load(scores_infile)\n",
    "            self.models[manager.model_config.output_name]['coverage_scores'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = ModelEval(output_dir)\n",
    "ev.create_managers()\n",
    "ev.create_test_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final validation loss and accuracy after training for all loaded models\n",
    "print(f\"{'Model':>35} ValLoss ValAcc\")\n",
    "print(\"=\"*60)\n",
    "for model_name in ev.models.keys():\n",
    "    final_validation_loss = ev.models[model_name]['test_metrics'][1, -1]\n",
    "    final_validation_acc = ev.models[model_name]['test_metrics'][2, -1]\n",
    "    print(f\"{model_name:>35}  {final_validation_loss:.4f} {final_validation_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.get_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.models['simnet_all']['coverage_scores'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_END_TIMESTAMP = datetime.strptime(\"2021-07-01\", \"%Y-%m-%d\").timestamp() * 1000\n",
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test' or md['type'] == 'predict']\n",
    "test_md_list = [md for md in md_list if md['has_target'] and md['timestamp'] > VALIDATION_END_TIMESTAMP]\n",
    "len(test_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata_ids = set([md['metadata_id'] for md in test_md_list])\n",
    "len(test_metadata_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ranks = True\n",
    "for manager in ev.managers:\n",
    "    metadata_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_test_metadata.ndjson')\n",
    "    assert os.path.exists(metadata_filepath)\n",
    "    target_ranks = []\n",
    "    with open(metadata_filepath, 'r') as metadata_file:\n",
    "        for line in tqdm(metadata_file, total=len(test_md_list) + 1000, desc=f'Reading metrics {manager.model_config.output_name}'):\n",
    "            md = json.loads(line)\n",
    "            if md['metadata_id'] not in test_metadata_ids:\n",
    "                continue\n",
    "            metrics = md[manager.model_config.output_name + \"_metrics\"]\n",
    "            target_rank = metrics['target_rank']\n",
    "            target_ranks.append(target_rank)\n",
    "            \n",
    "    target_ranks = np.array(target_ranks)\n",
    "    mrr = (1 / target_ranks).mean()\n",
    "    hr1 = (target_ranks == 1).sum() / len(target_ranks) * 100\n",
    "    hr5 = (target_ranks <= 5).sum() / len(target_ranks) * 100\n",
    "    ev.models[manager.model_config.output_name]['metrics'] = {\n",
    "        'mrr': mrr,\n",
    "        'hr1': hr1,\n",
    "        'hr5': hr5,\n",
    "    }\n",
    "    if save_ranks:\n",
    "        ev.models[manager.model_config.output_name]['metrics']['ranks'] = target_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{'model_name': model_name, **ev.models[model_name]['metrics']} for model_name in ev.models.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: Create the needed coverage data\n",
    "\n",
    "Based on the sites available at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageHelper:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test' or md['type'] == 'predict']\n",
    "len(md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_md_list = [md for md in md_list if not md['has_target']]\n",
    "len(coverage_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coverage_predictions(config, coverage_md_list):\n",
    "    db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "    with db:\n",
    "        coverage_rcs = []\n",
    "        for test_context_md in tqdm(coverage_md_list, desc=\"Loading coverage data\"):\n",
    "            test_context = cbrec.featuredb.get_test_context_by_metadata_id(db, test_context_md['metadata_id'], config)\n",
    "            rc = cbrec.reccontext.RecContext.create_from_test_context(config, test_context_md, test_context)\n",
    "            coverage_rcs.append(rc)\n",
    "    return coverage_rcs\n",
    "\n",
    "\n",
    "cov_helper = CoverageHelper()\n",
    "    \n",
    "coverage_rcs = load_coverage_predictions(config, coverage_md_list)\n",
    "assert len(coverage_rcs) == 1000\n",
    "\n",
    "coverage_sites = set()\n",
    "for coverage_rc in coverage_rcs:\n",
    "    coverage_sites.update(set(coverage_rc.candidate_usp_arr[:,1]))\n",
    "coverage_sites = sorted(list(coverage_sites))\n",
    "print(f\"# eligible coverage sites: {len(coverage_sites)}\")\n",
    "cov_helper.coverage_sites = coverage_sites\n",
    "\n",
    "eligible_sites = set(coverage_sites)\n",
    "len(eligible_sites)\n",
    "cov_helper.eligible_sites = eligible_sites\n",
    "\n",
    "site_id_arr_map = {}\n",
    "for coverage_rc in coverage_rcs:\n",
    "    site_id_arr, _ = np.unique(coverage_rc.candidate_usp_arr[:,1], return_index=True)\n",
    "    assert len(site_id_arr) <= len(coverage_sites)\n",
    "    site_id_arr_map[coverage_rc.metadata_id] = site_id_arr\n",
    "cov_helper.site_id_arr_map = site_id_arr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_helper.timestamp = 1609502404437  # this is the timestamp when recommendations were generated for coverage\n",
    "assert cov_helper.timestamp == coverage_rc.timestamp\n",
    "datetime.utcfromtimestamp(cov_helper.timestamp/1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal metadata\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read interactions dataframe\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "ints_df = pd.read_feather(os.path.join(model_data_dir, 'ints_df.feather'))\n",
    "print(f\"Read {len(ints_df)} rows ({len(set(ints_df.user_id))} unique users) in {datetime.now() - s}.\")\n",
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_usp_set = set([(row.user_id, row.site_id) for row in journal_df.itertuples()])\n",
    "len(author_usp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df = ints_df.sort_values(by='created_at').drop_duplicates(subset=['user_id', 'site_id'], keep='first').copy()\n",
    "len(inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df['usp'] = [(row.user_id, row.site_id) for row in inits_df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df = inits_df[~inits_df.usp.isin(author_usp_set)]\n",
    "len(inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df = inits_df[inits_df.created_at < cov_helper.timestamp]\n",
    "len(inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_int_site_ids = set(inits_df.site_id)\n",
    "len(previous_int_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_with_previous_ints = previous_int_site_ids & cov_helper.eligible_sites\n",
    "len(sites_with_previous_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_helper.sites_with_previous_ints = sites_with_previous_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"num eligible sites with > 0 indegree: {len(sites_with_previous_ints)}\")\n",
    "print(f\"num eligible sites: {len(eligible_sites)}\")\n",
    "print(f\"pct > 0 indegree: {len(sites_with_previous_ints) / len(eligible_sites):.3%}\")\n",
    "print(f\"pct zero indegree: {1 - (len(sites_with_previous_ints) / len(eligible_sites)):.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = journal_df[(journal_df.published_at.notna())&(journal_df.published_at > 0)].sort_values(by='published_at').drop_duplicates(subset='site_id', keep='first')\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_first_journal_timestamp_map = sdf[sdf.site_id.isin(eligible_sites)].set_index('site_id').created_at.to_dict()\n",
    "len(site_first_journal_timestamp_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.array([cov_helper.timestamp - site_first_journal_timestamp_map[site_id] for site_id in coverage_sites])\n",
    "ages = ages / 1000 / 60 / 60 / 24 / 7  # convert to weeks\n",
    "len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median eligible site has been around for 93 weeks\n",
    "ages.min(), ages.mean(), ages.std(), np.median(ages), ages.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_helper.site_first_journal_timestamp_map = site_first_journal_timestamp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_helper.n = 5  # number of recs to make in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cov_helper to pickle\n",
    "coverage_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/feature_data/coverage\"\n",
    "with open(os.path.join(coverage_dir, 'cov_helper.pkl'), 'wb') as coverage_helper_file:\n",
    "    pickle.dump(cov_helper, coverage_helper_file)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make coverage predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cov_helper from pickle\n",
    "coverage_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/feature_data/coverage\"\n",
    "with open(os.path.join(coverage_dir, 'cov_helper.pkl'), 'rb') as coverage_helper_file:\n",
    "    cov_helper = pickle.load(coverage_helper_file)\n",
    "cov_helper.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coverage_metrics(model_coverage_scores, cov_helper):\n",
    "    recs = []\n",
    "    for scores_md in model_coverage_scores:\n",
    "        metadata_id = scores_md['metadata_id']\n",
    "        y_score_site = scores_md['scores']\n",
    "        site_id_arr = cov_helper.site_id_arr_map[metadata_id]\n",
    "        assert y_score_site.shape == site_id_arr.shape\n",
    "\n",
    "        # create rec batch\n",
    "        sort_inds = np.argsort(y_score_site)\n",
    "        # TODO need to compute ranks if there are ties; for now, we'll assume there aren't any ties\n",
    "        # in the case of ties, not clear what order argsort prefers\n",
    "        #ranks = rankdata(-1 * y_score_site, method='max')\n",
    "\n",
    "        #highest_scores = y_score_site[sort_inds[-cov_helper.n:]]\n",
    "        highest_score_site_ids = site_id_arr[sort_inds[-cov_helper.n:]]\n",
    "        recs.append(list(highest_score_site_ids))\n",
    "        \n",
    "    recced_sites = set()\n",
    "    for rec in recs:\n",
    "        recced_sites.update(rec)\n",
    "    nonrecced_sites = cov_helper.eligible_sites - recced_sites\n",
    "    \n",
    "    recced_inted = len(recced_sites & cov_helper.sites_with_previous_ints) / len(recced_sites)\n",
    "    nonrecced_inted = len(nonrecced_sites & cov_helper.sites_with_previous_ints) / len(nonrecced_sites)\n",
    "    \n",
    "    site_ages = []\n",
    "    for rec in recs:\n",
    "        ages = np.array([cov_helper.timestamp - cov_helper.site_first_journal_timestamp_map[site_id] for site_id in rec])\n",
    "        ages = ages / 1000 / 60 / 60 / 24 / 7  # convert to weeks\n",
    "        assert np.all(ages > 0)\n",
    "        site_ages.append({\n",
    "            'min': ages.min(),\n",
    "            #'mean': ages.mean(),\n",
    "            #'std': ages.std(),\n",
    "            'median': np.median(ages),\n",
    "            #'max': ages.max(),\n",
    "        })\n",
    "    mean_min_age = np.mean([a['min'] for a in site_ages])\n",
    "    mean_median_age = np.mean([a['median'] for a in site_ages])\n",
    "    \n",
    "    return {\n",
    "        'n_recced_sites': len(recced_sites),\n",
    "        'n_nonrecced_sites': len(nonrecced_sites),\n",
    "        'pct_eligible_recced': len(recced_sites) / len(eligible_sites),\n",
    "        'pct_unique_recs': len(recced_sites) / (5 * 1000),\n",
    "        'pct_recced_with_int': recced_inted,\n",
    "        'pct_nonrecced_with_int': nonrecced_inted,\n",
    "        'pct_recced_without_int': 1 - recced_inted,\n",
    "        'pct_nonrecced_without_int': 1 - nonrecced_inted,\n",
    "        'ratio_int': recced_inted / nonrecced_inted,\n",
    "        'ratio_noint': (1 - recced_inted) / (1 - nonrecced_inted),\n",
    "        'mean_min_age': mean_min_age,\n",
    "        'mean_median_age': mean_median_age,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_metrics_list = []\n",
    "for model_name in ev.models.keys():\n",
    "    model_coverage_scores = ev.models[model_name]['coverage_scores']\n",
    "    coverage_metrics = compute_coverage_metrics(model_coverage_scores, cov_helper)\n",
    "    coverage_metrics_list.append({\n",
    "        'model_name': model_name,\n",
    "        **coverage_metrics\n",
    "    })\n",
    "pd.DataFrame(coverage_metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpuonly",
   "language": "python",
   "name": "pytorch-cpuonly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
