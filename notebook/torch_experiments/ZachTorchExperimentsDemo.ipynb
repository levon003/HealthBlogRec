{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zach's Torch Experiments Demo\n",
    "===\n",
    "\n",
    "Notes from Sept 28 and Oct 1, 2021 meetings about using PyTorch and loading feature data.\n",
    "\n",
    "What we did:\n",
    " - Installed the pytorch-cpuonly kernel so that we could use it with Jupyter.\n",
    " - Imported some stuff from the notebook/eval/PytorchTraining notebook\n",
    " - Loaded feature data from a directory\n",
    " - Used that feature data to train a model\n",
    " \n",
    " \n",
    "## Evaluation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#metrics-and-scoring-quantifying-the-quality-of-predictions\n",
    "\n",
    "## Neural Net training tips\n",
    "\n",
    "Training neural nets (although some of this is vision-specific): \n",
    "http://karpathy.github.io/2019/04/25/recipe/\n",
    "\n",
    "https://twitter.com/jmhessel/status/1111715093404884992?s=21\n",
    "\n",
    "## Random Q&A\n",
    "\n",
    " - How are texts tokenized? https://huggingface.co/transformers/tokenizer_summary.html#byte-pair-encoding\n",
    " - How can we combine word embeddings? https://arxiv.org/abs/1805.09843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging; only run this cell once\n",
    "import logging\n",
    "use_cbrec_logging = True\n",
    "if not use_cbrec_logging:\n",
    "    # this is a demo of how to set up logging\n",
    "    # since we use cbrec logging below, this will be done for us when we call set_up_logging.\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.DEBUG)\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    root.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cbcore\n",
    "\n",
    "Only necessary for paths and some utility functions, so you may not need this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)\n",
    "import cbcore.data.paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cbrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.genconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config, which is needed by lots of the components for resolving paths, etc.\n",
    "config = cbrec.genconfig.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.evaluation\n",
    "import cbrec.reccontext\n",
    "import cbrec.featuredb\n",
    "import cbrec.torchmodel\n",
    "import cbrec.utils\n",
    "import cbrec.logutils\n",
    "import cbrec.feature_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbrec.logutils.set_up_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off matplotlib logging\n",
    "# which can be quite verbose and usually is not useful\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train features\n",
    "feature_cache_dir = os.path.join(config.torch_experiments_dir, 'feature_cache')\n",
    "filenames = [\n",
    "    ('X_train_raw.pkl', 'y_train_raw.pkl'),\n",
    "    ('X_test2train_raw.pkl', 'y_test2train_raw.pkl'),\n",
    "]\n",
    "\n",
    "def get_features(x_filename, y_filename):\n",
    "    with open(os.path.join(feature_cache_dir, x_filename), 'rb') as infile:\n",
    "        X = pickle.load(infile)\n",
    "    with open(os.path.join(feature_cache_dir, y_filename), 'rb') as infile:\n",
    "        y = pickle.load(infile)\n",
    "    return X, y\n",
    "\n",
    "x_filename, y_filename = filenames[0]\n",
    "X_train, y_train = get_features(x_filename, y_filename)\n",
    "    \n",
    "x_filename, y_filename = filenames[1]\n",
    "X_test, y_test = get_features(x_filename, y_filename)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature details\n",
    "\n",
    "Why is each row 1563?\n",
    "\n",
    "Each row represents TWO USPs.\n",
    "Each USP is composed of activity features (9), network features (3), and text features (768).\n",
    "In addition, the 2 USPs have SHARED features. (3)\n",
    "\n",
    "The two USPs are:\n",
    " - the SOURCE\n",
    " - the CANDIDATE\n",
    " \n",
    "For the non-text features, you can find the code that generates them in cbrec.feature_extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(9 + 3 + 768) * 2 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half of the training data is 1s, the other half is 0s\n",
    "np.sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "inds = np.arange(len(X_train))\n",
    "np.random.shuffle(inds)\n",
    "X_train = X_train[inds]\n",
    "y_train = y_train[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_train, axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data to speed up convergence\n",
    "import sklearn.preprocessing\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_train, axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple neural net with 2 hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden, dropout_p=0.2):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # note: 768 is the size of the roBERTa outputs\n",
    "        self.fc1 = nn.Linear(n_input, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, 1, bias=False)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_p)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)  # note: not using F.sigmoid here, as the loss used includes the Sigmoid transformation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"notebook.ZachTorchExperimentsDemo\")\n",
    "    \n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n",
    "\n",
    "verbose = True\n",
    "n_hidden = 100\n",
    "n_epochs = 100\n",
    "lr_init = 0.01\n",
    "max_lr = 0.02  # 0.0155\n",
    "dropout_p = 0.1\n",
    "minibatch_size = len(y_train)\n",
    "minibatch_size = min(n_train, minibatch_size)  # if minibatch_size is larger than n_train, force it to n_train\n",
    "n_minibatches = int(np.ceil(n_train / minibatch_size))\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "# note: input dim is 27 for non-text features + 768 for text features\n",
    "net = LinearNet(n_input, n_hidden, dropout_p)\n",
    "\n",
    "#optimizer = optim.SGD(net.parameters(), lr=lr_init, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr_init)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    steps_per_epoch=n_minibatches,\n",
    "    epochs=n_epochs,\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # pointwise loss function\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)  # make labels 2-dimensional\n",
    "y_train_tensor = y_train_tensor.type_as(X_train_tensor)\n",
    "if verbose:\n",
    "    logger.info(f\"Input tensor sizes: {X_train_tensor.size()}, {y_train_tensor.size()}\")\n",
    "\n",
    "net.train()\n",
    "for epoch in range(n_epochs):\n",
    "    s = datetime.now()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # shuffle the training data\n",
    "    # I am not sure if this matters at all\n",
    "    epoch_order = torch.randperm(n_train)\n",
    "\n",
    "    mb_metrics = []  # store the minibatch_metrics, then average after\n",
    "    for minibatch in range(n_minibatches):\n",
    "        minibatch_start = minibatch * minibatch_size\n",
    "        minibatch_end = min(minibatch_start + minibatch_size, n_train)\n",
    "        if verbose and epoch == 0:\n",
    "            logger.info(f\"    Minibatch for inds in {minibatch_start} - {minibatch_end}.\")\n",
    "        minibatch_inds = epoch_order[minibatch_start:minibatch_end]\n",
    "\n",
    "        inputs = X_train_tensor[minibatch_inds]\n",
    "        labels = y_train_tensor[minibatch_inds]\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # compute and log the loss\n",
    "        y_train_pred = torch.sigmoid(outputs.detach()).view((-1,)).numpy()\n",
    "        y_train_pred = (y_train_pred >= 0.5).astype(int)  # binarize predictions with a 0.5 decision boundary\n",
    "        y_train_minibatch = y_train[minibatch_inds.numpy()]\n",
    "        acc = np.sum(y_train_pred == y_train_minibatch) / len(y_train_minibatch)\n",
    "\n",
    "        mb_metrics.append((loss.item(), acc))\n",
    "    loss, acc = np.mean(np.array(mb_metrics), axis=0)\n",
    "\n",
    "    should_stop_early = loss < 0.001\n",
    "    if verbose and (epoch < 5 or epoch == n_epochs - 1 or epoch % 10 == 0 or should_stop_early):\n",
    "        # TODO we should compute loss and accuracy based on the validation set here\n",
    "        logger.info(f\"{epoch:>3} ({datetime.now() - s}): loss={loss:.4f} accuracy={acc*100:.2f}% LR={optimizer.param_groups[0]['lr']:.2E}\")\n",
    "    if should_stop_early:\n",
    "        break\n",
    "# this is a hack, but we store training results info back through the learner_config dictionary\n",
    "final_train_loss = loss\n",
    "final_epoch_count = epoch + 1\n",
    "if verbose:\n",
    "    logger.info(f\"Completed {final_epoch_count} epochs with a final train loss of {final_train_loss:.4f}.\")\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.from_numpy(X_test)\n",
    "    outputs = net(X_test_tensor)\n",
    "    y_test_pred = torch.sigmoid(outputs.detach()).view((-1,)).numpy()\n",
    "    y_test_pred = (y_test_pred >= 0.5).astype(int)\n",
    "    acc = np.sum(y_test_pred == y_test) / len(y_test)\n",
    "    logger.info(f\"Test acc: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_cache_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'torch_experiments', 'model_cache')\n",
    "torch.save(net.state_dict(), os.path.join(model_cache_dir, 'ZachTestNet.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test RecContexts\n",
    "\n",
    "## Create RecContexts from scratch\n",
    "\n",
    "None of this code needs to be run; you can just skip to the next section, which loads from the pickle file containing the instantiated RecContext objects.\n",
    "\n",
    "Note that the FULL RecContext file is 117 GB!  That's why you should load the random set of 2000 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test']\n",
    "len(test_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = cbrec.feature_loader.FeatureLoader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.text.embeddingdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_embedding_map = {}\n",
    "\n",
    "config.text_feature_db_filepath = os.path.join(config.feature_data_dir, 'test_text_feature.sqlite')\n",
    "db = cbrec.text.embeddingdb.get_text_feature_db(config)\n",
    "with db:\n",
    "    for text in tqdm(cbrec.text.embeddingdb.stream_text_features(db), total=998905):\n",
    "        journal_id = text['text_id']\n",
    "        journal_embedding_map[journal_id] = text['feature_arr']\n",
    "len(journal_embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_map = {md['metadata_id']: md for md in test_md_list}\n",
    "\n",
    "required_journal_ids = set()\n",
    "n_invalid = 0\n",
    "n_error = 0\n",
    "\n",
    "db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "with db:\n",
    "    for test_context in tqdm(cbrec.featuredb.stream_test_contexts(db, config), desc='Streaming test contexts', total=32612):\n",
    "        test_context_md = test_md_map[test_context['metadata_id']]\n",
    "        interaction_timestamp = int(test_context_md['timestamp'])\n",
    "        source_usp_arr = test_context['source_usp_arr']\n",
    "        source_usps = [(source_usp_arr[i,0], source_usp_arr[i,1]) for i in range(source_usp_arr.shape[0])]\n",
    "        candidate_usp_arr = test_context['candidate_usp_arr']\n",
    "        candidate_usps = [(candidate_usp_arr[i,0], candidate_usp_arr[i,1]) for i in range(candidate_usp_arr.shape[0])]\n",
    "        error = False\n",
    "        for usp in source_usps + candidate_usps:\n",
    "            journal_ids = fl.journal_id_lookup.get_journal_updates_before(usp, interaction_timestamp)\n",
    "            if len(journal_ids) < 3:\n",
    "                n_invalid += 1\n",
    "                error = True\n",
    "            else:\n",
    "                required_journal_ids.update(journal_ids)\n",
    "        if error:\n",
    "            n_error += 1\n",
    "len(required_journal_ids), n_error, n_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_journal_ids_filepath = os.path.join(config.model_data_dir, 'test_journal_oids.txt')\n",
    "with open(required_journal_ids_filepath, 'w') as outfile:\n",
    "    for journal_oid in required_journal_ids:\n",
    "        outfile.write(journal_oid + \"\\n\")\n",
    "logging.info(f\"Wrote {len(required_journal_ids)} journal ids to '{required_journal_ids_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_map = {md['metadata_id']: md for md in test_md_list}\n",
    "\n",
    "db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "with db:\n",
    "    for test_context in tqdm(cbrec.featuredb.stream_test_contexts(db, config), desc='Streaming test contexts'):\n",
    "        test_context_md = test_md_map[test_context['metadata_id']]\n",
    "        rc = cbrec.reccontext.RecContext.create_from_test_context(config, md, test_context)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(13)\n",
    "subset_md_list = rng.choice(test_md_list, size=1000, replace=False)\n",
    "len(subset_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_list = []\n",
    "db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "with db:\n",
    "    for md in tqdm(subset_md_list, desc=\"Creating test RecContexts\"):\n",
    "        metadata_id = md['metadata_id']\n",
    "        test_context = cbrec.featuredb.get_test_context_by_metadata_id(db, metadata_id, config)\n",
    "        rc = cbrec.reccontext.RecContext.create_from_test_context(config, md, test_context)\n",
    "        rc_list.append(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = cbrec.feature_loader.FeatureLoader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify how many journal ids are required to create the appropriate feature matrices WITH text data\n",
    "required_journal_ids = fl.identify_required_journal_ids(subset_md_list)\n",
    "len(required_journal_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test equivalence with previously written required journals\n",
    "# note: just makes sure required_journal_ids are in test1000_required_journal_oids.txt, not if we are missing some ids in required_journal_ids\n",
    "with open(os.path.join(config.model_data_dir, 'test1000_required_journal_oids.txt'), 'r') as infile:\n",
    "    for line in infile:\n",
    "        journal_id = line.strip()\n",
    "        if journal_id != \"\":\n",
    "            assert journal_id in required_journal_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config.model_data_dir, 'test1000_required_journal_oids.txt'), 'w') as outfile:\n",
    "    for journal_id in required_journal_ids:\n",
    "        outfile.write(journal_id + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the feature database:\n",
    "\n",
    "    sbatch -p amdsmall make_text_features_test.sh\n",
    "\n",
    "Which runs: \n",
    "\n",
    "    python cbrec/text/createTextFeatureSqlite.py --text-id-txt /home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data/test1000_required_journal_oids.txt --text-feature-db-filename test_text_feature.sqlite --n-processes 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = rc_list[0]\n",
    "rc.target_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.candidate_usp_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the target USP, from within the list of candidates\n",
    "rc.candidate_usp_arr[rc.target_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this takes almost 2 hours!\n",
    "# it also uses a ton of RAM\n",
    "# load from the pickle instead\n",
    "for rc in tqdm(rc_list, desc='Creating feature matrices'):\n",
    "    arrs = []\n",
    "    for i in range(len(rc.source_usp_mat)):\n",
    "        source_feature_arr = rc.source_usp_mat[i,:]\n",
    "        for j in range(len(rc.candidate_usp_mat)):\n",
    "            candidate_feature_arr = rc.candidate_usp_mat[j,:]\n",
    "\n",
    "            ind = (i * len(rc.candidate_usp_arr)) + j\n",
    "            source_candidate_feature_arr = rc.user_pair_mat[ind,:]\n",
    "\n",
    "            arr = np.concatenate([source_feature_arr, candidate_feature_arr, source_candidate_feature_arr])\n",
    "            arrs.append(arr)\n",
    "    X = np.vstack(arrs)\n",
    "    rc.X_test = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rc_list to pickle\n",
    "s = datetime.now()\n",
    "feature_cache_dir = os.path.join(config.torch_experiments_dir, 'feature_cache')\n",
    "with open(os.path.join(feature_cache_dir, 'rc_test_notext.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(rc_list, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Saved {len(rc_list)} to pickle in {datetime.now() - s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save subset of rc_list to pickle\n",
    "rng = np.random.default_rng(12)\n",
    "subset_rc_list = rng.choice(rc_list, size=2000, replace=False)\n",
    "\n",
    "s = datetime.now()\n",
    "feature_cache_dir = os.path.join(config.torch_experiments_dir, 'feature_cache')\n",
    "with open(os.path.join(feature_cache_dir, 'rc_test_notext_2000.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(subset_rc_list, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Saved {len(subset_rc_list)} to pickle in {datetime.now() - s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl.config.text_feature_db_filepath = os.path.join(fl.config.feature_data_dir, 'test_text_feature.sqlite')\n",
    "fl.config.text_feature_db_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rc in tqdm(rc_list):\n",
    "    rc.X_test = fl.get_input_matrix_from_test_context(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = cbrec.data.DataManager(config, load_ints=False, load_journals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = dm.get_filtered_journals().sort_values(by=['user_id', 'site_id', 'published_at'])\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = rc_list[0].timestamp\n",
    "usp = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = rc_list[0]\n",
    "int_created_at = int(rc.timestamp)\n",
    "int_user_id = rc.source_user_id\n",
    "int_site_id = rc.target_site_id\n",
    "int_created_at, int_user_id, int_site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df[(journal_df.user_id == usp[0])&(journal_df.site_id == usp[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df[journal_df.site_id == usp[1]].user_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df[journal_df.user_id == usp[0]].site_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write a function to identify invalid usps... but more importantly, investigate why those usps are in the candidate list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_not_present = 0\n",
    "n_candidate_usps = 0\n",
    "for usp in rc.candidate_usp_arr:\n",
    "    usp = (usp[0], usp[1])\n",
    "    n_candidate_usps += 1 \n",
    "    if usp not in fl.journal_id_lookup.usp_journal_timestamp_map:\n",
    "        n_not_present += 1\n",
    "n_not_present, n_candidate_usps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert usp in fl.journal_id_lookup.usp_journal_timestamp_map\n",
    "fl.journal_id_lookup.get_journal_updates_before(usp, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the rc_list isn't already generated, can create it WITH text data using the following\n",
    "rc_list = fl.get_reccontexts_from_test_contexts(subset_md_list)\n",
    "len(rc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score and inspect scores for RecContexts\n",
    "\n",
    "Loads the subset of 2000 RecContexts from pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rc_list from pickle\n",
    "s = datetime.now()\n",
    "feature_cache_dir = os.path.join(config.torch_experiments_dir, 'feature_cache')\n",
    "with open(os.path.join(feature_cache_dir, 'rc_test_notext_2000.pkl'), 'rb') as infile:\n",
    "    rc_list = pickle.load(infile)\n",
    "print(f\"Loaded {len(rc_list)} RecContexts in {datetime.now() - s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rc in rc_list:\n",
    "    if len(rc.source_usp_arr) > 2:\n",
    "        print(rc.X_test.shape, len(rc.source_usp_arr), len(rc.candidate_usp_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the feature description above!\n",
    "rc.X_test[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.evaluation\n",
    "class CustomModelScorer(cbrec.evaluation.Scorer):\n",
    "    def __init__(self, config, test_context: cbrec.reccontext.RecContext, \n",
    "                 # TODO pass in a model object here, if appropriate\n",
    "                model_name=\"CustomModel\"):\n",
    "        super().__init__(config, test_context, coverage_tracker=None, save_scores=True)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def score(self):\n",
    "        \"\"\"\n",
    "        Score the RecContext.\n",
    "        \n",
    "        Use self.text_context to produce a y_score_site list, and return a dictionary of metrics.\n",
    "        \n",
    "        \"\"\"\n",
    "        X = self.test_context.X_test\n",
    "        #y_score = np.random.random(size=(X.shape[0]))  # random model\n",
    "        y_score = X[:,-1].astype(int)  # in the rc arrays, the last feature corresponds to \"is_reciprocal\"\n",
    "        \n",
    "        y_score_mat = self.get_empty_score_arr('full')\n",
    "        y_score_mat = y_score.reshape((y_score_mat.shape[1], y_score_mat.shape[0])).T\n",
    "\n",
    "        y_score_site = self.reduce_usp_ranking_to_site(self.merge_multisource_rankings(y_score_mat))\n",
    "        self.compute_metrics(y_score_site, model_name=self.model_name)\n",
    "        \n",
    "        return self.metrics_dict[self.model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = CustomModelScorer(config, rc_list[0])\n",
    "scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dicts = []\n",
    "for rc in tqdm(rc_list):\n",
    "    scorer = CustomModelScorer(config, rc)\n",
    "    metric_dict = scorer.score()\n",
    "    metric_dicts.append(metric_dict)\n",
    "len(metric_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the metrics into a Pandas dataframe for easier management\n",
    "df = pd.DataFrame(metric_dicts)\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute \"Mean Reciprocal Rank\"\n",
    "df['mrr'] = 1 / df.target_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a key model metric!\n",
    "df.mrr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.hist(df.target_raw_score)\n",
    "ax.set_title(\"Distribution of target scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "sdf = df[df.target_raw_score == 0]\n",
    "ax.hist(1 - (sdf.target_rank / sdf.n), bins=20)\n",
    "\n",
    "ax = axes[1]\n",
    "sdf = df[df.target_raw_score == 1]\n",
    "ax.hist(1 - (sdf.target_rank / sdf.n), bins=50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Inspecting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all of the objects defined on the test RecContexts\n",
    "[v for v in rc.__dir__() if not v.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a dataframe from the metadata that describes these RecContexts\n",
    "md_df = pd.DataFrame([rc.md for rc in rc_list])\n",
    "len(md_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_counts is a very useful function\n",
    "md_df.n_target_usps.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every RC has a target\n",
    "md_df.has_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are cases where the target user was not an active user\n",
    "md_df.test_target_usp_adjustment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can add new columns to the dataframe to aid in our analysis\n",
    "# for example, we add the length of target_inds\n",
    "n_target_inds = [len(rc.target_inds) for rc in rc_list]\n",
    "md_df['n_target_inds'] = n_target_inds\n",
    "md_df.n_target_inds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uh oh! this looks like a bug: target_inds should have the same length as n_target_usps\n",
    "# but it looks like it is incorrectly empty when there are multiple target_usps\n",
    "# for now, we'll have to just ignore those cases where n_target_inds == 0\n",
    "pd.crosstab(md_df.n_target_inds, md_df.n_target_usps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix that contains the features for all targets\n",
    "arrs = []\n",
    "for rc in rc_list:\n",
    "    if len(rc.target_inds) == 0:\n",
    "        continue\n",
    "    target_feature_arr = rc.candidate_usp_mat[rc.target_inds]  # shape: 12 x 1\n",
    "    source_target_feature_arr = rc.user_pair_mat[rc.target_inds]  # shape: 3 x 1\n",
    "    arr = np.concatenate([target_feature_arr.reshape(-1), source_target_feature_arr.reshape(-1)])\n",
    "    arrs.append(arr)\n",
    "X_target = np.vstack(arrs)\n",
    "X_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, we inspect the distributions of the features that are computed, in terms of their value for the TARGET usp\n",
    "fig, axes = plt.subplots(4, 4, figsize=(14, 12))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "\n",
    "# this is a custom list of feature names, which you may find useful to use\n",
    "usp_feature_ind2name_map = {\n",
    "    0: 'indegree',\n",
    "    1: 'outdegree',\n",
    "    2: 'component_size',\n",
    "    3: 'journal_count',\n",
    "    4: 'journal_time_to_most_recent',\n",
    "    5: 'amp_count',\n",
    "    6: 'amp_time_to_most_recent',\n",
    "    7: 'comment_count',\n",
    "    8: 'comment_time_to_most_recent',\n",
    "    9: 'guestbook_count',\n",
    "    10: 'guestbook_time_to_most_recent',\n",
    "    11: 'time_to_first_update',\n",
    "}\n",
    "pair_feature_ind2name_map = {\n",
    "    0: 'are_weakly_connected',\n",
    "    1: 'is_fof',\n",
    "    2: 'is_reciprocal',\n",
    "}\n",
    "\n",
    "for i in range(X_target.shape[1]):  # for each feature column....\n",
    "    x = X_target[:,i]\n",
    "    ax = axes[i]\n",
    "    ax.hist(x, bins=20, log=True)\n",
    "    if i in usp_feature_ind2name_map:\n",
    "        ax.set_title(usp_feature_ind2name_map[i])\n",
    "    else:\n",
    "        ax.set_title(pair_feature_ind2name_map[i - 12])\n",
    "    \n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Inspecting the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column order in the training data is source_feature_arr, candidate_feature_arr, source_candidate_feature_arr, source_text_arr, candidate_text_arr\n",
    "# column order in the test data (as defined in this notebook) is source_feature_arr, candidate_feature_arr, source_candidate_feature_arr\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick-and-dirty plot with Matplotlib, revealing that indegree follows a power law distribution\n",
    "plt.hist(X_train[:,0], log=True, bins=np.arange(0, 100))\n",
    "plt.title(\"Distribution of source user's indegree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more complex Matplotlib plot, using the Pythonic API\n",
    "# we compare targets and alts\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "ax.hist(X_train[y_train==1,12], log=True, bins=np.arange(0, 100), alpha=0.5, label='Target')\n",
    "ax.hist(X_train[y_train==0,12], log=True, bins=np.arange(0, 100), alpha=0.5, label='Alts')\n",
    "ax.set_title(\"Distribution of target and alt indegree\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.DataFrame(data={'y': y_train, 'indegree': X_train[:,12]})\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(sdf.y, (sdf.indegree > 0).rename('previously received initiation?'), normalize='index' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "ax.hist(X_train[y_train==1,13], log=True, bins=np.arange(0, 100), alpha=0.5, label='Target')\n",
    "ax.hist(X_train[y_train==0,13], log=True, bins=np.arange(0, 100), alpha=0.5, label='Alts')\n",
    "ax.set_title(\"Distribution of target and alt outdegree\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.DataFrame(data={'y': y_train, 'outdegree': X_train[:,13]})\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(sdf.y, (sdf.outdegree > 0).rename('previous initiation?'), normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bugfix: the target_inds bug identified above\n",
    "\n",
    "Here's the original buggy snippet of code (with an added assertion that will fail when n_target_inds > 1):\n",
    "\n",
    "```python\n",
    "sort_inds = self.candidate_usp_arr[:,1].argsort()\n",
    "self.candidate_usp_arr = self.candidate_usp_arr[sort_inds]\n",
    "# update which inds contain the target (if any)\n",
    "n_target_inds = len(self.target_inds)\n",
    "self.target_inds = np.argwhere(self.target_inds == sort_inds)\n",
    "assert len(self.target_inds) == n_target_inds\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_usp_arr = np.array([\n",
    "    [2, 3],\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "    [2, 4],\n",
    "    [3, 5],\n",
    "])\n",
    "target_inds = np.array([\n",
    "    0, 3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_inds = candidate_usp_arr[:,1].argsort()\n",
    "candidate_usp_arr = candidate_usp_arr[sort_inds]\n",
    "candidate_usp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isin(sort_inds, target_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update which inds contain the target (if any)\n",
    "n_target_inds = len(target_inds)\n",
    "target_inds = np.argwhere(np.isin(sort_inds, target_inds))\n",
    "assert len(target_inds) == n_target_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with single target ind\n",
    "# which revealed that we should also be called .ravel() to maintain the shape of the target_inds\n",
    "candidate_usp_arr = np.array([\n",
    "    [2, 3],\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "    [3, 5],\n",
    "])\n",
    "target_inds = np.array([\n",
    "    0,\n",
    "])\n",
    "\n",
    "sort_inds = candidate_usp_arr[:,1].argsort()\n",
    "candidate_usp_arr = candidate_usp_arr[sort_inds]\n",
    "# update which inds contain the target (if any)\n",
    "n_target_inds = len(target_inds)\n",
    "target_inds = np.argwhere(np.isin(sort_inds, target_inds)).ravel()\n",
    "assert len(target_inds) == n_target_inds\n",
    "\n",
    "candidate_usp_arr, target_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cbrec.modeling Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.modeling.modelconfig\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = cbrec.modeling.modelconfig.ModelConfig()\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override default configuration values here\n",
    "model_config.train_n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = cbrec.modeling.manager.ModelManager(model_config, config=config)\n",
    "model_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager.train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model_manager to score the data\n",
    "y_test_score = model_manager.score_test_matrix(X_test)\n",
    "y_test_score.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test accuracy from the scores\n",
    "y_pred = (y_test_score >= 0.5).astype(int)\n",
    "np.sum(y_pred == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = cbrec.modeling.manager.ModelManager.load_from_model_name('LinearNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.load_model(load_model_state_dict=True, load_training_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've loaded the saved model data, we can plot the model's metrics\n",
    "train_metrics, test_metrics = mm.model_trainer.get_train_metrics()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "xs = test_metrics.T[:,0]\n",
    "ys = test_metrics.T[:,1]\n",
    "ax.plot(xs, ys, label='Test')\n",
    "\n",
    "xs = train_metrics.T[:,0]\n",
    "ys = train_metrics.T[:,1]\n",
    "ax.plot(xs, ys, label='Train')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
