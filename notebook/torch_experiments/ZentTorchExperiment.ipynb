{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import dateutil.parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace packages\n",
    "import transformers\n",
    "import tokenizers\n",
    "import torch\n",
    "\n",
    "# more torch imports\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "import cbrec.genconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cbrec.genconfig.Config()\n",
    "#config.metadata_filepath += \"_old\"\n",
    "#config.feature_db_filepath += \"_old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.featuredb\n",
    "import cbrec.utils\n",
    "import cbrec.data\n",
    "import cbrec.reccontext\n",
    "import cbrec.evaluation\n",
    "import cbrec.torchmodel\n",
    "import cbrec.text.embeddingdb\n",
    "import cbrec.text.journalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.logutils\n",
    "cbrec.logutils.set_up_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off matplotlib logging\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)\n",
    "import cbcore.data.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train features\n",
    "feature_cache_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'torch_experiments', 'feature_cache')\n",
    "filenames = [\n",
    "    ('X_train_raw.pkl', 'y_train_raw.pkl'),\n",
    "    ('X_test2train_raw.pkl', 'y_test2train_raw.pkl'),\n",
    "]\n",
    "\n",
    "def get_features(x_filename, y_filename):\n",
    "    with open(os.path.join(feature_cache_dir, x_filename), 'rb') as infile:\n",
    "        X = pickle.load(infile)\n",
    "    with open(os.path.join(feature_cache_dir, y_filename), 'rb') as infile:\n",
    "        y = pickle.load(infile)\n",
    "    return X, y\n",
    "\n",
    "x_filename, y_filename = filenames[0]\n",
    "X_train, y_train = get_features(x_filename, y_filename)\n",
    "    \n",
    "x_filename, y_filename = filenames[1]\n",
    "X_test, y_test = get_features(x_filename, y_filename)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "inds = np.arange(len(X_train))\n",
    "np.random.shuffle(inds)\n",
    "X_train = X_train[inds]\n",
    "y_train = y_train[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data to speed up convergence\n",
    "import sklearn.preprocessing\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple neural net with 2 hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden, dropout_p=0.2):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # note: 768 is the size of the roBERTa outputs\n",
    "        self.fc1 = nn.Linear(n_input, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, 1, bias=False)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_p)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)  # note: not using F.sigmoid here, as the loss used includes the Sigmoid transformation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"notebook.ZentTorchExperiment\")\n",
    "    \n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n",
    "\n",
    "verbose = True\n",
    "n_hidden = 100\n",
    "n_epochs = 100\n",
    "lr_init = 0.01\n",
    "max_lr = 0.02  # 0.0155\n",
    "dropout_p = 0.1\n",
    "minibatch_size = len(y_train)\n",
    "minibatch_size = min(n_train, minibatch_size) # if minibatch_size is larger than n_train, force it to n_train\n",
    "n_minibatches = int(np.ceil(n_train / minibatch_size))\n",
    "\n",
    "validation_rate = 0.1 # (vr) we will compute loss and accuracy against the validation set on vr of the epochs\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "# note: input dim is 27 for non-text features + 768 for text features\n",
    "net = LinearNet(n_input, n_hidden, dropout_p)\n",
    "\n",
    "#optimizer = optim.SGD(net.parameters(), lr=lr_init, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr_init)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    steps_per_epoch=n_minibatches,\n",
    "    epochs=n_epochs,\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # pointwise loss function\n",
    "\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)  # make labels 2-dimensional\n",
    "y_train_tensor = y_train_tensor.type_as(X_train_tensor)\n",
    "if verbose:\n",
    "    logger.info(f\"Input tensor sizes: {X_train_tensor.size()}, {y_train_tensor.size()}\")\n",
    "    logger.info(f\"Validating model every {1/validation_rate} epochs.\")\n",
    "\n",
    "# _metrics[0] -> Epoch, metrics[1] -> loss, _metrics[2] -> accuracy\n",
    "test_metrics = np.zeros((3,int(n_epochs*validation_rate+1))) #+1 to ensure space for final epoch metric\n",
    "train_metrics = np.zeros((3,n_epochs))\n",
    "\n",
    "net.train()\n",
    "for epoch in range(n_epochs):\n",
    "    s = datetime.now()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # shuffle the training data\n",
    "    # This will randomize our minibatches at each epoch\n",
    "    epoch_order = torch.randperm(n_train)\n",
    "\n",
    "    mb_metrics = []  # store the minibatch_metrics, then average after\n",
    "    for minibatch in range(n_minibatches):\n",
    "        minibatch_start = minibatch * minibatch_size\n",
    "        minibatch_end = min(minibatch_start + minibatch_size, n_train)\n",
    "        if verbose and epoch == 0:\n",
    "            logger.info(f\"    Minibatch for inds in {minibatch_start} - {minibatch_end}.\")\n",
    "        minibatch_inds = epoch_order[minibatch_start:minibatch_end]\n",
    "\n",
    "        inputs = X_train_tensor[minibatch_inds]\n",
    "        train_labels = y_train_tensor[minibatch_inds]\n",
    "\n",
    "        net.train()\n",
    "        train_outputs = net(inputs)\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # compute and log the loss\n",
    "        y_train_pred = torch.sigmoid(train_outputs.detach()).view((-1,)).numpy()\n",
    "        y_train_pred = (y_train_pred >= 0.5).astype(int)  # binarize predictions with a 0.5 decision boundary\n",
    "        y_train_minibatch = y_train[minibatch_inds.numpy()]\n",
    "        train_acc = np.sum(y_train_pred == y_train_minibatch) / len(y_train_minibatch)\n",
    "        \n",
    "        mb_metrics.append((train_loss.item(), train_acc))\n",
    "    train_loss, train_acc = np.mean(np.array(mb_metrics), axis=0)\n",
    "    train_metrics[0,epoch] = epoch\n",
    "    train_metrics[1,epoch] = train_loss\n",
    "    train_metrics[2,epoch] = train_acc\n",
    "    \n",
    "    should_stop_early = train_loss < 0.001\n",
    "    if verbose and (epoch < 5 or epoch == n_epochs - 1 or epoch % 10 == 0 or should_stop_early):\n",
    "        logger.info(f\"{epoch:>3} ({datetime.now() - s}): train loss={train_loss:.4f} train accuracy={train_acc*100:.2f}% LR={optimizer.param_groups[0]['lr']:.2E}\")\n",
    "    if should_stop_early:\n",
    "        break\n",
    "        \n",
    "    if epoch % (1/validation_rate) == 0:\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = net(X_test_tensor)\n",
    "            test_loss = criterion(test_outputs.detach(), y_test_tensor.unsqueeze(1).float())\n",
    "            y_test_pred = torch.sigmoid(test_outputs.detach()).view((-1,)).numpy()\n",
    "            y_test_pred = (y_test_pred >= 0.5).astype(int)\n",
    "            test_acc = np.sum(y_test_pred == y_test) / len(y_test)\n",
    "        logger.info(f\"    {epoch:>3}: test loss={test_loss:.4f} test accuracy={test_acc*100:.2f}%\")\n",
    "        metric_ind = int(epoch*validation_rate)\n",
    "        test_metrics[0,metric_ind] = epoch\n",
    "        test_metrics[1,metric_ind] = test_loss\n",
    "        test_metrics[2,metric_ind] = test_acc\n",
    "\n",
    "# this is a hack, but we store training results info back through the learner_config dictionary\n",
    "final_train_loss = train_loss\n",
    "final_epoch_count = epoch + 1\n",
    "if verbose:\n",
    "    logger.info(f\"Completed {final_epoch_count} epochs with a final train loss of {final_train_loss:.4f}.\")\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.from_numpy(X_test)\n",
    "    outputs = net(X_test_tensor)\n",
    "    test_loss = criterion(test_outputs.detach(), y_test_tensor.unsqueeze(1).float())\n",
    "    y_test_pred = torch.sigmoid(outputs.detach()).view((-1,)).numpy()\n",
    "    y_test_pred = (y_test_pred >= 0.5).astype(int)\n",
    "    acc = np.sum(y_test_pred == y_test) / len(y_test)\n",
    "    logger.info(f\"Test acc: {acc*100:.2f}%\")\n",
    "    test_metrics[0, test_metrics.shape[1] - 1] = epoch\n",
    "    test_metrics[1, test_metrics.shape[1] - 1] = test_loss\n",
    "    test_metrics[2, test_metrics.shape[1] - 1 ] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_metrics[0],train_metrics[1])             \n",
    "plt.plot(test_metrics[0],test_metrics[1])\n",
    "plt.legend([\"Train\",\"Test\"])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_metrics[0],train_metrics[2])             \n",
    "plt.plot(test_metrics[0],test_metrics[2])\n",
    "plt.legend([\"Train\",\"Test\"])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train features\n",
    "feature_cache_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'torch_experiments', 'feature_cache')\n",
    "filenames = [\n",
    "    ('X_train_raw.pkl', 'y_train_raw.pkl'),\n",
    "    ('X_test2train_raw.pkl', 'y_test2train_raw.pkl'),\n",
    "]\n",
    "Xs = []\n",
    "ys = []\n",
    "for x_filename, y_filename in filenames:\n",
    "    with open(os.path.join(feature_cache_dir, x_filename), 'rb') as infile:\n",
    "        X = pickle.load(infile)\n",
    "        Xs.append(X)\n",
    "    with open(os.path.join(feature_cache_dir, y_filename), 'rb') as infile:\n",
    "        y = pickle.load(infile)\n",
    "        ys.append(y)\n",
    "\n",
    "X = np.concatenate(Xs, axis=0)\n",
    "y_true = np.concatenate(ys, axis=0)\n",
    "\n",
    "# shuffle the data\n",
    "inds = np.arange(len(X))\n",
    "np.random.shuffle(inds)\n",
    "X = X[inds]\n",
    "y_true = y_true[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cbrec.genconfig.Config()\n",
    "torch_model = cbrec.torchmodel.TorchModel(config)\n",
    "torch_model.set_training_data(X, y_true)\n",
    "logger.info(f\"Using training data in shape X={torch_model.X.shape}, y={torch_model.y_true.shape}.\")\n",
    "\n",
    "logger.info(\"Training model\")\n",
    "torch_model.train_model()\n",
    "\n",
    "logger.info(\"Model performance metrics\")\n",
    "torch_model.save_model_metrics(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"notebook.ZentTorchExperiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Model performance metrics\")\n",
    "torch_model.save_model_metrics(show_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test']\n",
    "len(test_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rc_list from pickle\n",
    "s = datetime.now()\n",
    "feature_cache_dir = os.path.join(config.torch_experiments_dir, 'feature_cache')\n",
    "with open(os.path.join(feature_cache_dir, 'rc_test_notext_2000.pkl'), 'rb') as infile:\n",
    "    rc_list = pickle.load(infile)\n",
    "print(f\"Loaded {len(rc_list)} RecContexts in {datetime.now() - s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple neural net with 2 hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden, dropout_p=0.2):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # note: 768 is the size of the roBERTa outputs\n",
    "        self.fc1 = nn.Linear(n_input, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, 1, bias=False)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_p)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)  # note: not using F.sigmoid here, as the loss used includes the Sigmoid transformation\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = cbrec.torchmodel.TorchModel(config)\n",
    "\n",
    "dropout_p = 0.1\n",
    "n_hidden = 100\n",
    "n_input = 1563\n",
    "\n",
    "model_cache_dir = os.path.join(config.torch_experiments_dir, 'model_cache')\n",
    "torch_model.net = LinearNet(n_input, n_hidden, dropout_p)\n",
    "torch_model.net.load_state_dict(torch.load(os.path.join(model_cache_dir, 'LinearNet_20211007_e1400.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.evaluation\n",
    "class CustomModelScorer(cbrec.evaluation.Scorer):\n",
    "    def __init__(self, config, test_context: cbrec.reccontext.RecContext, \n",
    "                net, # TODO pass in a model object here, if appropriate\n",
    "                model_name=\"CustomModel\"):\n",
    "        super().__init__(config, test_context, coverage_tracker=None, save_scores=True)\n",
    "        self.model_name = model_name\n",
    "        self.net = net\n",
    "\n",
    "    def score(self):\n",
    "        \"\"\"\n",
    "        Score the RecContext.\n",
    "        \n",
    "        Use self.text_context to produce a y_score_site list, and return a dictionary of metrics.\n",
    "        \n",
    "        \"\"\"\n",
    "        X = self.test_context.X_test\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.from_numpy(X)\n",
    "            outputs = self.net(X_test_tensor)\n",
    "            y_score = torch.sigmoid(outputs.detach()).view((-1,)).numpy()\n",
    "        \n",
    "        y_score_mat = self.get_empty_score_arr('full')\n",
    "        y_score_mat = y_score.reshape((y_score_mat.shape[1], y_score_mat.shape[0])).T\n",
    "\n",
    "        y_score_site = self.reduce_usp_ranking_to_site(self.merge_multisource_rankings(y_score_mat))\n",
    "        self.compute_metrics(y_score_site, model_name=self.model_name)\n",
    "        \n",
    "        return self.metrics_dict[self.model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = CustomModelScorer(config, rc_list[0], torch_model.net)\n",
    "scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing cbrec/modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.modeling.modelconfig\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager\n",
    "\n",
    "config = cbrec.genconfig.Config()\n",
    "model_config = cbrec.modeling.modelconfig.ModelConfig()\n",
    "manager = cbrec.modeling.manager.ModelManager(model_config, config)\n",
    "manager.train_model(X_train, y_train)\n",
    "#model_config.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_config.as_dict())\n",
    "for col in model_config.preprocess_drop_columns:\n",
    "    print(model_config.column_keys.index(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.modeling.preprocess\n",
    "\n",
    "preprocesser = cbrec.modeling.preprocess.FeaturePreprocessor(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[:,4])\n",
    "print(X_train[:,5])\n",
    "\n",
    "X_train = preprocesser.preprocess(X_train)\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[:,4])\n",
    "print(X_train[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_config.as_dict()\n",
    "for hot_encoding in model_config.preprocess_encode_columns:\n",
    "    print(hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[:,1])\n",
    "\n",
    "print(model_config.as_dict())\n",
    "\n",
    "print(\"after:\")\n",
    "X_train = preprocesser.remove_feature_columns(X_train)\n",
    "print(model_config.as_dict())\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train features\n",
    "feature_cache_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'torch_experiments', 'feature_cache')\n",
    "filenames = [\n",
    "    ('X_train_raw.pkl', 'y_train_raw.pkl'),\n",
    "    ('X_test2train_raw.pkl', 'y_test2train_raw.pkl'),\n",
    "]\n",
    "\n",
    "def get_features(x_filename, y_filename):\n",
    "    with open(os.path.join(feature_cache_dir, x_filename), 'rb') as infile:\n",
    "        X = pickle.load(infile)\n",
    "    with open(os.path.join(feature_cache_dir, y_filename), 'rb') as infile:\n",
    "        y = pickle.load(infile)\n",
    "    return X, y\n",
    "\n",
    "x_filename, y_filename = filenames[0]\n",
    "X_train, y_train = get_features(x_filename, y_filename)\n",
    "    \n",
    "x_filename, y_filename = filenames[1]\n",
    "X_test, y_test = get_features(x_filename, y_filename)\n",
    "print(\"train/test shape\")\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# X = np.concatenate((X_train, X_test), axis=0)\n",
    "# y_true = np.concatenate((y_train, y_test), axis=0)\n",
    "# print(\"concat shape\")\n",
    "# X.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print(np.histogram(X_train[:,4], bins=[0, 1/60, 1, 24, 24*7, 24*7*365, math.inf]))\n",
    "print(np.histogram(X_train[:,6], bins=[0, 1/60, 1, 24, 24*7, 24*7*365, math.inf]))\n",
    "print(np.histogram(X_train[:,8], bins=[0, 1/60, 1, 24, 24*7, 24*7*365, math.inf]))\n",
    "print(np.histogram(X_train[:,10], bins=[0, 1/60, 1, 24, 24*7, 24*7*365, math.inf]))\n",
    "print(np.histogram(X_train[:,11], bins=[0, 1/60, 1, 24, 24*7, 24*7*365, math.inf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.set_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = {\"source_feature_arr\": 12, \"candidate_feature_arr\": 12, \"source_candidate_feature_arr\": 3, \"source_text_arr\": 768, \"candidate_text_arr\": 768}\n",
    "out_str = \"\"\n",
    "\n",
    "for key, value in list.items():\n",
    "    for i in range(value):\n",
    "        out_str+=\"'\" + key + \"_\" + str(i) + \"',\"\n",
    "        \n",
    "#print(out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.modeling.modelconfig\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager\n",
    "import cbrec.modeling.preprocess\n",
    "\n",
    "config = cbrec.genconfig.Config()\n",
    "model_config = cbrec.modeling.modelconfig.ModelConfig()\n",
    "\n",
    "feature_manager = cbrec.modeling.preprocess.FeatureManager(model_config)\n",
    "print(feature_manager.get_feature_index(\"source-indegree\"))\n",
    "print(feature_manager.get_feature_indices(\"*\", 'text', feature_descriptor_inverse = True))\n",
    "print(feature_manager.get_feature_indices(\"source\", 'indegree'))\n",
    "print(feature_manager.get_feature_indices(\"source\", '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "temp = OrderedDict()\n",
    "temp2 = []\n",
    "print(type(temp2))\n",
    "isinstance(temp, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train features\n",
    "feature_cache_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'torch_experiments', 'feature_cache')\n",
    "filenames = [\n",
    "    ('X_train_raw.pkl', 'y_train_raw.pkl'),\n",
    "    ('X_test2train_raw.pkl', 'y_test2train_raw.pkl'),\n",
    "]\n",
    "Xs = []\n",
    "ys = []\n",
    "for x_filename, y_filename in filenames:\n",
    "    with open(os.path.join(feature_cache_dir, x_filename), 'rb') as infile:\n",
    "        X = pickle.load(infile)\n",
    "        Xs.append(X)\n",
    "    with open(os.path.join(feature_cache_dir, y_filename), 'rb') as infile:\n",
    "        y = pickle.load(infile)\n",
    "        ys.append(y)\n",
    "\n",
    "X = np.concatenate(Xs, axis=0)\n",
    "y_true = np.concatenate(ys, axis=0)\n",
    "\n",
    "# shuffle the data\n",
    "inds = np.arange(len(X))\n",
    "np.random.shuffle(inds)\n",
    "X = X[inds]\n",
    "y_true = y_true[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values, bins = pd.qcut(X[:,1], 4, labels=False, retbins=True)\n",
    "print(X[:,1])\n",
    "print(new_values)\n",
    "print(bins)\n",
    "print(range(5))\n",
    "print(range(0, 5))\n",
    "print(type(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.modeling.modelconfig\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager\n",
    "from cbrec.modeling.preprocess import OneHotEncoding\n",
    "\n",
    "logger = logging.getLogger(\"notebook.ZentTorchExperiment\")\n",
    "\n",
    "model_config = cbrec.modeling.modelconfig.ModelConfig()\n",
    "model_config.preprocess_drop_columns = []\n",
    "model_config.experiment_name = '6_quant_duration_2'\n",
    "model_config.train_n_epochs = 1\n",
    "\n",
    "# for usp_type in ['source', 'candidate']:\n",
    "#     for int_type in ['journal', 'amp', 'comment', 'guestbook']:\n",
    "#         model_config.preprocess_encode_columns.append(OneHotEncoding(usp_type + \"-\" + int_type + \"_time_to_most_recent\", 6))\n",
    "#     model_config.preprocess_encode_columns.append(OneHotEncoding(usp_type + \"-\" + \"time_to_first_update\",6))\n",
    "model_config.preprocess_encode_columns.append(OneHotEncoding(\"source-time_to_first_update\",6))\n",
    "\n",
    "model_manager = cbrec.modeling.manager.ModelManager(model_config)\n",
    "logger.info(model_manager.model_config.output_basename)\n",
    "\n",
    "model_manager.train_model(X, y_true)\n",
    "model_manager.save_model()\n",
    "\n",
    "logger.info(\"Finished training and saving model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/LinearNet_wo_duration_1_20211118184004.json\"\n",
    "# with open(direct, 'rb') as infile:\n",
    "#     val_metrics = pickle.load(infile)\n",
    "import cbrec.modeling.modelconfig\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager\n",
    "\n",
    "#mm = cbrec.modeling.manager.ModelManager.load_from_model_name('LinearNet', experiment_name = \"wo_encode_dur_1\")\n",
    "mm = cbrec.modeling.manager.ModelManager.load_from_filepath(direct)\n",
    "\n",
    "mm.load_model(load_model_state_dict=True, load_training_metrics=True)\n",
    "    \n",
    "train_metrics, test_metrics = mm.model_trainer.get_train_metrics()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "xs = test_metrics.T[:,0]\n",
    "ys = test_metrics.T[:,1]\n",
    "ax.plot(xs, ys, label='Test')\n",
    "\n",
    "xs = train_metrics.T[:,0]\n",
    "ys = train_metrics.T[:,1]\n",
    "ax.plot(xs, ys, label='Train')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/LinearNet_encode_dur_1_20211104231737.json\"\n",
    "# with open(direct, 'rb') as infile:\n",
    "#     val_metrics = pickle.load(infile)\n",
    "import cbrec.modeling.modelconfig\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager\n",
    "\n",
    "#mm = cbrec.modeling.manager.ModelManager.load_from_model_name('LinearNet', experiment_name = \"wo_encode_dur_1\")\n",
    "mm = cbrec.modeling.manager.ModelManager.load_from_filepath(direct)\n",
    "\n",
    "mm.load_model(load_model_state_dict=True, load_training_metrics=True)\n",
    "    \n",
    "train_metrics, test_metrics = mm.model_trainer.get_train_metrics()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "xs = test_metrics.T[:,0]\n",
    "ys = test_metrics.T[:,1]\n",
    "ax.plot(xs, ys, label='Test')\n",
    "\n",
    "xs = train_metrics.T[:,0]\n",
    "ys = train_metrics.T[:,1]\n",
    "ax.plot(xs, ys, label='Train')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "subset = X[:,4] \n",
    "subset = subset[subset < (365*24)]\n",
    "plt.hist(subset, bins=100)\n",
    "print(f'max value:{max(subset)} of {len(subset)}')\n",
    "subset = X[:,6] \n",
    "subset = subset[subset < (365*24)]\n",
    "plt.hist(subset, bins=100)\n",
    "print(f'max value:{max(subset)} of {len(subset)}')\n",
    "subset = X[:,8] \n",
    "subset = subset[subset < (365*24)]\n",
    "plt.hist(subset, bins=100)\n",
    "print(f'max value:{max(subset)} of {len(subset)}')\n",
    "subset = X[:,10] \n",
    "subset = subset[subset < (365*24)]\n",
    "plt.hist(subset, bins=100)\n",
    "print(f'max value:{max(subset)} of {len(subset)}')\n",
    "plt.title(f\"Distribution of duration features less than 1 year\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "print(np.histogram(X[:,4], bins=[0, 1/60, 1, 24, 24*7, 24*7*365, math.inf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "subset = X[:,4] \n",
    "subset = subset[subset >= (365*24)]\n",
    "plt.hist(subset, bins=100)\n",
    "print(f'min value:{min(subset)} of {len(subset)}')\n",
    "subset = X[:,6] \n",
    "subset = subset[subset >= (365*24)]\n",
    "plt.hist(subset, bins=100)\n",
    "print(f'min value:{min(subset)} of {len(subset)}')\n",
    "subset = X[:,8] \n",
    "subset = subset[subset >= (365*24)]\n",
    "plt.hist(subset, bins=100)\n",
    "print(f'min value:{min(subset)} of {len(subset)}')\n",
    "subset = X[:,10] \n",
    "subset = subset[subset >= (365*24)]\n",
    "plt.hist(subset, bins=100)\n",
    "print(f'min value:{min(subset)} of {len(subset)}')\n",
    "plt.title(f\"Distribution of duration features over 1 year\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "print(np.histogram(X[:,4], bins=[0, 1/60, 1, 24, 24*7, 24*365, math.inf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X[:,8], bins=100)\n",
    "plt.title(f\"Distribution of {len(X)} feature 4\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X[:,10], bins=100)\n",
    "plt.title(f\"Distribution of {len(X)} feature 10\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X[:,11], bins=100)\n",
    "plt.title(f\"Distribution of {len(X)} feature 11\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print(np.histogram(X[:,4], bins=[0, 1/60, 1, 24, 24*7, 24*365, math.inf]))\n",
    "print(np.histogram(X[:,6], bins=[0, 1/60, 1, 24, 24*7, 24*365, math.inf]))\n",
    "print(np.histogram(X[:,8], bins=[0, 1/60, 1, 24, 24*7, 24*365, math.inf]))\n",
    "print(np.histogram(X[:,10], bins=[0, 1/60, 1, 24, 24*7, 24*365, math.inf]))\n",
    "print(np.histogram(X[:,11], bins=[0, 1/60, 1, 24, 24*7, 24*365, math.inf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpuonly",
   "language": "python",
   "name": "pytorch-cpuonly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
