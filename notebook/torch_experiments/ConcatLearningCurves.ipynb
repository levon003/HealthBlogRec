{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConcatNet Learning Curve analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import dateutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging; only run this cell once\n",
    "import logging\n",
    "use_cbrec_logging = True\n",
    "if not use_cbrec_logging:\n",
    "    # this is a demo of how to set up logging\n",
    "    # since we use cbrec logging below, this will be done for us when we call set_up_logging.\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.DEBUG)\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    root.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "import cbrec.genconfig\n",
    "config = cbrec.genconfig.Config()\n",
    "import cbrec.evaluation\n",
    "import cbrec.reccontext\n",
    "import cbrec.featuredb\n",
    "import cbrec.torchmodel\n",
    "import cbrec.utils\n",
    "import cbrec.logutils\n",
    "import cbrec.feature_loader\n",
    "cbrec.logutils.set_up_logging()\n",
    "# turn off matplotlib logging\n",
    "# which can be quite verbose and usually is not useful\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/concatnet_20220308033431/outputs\"\n",
    "assert os.path.exists(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/adam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "def identify_model_filepaths(model_dir):\n",
    "    logger = logging.getLogger(\"cbrec.modeling.submitEvalFromDirectory.identify_model_filepaths\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        raise ValueError(f\"Dir '{model_dir}' does not exist.\")\n",
    "    model_filepaths = []\n",
    "    for model_filepath in glob(os.path.join(model_dir, '*.json')):\n",
    "        model_filepaths.append(model_filepath)\n",
    "    if len(model_filepaths) == 0:\n",
    "        raise ValueError(f\"No .json files in dir '{model_dir}'.\")\n",
    "    logger.info(f\"Identified {len(model_filepaths)} model filepaths in dir {model_dir}.\")\n",
    "    return model_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepaths = identify_model_filepaths(output_dir)\n",
    "len(model_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbrec.modeling import scorer\n",
    "from cbrec.modeling import manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_filepath in model_filepaths:\n",
    "    mm = cbrec.modeling.manager.ModelManager.load_from_filepath(model_filepath)\n",
    "    mm.load_model(load_preprocessor=False, load_model_state_dict=False, load_training_metrics=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "metrics = mm.model_trainer.train_metrics\n",
    "xs = metrics[0,:]\n",
    "ys = metrics[1,:]\n",
    "ax.plot(xs, ys, color='blue', label='Train Loss', alpha=0.5)\n",
    "\n",
    "metrics = mm.model_trainer.test_metrics\n",
    "xs = metrics[0,:]\n",
    "ys = metrics[1,:]\n",
    "ax.plot(xs, ys, color='orange', label='Validation Loss')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(\"Learning curve for a single model\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_list = []\n",
    "for model_filepath in tqdm(model_filepaths):\n",
    "    mm = cbrec.modeling.manager.ModelManager.load_from_filepath(model_filepath)\n",
    "    mm.load_model(load_preprocessor=False, load_model_state_dict=False, load_training_metrics=True)\n",
    "    mm_list.append(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "for mm in mm_list:\n",
    "    metrics = mm.model_trainer.test_metrics\n",
    "    xs = metrics[0,:]\n",
    "    ys = metrics[1,:]\n",
    "    ax.plot(xs, ys, color='black', alpha=0.2)\n",
    "\n",
    "\n",
    "ax.set_title(\"Learning curve for all models\")\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_ylabel(\"Validation loss\")\n",
    "ax.set_xlabel(\"# of epochs trained\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/adam_randomsearch_experiment_20220213194145/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/adam_randomsearch_experiment_20220213194145/outputs\"\n",
    "model_filepaths = identify_model_filepaths(output_dir)\n",
    "mm_list = []\n",
    "for model_filepath in tqdm(model_filepaths):\n",
    "    mm = cbrec.modeling.manager.ModelManager.load_from_filepath(model_filepath)\n",
    "    mm.load_model(load_preprocessor=False, load_model_state_dict=False, load_training_metrics=True)\n",
    "    mm_list.append(mm)\n",
    "len(mm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "for mm in mm_list:\n",
    "    metrics = mm.model_trainer.test_metrics\n",
    "    xs = metrics[0,2:]\n",
    "    ys = metrics[1,2:]\n",
    "    ax.plot(xs, ys, color='black', alpha=0.2)\n",
    "\n",
    "\n",
    "ax.set_title(\"Learning curve for all models\")\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_ylabel(\"Validation loss\")\n",
    "ax.set_xlabel(\"# of epochs trained\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1\n",
    "for mm in mm_list:\n",
    "    metrics = mm.model_trainer.test_metrics\n",
    "    best_model_loss = np.min(metrics[1,:])\n",
    "    if best_model_loss < best_loss:\n",
    "        best_loss = best_model_loss\n",
    "print(f\"Best loss: {best_loss:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "for mm in mm_list:\n",
    "    metrics = mm.model_trainer.test_metrics\n",
    "    xs = metrics[0,2:]\n",
    "    ys = metrics[1,2:]\n",
    "    best_model_loss = np.min(metrics[1,:])\n",
    "    if best_model_loss < best_loss + 0.01:\n",
    "        ax.plot(xs, ys, color='black', alpha=0.2)\n",
    "\n",
    "ax.set_title(\"Learning curve for all models\")\n",
    "#ax.set_yscale('log')\n",
    "\n",
    "ax.set_ylabel(\"Validation loss\")\n",
    "ax.set_xlabel(\"# of epochs trained\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpuonly",
   "language": "python",
   "name": "pytorch-cpuonly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
