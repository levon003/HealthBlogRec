{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval Demo\n",
    "===\n",
    "\n",
    "Generate the evaluation data using the evalModels.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import dateutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging; only run this cell once\n",
    "import logging\n",
    "use_cbrec_logging = True\n",
    "if not use_cbrec_logging:\n",
    "    # this is a demo of how to set up logging\n",
    "    # since we use cbrec logging below, this will be done for us when we call set_up_logging.\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.DEBUG)\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    root.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cbrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.genconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config, which is needed by lots of the components for resolving paths, etc.\n",
    "config = cbrec.genconfig.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.evaluation\n",
    "import cbrec.reccontext\n",
    "import cbrec.featuredb\n",
    "import cbrec.torchmodel\n",
    "import cbrec.utils\n",
    "import cbrec.logutils\n",
    "import cbrec.feature_loader\n",
    "import cbrec.modeling\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbrec.logutils.set_up_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off matplotlib logging\n",
    "# which can be quite verbose and usually is not useful\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/field_study_model_experiment_20220609032420/outputs/\"\n",
    "assert os.path.exists(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_model_filepaths(model_dir):\n",
    "    logger = logging.getLogger(\"cbrec.modeling.submitEvalFromDirectory.identify_model_filepaths\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        raise ValueError(f\"Dir '{model_dir}' does not exist.\")\n",
    "    model_filepaths = []\n",
    "    for model_filepath in glob(os.path.join(model_dir, '*.json')):\n",
    "        model_filepaths.append(model_filepath)\n",
    "    if len(model_filepaths) == 0:\n",
    "        raise ValueError(f\"No .json files in dir '{model_dir}'.\")\n",
    "    logger.info(f\"Identified {len(model_filepaths)} model filepaths in dir {model_dir}.\")\n",
    "    return model_filepaths\n",
    "\n",
    "\n",
    "class ModelEval:\n",
    "    def __init__(self, model_output_dir):\n",
    "        self.logger = logging.getLogger('eval.ModelEval')\n",
    "        self.model_output_dir = model_output_dir\n",
    "        self.model_filepaths = self.identify_model_filepaths()\n",
    "        \n",
    "        self.models = {}\n",
    "        \n",
    "        \n",
    "    def identify_model_filepaths(self):\n",
    "        if not os.path.exists(self.model_output_dir):\n",
    "            raise ValueError(f\"Dir '{self.model_output_dir}' does not exist.\")\n",
    "        model_filepaths = []\n",
    "        for model_filepath in glob(os.path.join(self.model_output_dir, '*.json')):\n",
    "            model_filepaths.append(model_filepath)\n",
    "        if len(model_filepaths) == 0:\n",
    "            raise ValueError(f\"No .json files in dir '{model_dir}'.\")\n",
    "        self.logger.info(f\"Identified {len(model_filepaths)} model filepaths in dir {self.model_output_dir}.\")\n",
    "        return model_filepaths\n",
    "\n",
    "        \n",
    "    def create_managers(self):\n",
    "        self.managers = []\n",
    "        for model_filepath in self.model_filepaths:\n",
    "            manager = cbrec.modeling.manager.ModelManager.load_from_filepath(model_filepath)\n",
    "            self.managers.append(manager)\n",
    "            \n",
    "            self.models[manager.model_config.output_name] = {}\n",
    "            \n",
    "            \n",
    "    def create_test_metrics(self):\n",
    "        for manager in self.managers:\n",
    "            manager.load_model(load_preprocessor=False, load_model_state_dict=False, load_training_metrics=True)\n",
    "            self.models[manager.model_config.output_name]['train_metrics'] = manager.model_trainer.train_metrics\n",
    "            self.models[manager.model_config.output_name]['test_metrics'] = manager.model_trainer.test_metrics\n",
    "\n",
    "    def get_scores(self, subset=None):\n",
    "        for manager in self.managers:\n",
    "            if subset is not None and manager.model_config.output_name not in subset:\n",
    "                continue\n",
    "            #metadata_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_test_metadata.ndjson')\n",
    "            scores_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_coverage_scores.pkl')\n",
    "            #assert os.path.exists(metadata_filepath)\n",
    "            assert os.path.exists(scores_filepath)\n",
    "            \n",
    "            with open(scores_filepath, 'rb') as scores_infile:\n",
    "                scores = pickle.load(scores_infile)\n",
    "            self.models[manager.model_config.output_name]['coverage_scores'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = ModelEval(output_dir)\n",
    "logging.disable(level=logging.INFO)\n",
    "ev.create_managers()\n",
    "logging.disable(logging.NOTSET)\n",
    "len(ev.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(level=logging.INFO)\n",
    "ev.create_test_metrics()\n",
    "logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_keys = ['train_max_lr', 'LinearNet_n_hidden', 'train_weight_decay', 'LinearNet_dropout_p']\n",
    "model_group_map = {}\n",
    "for manager in ev.managers:\n",
    "    model_name = manager.model_config.output_name\n",
    "    mc = manager.model_config.as_dict()\n",
    "    group_key = tuple([mc[key] for key in grouping_keys])\n",
    "    if group_key not in model_group_map:\n",
    "        model_group_map[group_key] = []\n",
    "    model_group_map[group_key].append(model_name)\n",
    "len(model_group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_short_name_map = {\n",
    "    'LinearNet_n_hidden': 'n_hidden',\n",
    "    'train_weight_decay': 'wd',\n",
    "    'train_max_lr': 'max_lr',\n",
    "    'LinearNet_dropout_p': 'do',\n",
    "}\n",
    "\n",
    "group_metrics_list = []\n",
    "for group, model_names in model_group_map.items():\n",
    "    best_validation_losses = [ev.models[model_name]['test_metrics'][1,:].min() for model_name in model_names]\n",
    "    best_validation_accs = [ev.models[model_name]['test_metrics'][2,:].max() for model_name in model_names]\n",
    "    final_validation_losses = [ev.models[model_name]['test_metrics'][1, -1] for model_name in model_names]\n",
    "    final_validation_accs = [ev.models[model_name]['test_metrics'][2, -1] for model_name in model_names]\n",
    "    group_name = \"; \".join([f\"{key if key not in key_short_name_map else key_short_name_map[key]}={value}\" for key, value in zip(grouping_keys, group)])\n",
    "    \n",
    "    group_metrics_list.append({\n",
    "        'group_name': group_name,\n",
    "        **{key: value for key, value in zip(grouping_keys, group)},\n",
    "        'val_loss_min': np.min(best_validation_losses),\n",
    "        'val_loss_median': np.median(best_validation_losses),\n",
    "        'val_loss_max': np.max(best_validation_losses),\n",
    "        'val_loss_ptp': np.ptp(best_validation_losses),\n",
    "        'val_acc_min': np.min(best_validation_accs),\n",
    "        'val_acc_median': np.median(best_validation_accs),\n",
    "        'val_acc_max': np.max(best_validation_accs),\n",
    "        'val_acc_ptp': np.ptp(best_validation_accs),\n",
    "    })\n",
    "eval_df = pd.DataFrame(group_metrics_list)\n",
    "len(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.sample(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.sort_values(by='val_loss_min', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.hist(eval_df.val_loss_ptp, bins=20)\n",
    "ax.set_title(\"Distribution of within-group variance in validation loss\")\n",
    "\n",
    "plt.show()\n",
    "eval_df.sort_values(by='val_loss_ptp', ascending=False)[['group_name', 'val_loss_min', 'val_acc_max', 'val_loss_ptp']].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in grouping_keys:\n",
    "    display(eval_df.groupby(key).val_loss_min.agg(['min', np.median, 'max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.sort_values(by='val_loss_min', ascending=True)[['group_name', 'val_loss_min', 'val_acc_max']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(eval_df.train_max_lr, [eval_df.LinearNet_n_hidden, eval_df.LinearNet_dropout_p], values=eval_df.val_acc_max, aggfunc=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"\"\"\n",
    "val_acc_max ~ C(train_max_lr)*C(train_weight_decay)\n",
    "\n",
    "\"\"\"\n",
    "md = smf.ols(formula=formula, data=eval_df)\n",
    "res = md.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final validation loss and accuracy after training for all loaded models\n",
    "print(f\"{'Model':>35} ValLoss ValAcc\")\n",
    "print(\"=\"*60)\n",
    "for model_name in ev.models.keys():\n",
    "    final_validation_loss = ev.models[model_name]['test_metrics'][1, -1]\n",
    "    final_validation_acc = ev.models[model_name]['test_metrics'][2, -1]\n",
    "    print(f\"{model_name:>35}  {final_validation_loss:.4f} {final_validation_acc:.2%}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_END_TIMESTAMP = datetime.strptime(\"2021-07-01\", \"%Y-%m-%d\").timestamp() * 1000\n",
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test' or md['type'] == 'predict']\n",
    "valid_md_list = [md for md in md_list if md['has_target'] and md['timestamp'] <= VALIDATION_END_TIMESTAMP]\n",
    "len(valid_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metadata_ids = set([md['metadata_id'] for md in valid_md_list])\n",
    "len(valid_metadata_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for manager in tqdm(ev.managers, desc='Loading validation metrics'):\n",
    "    metadata_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_validation_metadata.ndjson')\n",
    "    assert os.path.exists(metadata_filepath)\n",
    "    target_ranks = []\n",
    "    with open(metadata_filepath, 'r') as metadata_file:\n",
    "        for line in tqdm(metadata_file, total=len(valid_md_list), desc=f'Reading metrics {manager.model_config.output_name}', disable=True):\n",
    "            md = json.loads(line)\n",
    "            if md['metadata_id'] not in valid_metadata_ids:\n",
    "                continue\n",
    "            metrics = md[manager.model_config.output_name + \"_metrics\"]\n",
    "            target_rank = metrics['target_rank']\n",
    "            target_ranks.append(target_rank)\n",
    "            \n",
    "    target_ranks = np.array(target_ranks)\n",
    "    mrr = (1 / target_ranks).mean()\n",
    "    hr1 = (target_ranks == 1).sum() / len(target_ranks) * 100\n",
    "    hr5 = (target_ranks <= 5).sum() / len(target_ranks) * 100\n",
    "    ev.models[manager.model_config.output_name]['metrics'] = {\n",
    "        'n': len(target_ranks),\n",
    "        'mrr': mrr,\n",
    "        'hr1': hr1,\n",
    "        'hr5': hr5,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.DataFrame([{'model_name': model_name, **ev.models[model_name]['metrics']} for model_name in ev.models.keys()])\n",
    "print(len(valid_df))\n",
    "(valid_df.n != len(valid_metadata_ids)).sum(), (valid_df.n - len(valid_metadata_ids)).value_counts().rename(\"n_missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_keys = ['train_max_lr', 'LinearNet_n_hidden', 'train_weight_decay', 'LinearNet_dropout_p']\n",
    "key_short_name_map = {\n",
    "    'LinearNet_n_hidden': 'n_hidden',\n",
    "    'train_weight_decay': 'wd',\n",
    "    'train_max_lr': 'max_lr',\n",
    "    'LinearNet_dropout_p': 'do',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_metrics_list = []\n",
    "for group, model_names in model_group_map.items():\n",
    "    best_validation_losses = [ev.models[model_name]['test_metrics'][1,:].min() for model_name in model_names]\n",
    "    best_validation_accs = [ev.models[model_name]['test_metrics'][2,:].max() for model_name in model_names]\n",
    "    final_validation_losses = [ev.models[model_name]['test_metrics'][1, -1] for model_name in model_names]\n",
    "    final_validation_accs = [ev.models[model_name]['test_metrics'][2, -1] for model_name in model_names]\n",
    "    group_name = \"; \".join([f\"{key if key not in key_short_name_map else key_short_name_map[key]}={value}\" for key, value in zip(grouping_keys, group)])\n",
    "    \n",
    "    group_metrics = {\n",
    "        'group_name': group_name,\n",
    "        **{key: value for key, value in zip(grouping_keys, group)},\n",
    "        'n_models': len(model_names),\n",
    "    }\n",
    "    for metric in ['n', 'mrr', 'hr1', 'hr5']:\n",
    "        metric_values = [ev.models[model_name]['metrics'][metric] for model_name in model_names]\n",
    "        group_metrics[metric + \"_min\"] = np.min(metric_values)\n",
    "        group_metrics[metric + \"_median\"] = np.median(metric_values)\n",
    "        group_metrics[metric + \"_max\"] = np.max(metric_values)\n",
    "    \n",
    "    group_metrics.update({        \n",
    "        'val_loss_min': np.min(best_validation_losses),\n",
    "        'val_loss_median': np.median(best_validation_losses),\n",
    "        'val_loss_max': np.max(best_validation_losses),\n",
    "        'val_loss_ptp': np.ptp(best_validation_losses),\n",
    "        'val_acc_min': np.min(best_validation_accs),\n",
    "        'val_acc_median': np.median(best_validation_accs),\n",
    "        'val_acc_max': np.max(best_validation_accs),\n",
    "        'val_acc_ptp': np.ptp(best_validation_accs),\n",
    "    })\n",
    "    \n",
    "    group_metrics_list.append(group_metrics)\n",
    "eval_df = pd.DataFrame(group_metrics_list)\n",
    "len(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strong correlations between training hold-out accuracy on validation MRR\n",
    "for comparison_type in ['max', 'median']:\n",
    "    for valid_comparison_metric in ['mrr', 'hr1', 'hr5']:\n",
    "        for train_comparison_metric in ['val_loss', 'val_acc']:\n",
    "            train_key = train_comparison_metric + \"_\" + comparison_type\n",
    "            if comparison_type == 'max' and 'loss' in train_comparison_metric:\n",
    "                train_key = train_comparison_metric + \"_min\"\n",
    "            corr = eval_df[valid_comparison_metric + \"_\" + comparison_type].corr(eval_df[train_key])\n",
    "            print(f\"{comparison_type} {valid_comparison_metric} {train_comparison_metric} {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very strong correlations between validation metrics\n",
    "import itertools\n",
    "for comparison_type in ['max', 'median']:\n",
    "    for valid_comparison_metric1, valid_comparison_metric2 in itertools.combinations(['mrr', 'hr1', 'hr5'], 2):\n",
    "        corr = eval_df[valid_comparison_metric1 + \"_\" + comparison_type].corr(eval_df[valid_comparison_metric2 + \"_\" + comparison_type])\n",
    "        print(f\"{comparison_type} {valid_comparison_metric1} {valid_comparison_metric2} {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.sort_values(by='mrr_median', ascending=False)[['group_name', 'mrr_median', 'hr5_median', 'hr1_median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the configuration as used for the model\n",
    "# see code from the time of b0: https://github.com/umncs-caringbridge/recsys-peer-match/blob/33d258d8c514f6fb14930a034e8a9c7e2270f745/src/cbrec/torchmodel.py\n",
    "eval_df[(eval_df.LinearNet_n_hidden == 100)&(eval_df.LinearNet_dropout_p == 0.1)&(eval_df.train_weight_decay == 0)].sort_values(by='mrr_median', ascending=False)[['group_name', 'mrr_median', 'hr5_median', 'hr1_median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models = model_group_map[(0.01, 300, 0.0001, 0.5)]\n",
    "sorted(tuned_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_models = model_group_map[(0.012, 100, 0, 0.1)]\n",
    "sorted(study_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in tuned_models + study_models:\n",
    "    model_filepath = os.path.join(output_dir, f\"{model_name}.json\")\n",
    "    username = \"levon003\"\n",
    "    script_path = f\"/home/lana/{username}/repos/recsys-peer-match/src/cbrec/modeling/submitEvalFromDirectory.py\"\n",
    "    print(f\"python {script_path} --username {username} --model-filepath {model_filepath} --test-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h /home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/field_study_model_experiment_20220609032420/outputs/field_study_model_experiment_164.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test metric computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_END_TIMESTAMP = datetime.strptime(\"2021-07-01\", \"%Y-%m-%d\").timestamp() * 1000\n",
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test' or md['type'] == 'predict']\n",
    "test_md_list = [md for md in md_list if md['has_target'] and md['timestamp'] > VALIDATION_END_TIMESTAMP]\n",
    "len(test_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata_ids = set([md['metadata_id'] for md in test_md_list])\n",
    "len(test_metadata_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ranks = True\n",
    "for manager in ev.managers:\n",
    "    metadata_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_test_metadata.ndjson')\n",
    "    if not os.path.exists(metadata_filepath):\n",
    "        continue\n",
    "    target_ranks = []\n",
    "    with open(metadata_filepath, 'r') as metadata_file:\n",
    "        for line in tqdm(metadata_file, total=len(test_md_list) + 1000, desc=f'Reading metrics {manager.model_config.output_name}'):\n",
    "            md = json.loads(line)\n",
    "            if md['metadata_id'] not in test_metadata_ids:\n",
    "                continue\n",
    "            metrics = md[manager.model_config.output_name + \"_metrics\"]\n",
    "            target_rank = metrics['target_rank']\n",
    "            target_ranks.append(target_rank)\n",
    "            \n",
    "    target_ranks = np.array(target_ranks)\n",
    "    mrr = (1 / target_ranks).mean()\n",
    "    hr1 = (target_ranks == 1).sum() / len(target_ranks) * 100\n",
    "    hr5 = (target_ranks <= 5).sum() / len(target_ranks) * 100\n",
    "    ev.models[manager.model_config.output_name]['metrics'] = {\n",
    "        'mrr': mrr,\n",
    "        'hr1': hr1,\n",
    "        'hr5': hr5,\n",
    "    }\n",
    "    if save_ranks:\n",
    "        ev.models[manager.model_config.output_name]['metrics']['ranks'] = target_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{'model_name': model_name, **ev.models[model_name]['metrics']} for model_name in ev.models.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for manager in tqdm(ev.managers, desc='Loading test metrics'):\n",
    "    metadata_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_test_metadata.ndjson')\n",
    "    if not os.path.exists(metadata_filepath):\n",
    "        continue\n",
    "    target_ranks = []\n",
    "    with open(metadata_filepath, 'r') as metadata_file:\n",
    "        for line in tqdm(metadata_file, total=len(test_md_list), desc=f'Reading metrics {manager.model_config.output_name}', disable=False):\n",
    "            md = json.loads(line)\n",
    "            if md['metadata_id'] not in test_metadata_ids:\n",
    "                continue\n",
    "            metrics = md[manager.model_config.output_name + \"_metrics\"]\n",
    "            target_rank = metrics['target_rank']\n",
    "            target_ranks.append(target_rank)\n",
    "    assert len(target_ranks) > 0\n",
    "    \n",
    "    target_ranks = np.array(target_ranks)\n",
    "    mrr = (1 / target_ranks).mean()\n",
    "    hr1 = (target_ranks == 1).sum() / len(target_ranks) * 100\n",
    "    hr5 = (target_ranks <= 5).sum() / len(target_ranks) * 100\n",
    "    ev.models[manager.model_config.output_name]['metrics'] = {\n",
    "        'n': len(target_ranks),\n",
    "        'mrr': mrr,\n",
    "        'hr1': hr1,\n",
    "        'hr5': hr5,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame([{\n",
    "    'model': 'study' if model_name in study_models else 'tuned', \n",
    "    'model_name': model_name,\n",
    "    **ev.models[model_name]['metrics']\n",
    "} for model_name in study_models + tuned_models])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the results for the MLP_study and MLP_tuned models\n",
    "test_df.groupby('model').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: Metrics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "for manager in ev.managers:\n",
    "    test_metadata_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_test_metadata.ndjson')\n",
    "    if not os.path.exists(test_metadata_filepath):\n",
    "        continue\n",
    "    if manager.model_config.output_name not in study_models:\n",
    "        continue\n",
    "    validation_metadata_filepath = os.path.join(manager.model_config.output_dir, f'{manager.model_config.experiment_name}_{manager.model_config.output_name}_validation_metadata.ndjson')\n",
    "    assert os.path.exists(validation_metadata_filepath)\n",
    "    for metadata_filepath in [test_metadata_filepath, validation_metadata_filepath]:\n",
    "        with open(metadata_filepath, 'r') as metadata_file:\n",
    "            for line in tqdm(metadata_file, total=len(test_md_list), desc=f'Reading ranks from {os.path.basename(metadata_filepath)}', disable=False):\n",
    "                md = json.loads(line)\n",
    "                if not md['has_target']:\n",
    "                    continue\n",
    "                metrics = md[manager.model_config.output_name + \"_metrics\"]\n",
    "                target_rank = metrics['target_rank']\n",
    "                ranks.append({\n",
    "                    'model': 'study' if model_name in study_models else 'tuned',\n",
    "                    'model_name': manager.model_config.output_name,\n",
    "                    'metadata_id': md['metadata_id'],\n",
    "                    'timestamp': md['timestamp'],\n",
    "                    'target_rank': target_rank,\n",
    "                })\n",
    "    \n",
    "len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df = pd.DataFrame(ranks)\n",
    "rank_df.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = dateutil.parser.parse(\"2022-01-01\").replace(tzinfo=pytz.UTC)\n",
    "start_time = dateutil.parser.parse(\"2021-01-01\").replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "curr_time = start_time\n",
    "while curr_time < end_time:\n",
    "    bins.append(curr_time.timestamp() * 1000)\n",
    "    curr_time += relativedelta(weeks=1)\n",
    "bins.append(curr_time.timestamp() * 1000)\n",
    "print(len(bins))\n",
    "rank_df['week'] = np.digitize(rank_df.timestamp, bins=bins)\n",
    "rank_df.week.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(rank_df.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.family'] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5.4, 1.5))\n",
    "cmap = matplotlib.cm.viridis\n",
    "#ax.set_title(\"MRR throughout test period\")\n",
    "ax.set_xlabel(\"Weeks since September 1, 2020\", fontsize=8)\n",
    "ax.set_ylabel(\"Metric\", fontsize=8)\n",
    "\n",
    "#week_df = mdf.groupby(['model_name', 'week']).agg({'reciprocal_rank': np.mean, 'metadata_id': len}).rename(columns={'reciprocal_rank': 'mrr', 'metadata_id': 'n'}).reset_index().sort_values(by='week')\n",
    "\n",
    "# 'NaiveNetwork'\n",
    "i = 0\n",
    "for metric_name in ['MRR', 'HR@1', 'HR@5']:\n",
    "    sdf = rank_df\n",
    "    sdf = sdf.groupby('week').target_rank.agg([\n",
    "        lambda r: (1 / r).mean(),\n",
    "        lambda r: (r == 1).sum() / len(r),\n",
    "        lambda r: (r <= 5).sum() / len(r),\n",
    "    ]).rename(columns={'<lambda_0>': 'MRR', '<lambda_1>': 'HR@1', '<lambda_2>': 'HR@5',}).reset_index()\n",
    "    sdf = sdf.groupby('week').median().reset_index()\n",
    "    \n",
    "    print(sdf[metric_name].corr(sdf.week))\n",
    "    \n",
    "    # fit a model to check the linear slope over time\n",
    "    # (is MRR decreasing over time?)\n",
    "    if metric_name == 'MRR':\n",
    "        md = smf.ols(formula='MRR ~ week', data=sdf)\n",
    "        res = md.fit()\n",
    "        #print(res.summary())\n",
    "        beta, p = res.params.week, res.pvalues.week\n",
    "        print(beta, p)\n",
    "    label = f\"{metric_name}\"\n",
    "    \n",
    "    # plot the data\n",
    "    linestyle = '-'\n",
    "    if i == 1:\n",
    "        linestyle = 'dashed'\n",
    "    elif i == 2:\n",
    "        linestyle = 'dotted'\n",
    "    ax.plot(sdf.week, sdf[metric_name], label=label, color=cmap(i * 0.3), linestyle=linestyle)\n",
    "    ax.text(0.7, sdf.loc[sdf.week == 1, metric_name].iloc[0], metric_name, fontsize=7, ha='right', va='center')\n",
    "    i += 1\n",
    "\n",
    "ax.set_xlim((-3.5, 52))\n",
    "\n",
    "validation_end_week = 52 / 2\n",
    "ax.axvline(validation_end_week, linestyle='dashdot', color='gray')#, label='Start of test period')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "#ax.legend(fontsize=7)\n",
    "\n",
    "#ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: f\"{datetime.utcfromtimestamp(x / 1000).strftime('%m/%d/%Y')}\"))\n",
    "\n",
    "fig.tight_layout()\n",
    "image_shortfilename = f\"mlpstudy_metrics_over_time.pdf\"\n",
    "figures_dir = os.path.join(git_root_dir, 'figures')\n",
    "image_filename = os.path.join(figures_dir, image_shortfilename)\n",
    "fig.savefig(image_filename, format='pdf', dpi=200, pad_inches=0, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating that the validation period ends after 52//2 weeks\n",
    "(VALIDATION_END_TIMESTAMP - bins[52 // 2]) / 1000 / 60 / 60 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "cmap = matplotlib.cm.viridis\n",
    "ax.set_title(\"MRR throughout test period\")\n",
    "ax.set_xlabel(\"Initiation timestamp\")\n",
    "ax.set_ylabel(\"MRR (by week)\")\n",
    "\n",
    "end_time = dateutil.parser.parse(\"2022-01-01\").replace(tzinfo=pytz.UTC)\n",
    "start_time = dateutil.parser.parse(\"2021-01-01\").replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "curr_time = start_time\n",
    "while curr_time < end_time:\n",
    "    bins.append(curr_time.timestamp() * 1000)\n",
    "    curr_time += relativedelta(weeks=1)\n",
    "bins.append(curr_time.timestamp() * 1000)\n",
    "mdf['week'] = np.digitize(mdf.timestamp, bins=bins)\n",
    "\n",
    "week_df = mdf.groupby(['model_name', 'week']).agg({'reciprocal_rank': np.mean, 'metadata_id': len}).rename(columns={'reciprocal_rank': 'mrr', 'metadata_id': 'n'}).reset_index().sort_values(by='week')\n",
    "\n",
    "# 'NaiveNetwork'\n",
    "for model_name in ['NaiveNetwork', 'MostRecentlyInitiatedWith', 'MostRecentJournal', 'MostInitiatedWithRecently', 'simnet_all']:\n",
    "    sdf = week_df[week_df.model_name == model_name]\n",
    "    \n",
    "    # fit a model to check the linear slope over time\n",
    "    # (is MRR decreasing over time?)\n",
    "    md = smf.ols(formula='mrr ~ week', data=sdf)\n",
    "    res = md.fit()\n",
    "    #print(res.summary())\n",
    "    beta, p = res.params.week, res.pvalues.week\n",
    "    label = f\"{model_name} ($\\\\beta$={beta:.3f}, p<{p:.3f})\"\n",
    "    \n",
    "    # plot the data\n",
    "    plt.plot(sdf.week, sdf.mrr, label=label)\n",
    "    \n",
    "    \n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "#ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: f\"{datetime.utcfromtimestamp(x / 1000).strftime('%m/%d/%Y')}\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar sidebar: personalized recs for new authors?\n",
    "\n",
    "Option 1: compare first initiations to subsequent initations\n",
    "#Option 2: compare authors with exactly 3 journal updates to authors with > 3 journal updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate rank_df from above\n",
    "rank_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = rank_df[rank_df.model_name == 'field_study_model_experiment_219']\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath)]\n",
    "len(md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_initiated_user_ids = set()\n",
    "ds = []\n",
    "for md in md_list:\n",
    "    if md['type'] == 'ineligible' or md['type'] == 'predict':\n",
    "        continue\n",
    "    has_already_initiated = md['source_user_id'] in already_initiated_user_ids\n",
    "    if not has_already_initiated:\n",
    "        already_initiated_user_ids.add(md['source_user_id'])\n",
    "    ds.append({\n",
    "        'metadata_id': md['metadata_id'],\n",
    "        'type': md['type'],\n",
    "        'has_already_initiated': has_already_initiated,\n",
    "    })\n",
    "adf = pd.DataFrame(ds)\n",
    "len(adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.merge(adf, how='left', left_on='metadata_id', right_on='metadata_id')\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.has_already_initiated.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdf = sdf[(sdf.type == 'test')&(sdf.timestamp > VALIDATION_END_TIMESTAMP)]\n",
    "len(ssdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdf.groupby('has_already_initiated').agg({'target_rank': lambda tr: (1 / tr).mean()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: Create the needed coverage data\n",
    "\n",
    "Based on the sites available at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageHelper:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test' or md['type'] == 'predict']\n",
    "len(md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_md_list = [md for md in md_list if not md['has_target']]\n",
    "len(coverage_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coverage_predictions(config, coverage_md_list):\n",
    "    db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "    with db:\n",
    "        coverage_rcs = []\n",
    "        for test_context_md in tqdm(coverage_md_list, desc=\"Loading coverage data\"):\n",
    "            test_context = cbrec.featuredb.get_test_context_by_metadata_id(db, test_context_md['metadata_id'], config)\n",
    "            rc = cbrec.reccontext.RecContext.create_from_test_context(config, test_context_md, test_context)\n",
    "            coverage_rcs.append(rc)\n",
    "    return coverage_rcs\n",
    "\n",
    "\n",
    "cov_helper = CoverageHelper()\n",
    "    \n",
    "coverage_rcs = load_coverage_predictions(config, coverage_md_list)\n",
    "assert len(coverage_rcs) == 1000\n",
    "\n",
    "coverage_sites = set()\n",
    "for coverage_rc in coverage_rcs:\n",
    "    coverage_sites.update(set(coverage_rc.candidate_usp_arr[:,1]))\n",
    "coverage_sites = sorted(list(coverage_sites))\n",
    "print(f\"# eligible coverage sites: {len(coverage_sites)}\")\n",
    "cov_helper.coverage_sites = coverage_sites\n",
    "\n",
    "eligible_sites = set(coverage_sites)\n",
    "len(eligible_sites)\n",
    "cov_helper.eligible_sites = eligible_sites\n",
    "\n",
    "site_id_arr_map = {}\n",
    "for coverage_rc in coverage_rcs:\n",
    "    site_id_arr, _ = np.unique(coverage_rc.candidate_usp_arr[:,1], return_index=True)\n",
    "    assert len(site_id_arr) <= len(coverage_sites)\n",
    "    site_id_arr_map[coverage_rc.metadata_id] = site_id_arr\n",
    "cov_helper.site_id_arr_map = site_id_arr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_helper.timestamp = 1609502404437  # this is the timestamp when recommendations were generated for coverage\n",
    "assert cov_helper.timestamp == coverage_rc.timestamp\n",
    "datetime.utcfromtimestamp(cov_helper.timestamp/1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal metadata\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read interactions dataframe\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "ints_df = pd.read_feather(os.path.join(model_data_dir, 'ints_df.feather'))\n",
    "print(f\"Read {len(ints_df)} rows ({len(set(ints_df.user_id))} unique users) in {datetime.now() - s}.\")\n",
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_usp_set = set([(row.user_id, row.site_id) for row in journal_df.itertuples()])\n",
    "len(author_usp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df = ints_df.sort_values(by='created_at').drop_duplicates(subset=['user_id', 'site_id'], keep='first').copy()\n",
    "len(inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df['usp'] = [(row.user_id, row.site_id) for row in inits_df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df = inits_df[~inits_df.usp.isin(author_usp_set)]\n",
    "len(inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df = inits_df[inits_df.created_at < cov_helper.timestamp]\n",
    "len(inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_int_site_ids = set(inits_df.site_id)\n",
    "len(previous_int_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_with_previous_ints = previous_int_site_ids & cov_helper.eligible_sites\n",
    "len(sites_with_previous_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_helper.sites_with_previous_ints = sites_with_previous_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"num eligible sites with > 0 indegree: {len(sites_with_previous_ints)}\")\n",
    "print(f\"num eligible sites: {len(eligible_sites)}\")\n",
    "print(f\"pct > 0 indegree: {len(sites_with_previous_ints) / len(eligible_sites):.3%}\")\n",
    "print(f\"pct zero indegree: {1 - (len(sites_with_previous_ints) / len(eligible_sites)):.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = journal_df[(journal_df.published_at.notna())&(journal_df.published_at > 0)].sort_values(by='published_at').drop_duplicates(subset='site_id', keep='first')\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_first_journal_timestamp_map = sdf[sdf.site_id.isin(eligible_sites)].set_index('site_id').created_at.to_dict()\n",
    "len(site_first_journal_timestamp_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.array([cov_helper.timestamp - site_first_journal_timestamp_map[site_id] for site_id in coverage_sites])\n",
    "ages = ages / 1000 / 60 / 60 / 24 / 7  # convert to weeks\n",
    "len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median eligible site has been around for 93 weeks\n",
    "ages.min(), ages.mean(), ages.std(), np.median(ages), ages.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_helper.site_first_journal_timestamp_map = site_first_journal_timestamp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_helper.n = 5  # number of recs to make in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cov_helper to pickle\n",
    "coverage_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/feature_data/coverage\"\n",
    "with open(os.path.join(coverage_dir, 'cov_helper.pkl'), 'wb') as coverage_helper_file:\n",
    "    pickle.dump(cov_helper, coverage_helper_file)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of sidebar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make coverage predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageHelper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "# load cov_helper from pickle\n",
    "coverage_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/feature_data/coverage\"\n",
    "with open(os.path.join(coverage_dir, 'cov_helper.pkl'), 'rb') as coverage_helper_file:\n",
    "    cov_helper = pickle.load(coverage_helper_file)\n",
    "cov_helper.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coverage_metrics(model_coverage_scores, cov_helper):\n",
    "    recs = []\n",
    "    for scores_md in model_coverage_scores:\n",
    "        metadata_id = scores_md['metadata_id']\n",
    "        y_score_site = scores_md['scores']\n",
    "        site_id_arr = cov_helper.site_id_arr_map[metadata_id]\n",
    "        assert y_score_site.shape == site_id_arr.shape\n",
    "\n",
    "        # create rec batch\n",
    "        sort_inds = np.argsort(y_score_site)\n",
    "        # TODO need to compute ranks if there are ties; for now, we'll assume there aren't any ties\n",
    "        # in the case of ties, not clear what order argsort prefers\n",
    "        #ranks = rankdata(-1 * y_score_site, method='max')\n",
    "\n",
    "        #highest_scores = y_score_site[sort_inds[-cov_helper.n:]]\n",
    "        highest_score_site_ids = site_id_arr[sort_inds[-cov_helper.n:]]\n",
    "        recs.append(list(highest_score_site_ids))\n",
    "        \n",
    "    recced_sites = set()\n",
    "    for rec in recs:\n",
    "        recced_sites.update(rec)\n",
    "    nonrecced_sites = cov_helper.eligible_sites - recced_sites\n",
    "    \n",
    "    recced_inted = len(recced_sites & cov_helper.sites_with_previous_ints) / len(recced_sites)\n",
    "    nonrecced_inted = len(nonrecced_sites & cov_helper.sites_with_previous_ints) / len(nonrecced_sites)\n",
    "    \n",
    "    site_ages = []\n",
    "    for rec in recs:\n",
    "        ages = np.array([cov_helper.timestamp - cov_helper.site_first_journal_timestamp_map[site_id] for site_id in rec])\n",
    "        ages = ages / 1000 / 60 / 60 / 24 / 7  # convert to weeks\n",
    "        assert np.all(ages > 0)\n",
    "        site_ages.append({\n",
    "            'min': ages.min(),\n",
    "            #'mean': ages.mean(),\n",
    "            #'std': ages.std(),\n",
    "            'median': np.median(ages),\n",
    "            #'max': ages.max(),\n",
    "        })\n",
    "    mean_min_age = np.mean([a['min'] for a in site_ages])\n",
    "    mean_median_age = np.mean([a['median'] for a in site_ages])\n",
    "    \n",
    "    return {\n",
    "        'n_recced_sites': len(recced_sites),\n",
    "        'n_nonrecced_sites': len(nonrecced_sites),\n",
    "        'pct_eligible_recced': len(recced_sites) / len(cov_helper.eligible_sites),\n",
    "        'pct_unique_recs': len(recced_sites) / (5 * 1000),\n",
    "        'pct_recced_with_int': recced_inted,\n",
    "        'pct_nonrecced_with_int': nonrecced_inted,\n",
    "        'pct_recced_without_int': 1 - recced_inted,\n",
    "        'pct_nonrecced_without_int': 1 - nonrecced_inted,\n",
    "        'ratio_int': recced_inted / nonrecced_inted,\n",
    "        'ratio_noint': (1 - recced_inted) / (1 - nonrecced_inted),\n",
    "        'mean_min_age': mean_min_age,\n",
    "        'mean_median_age': mean_median_age,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.get_scores(subset=study_models + tuned_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_metrics_list = []\n",
    "for model_name in study_models + tuned_models:\n",
    "    model_coverage_scores = ev.models[model_name]['coverage_scores']\n",
    "    coverage_metrics = compute_coverage_metrics(model_coverage_scores, cov_helper)\n",
    "    coverage_metrics_list.append({\n",
    "        'model': 'study' if model_name in study_models else 'tuned',\n",
    "        'model_name': model_name,\n",
    "        **coverage_metrics\n",
    "    })\n",
    "pd.DataFrame(coverage_metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df = pd.DataFrame(coverage_metrics_list)\n",
    "#cov_df.set_index(['model', 'n_recced_sites']).loc[cov_df.groupby('model').n_recced_sites.median().reset_index().set_index(['model', 'n_recced_sites'])]\n",
    "cov_df.groupby('model').n_recced_sites.median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coverage_scores = ev.models['field_study_model_experiment_219']['coverage_scores']\n",
    "len(model_coverage_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coverage_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rec_df(model_coverage_scores, cov_helper):\n",
    "    recs = []\n",
    "    for scores_md in model_coverage_scores:\n",
    "        metadata_id = scores_md['metadata_id']\n",
    "        y_score_site = scores_md['scores']\n",
    "        site_id_arr = cov_helper.site_id_arr_map[metadata_id]\n",
    "        assert y_score_site.shape == site_id_arr.shape\n",
    "\n",
    "        # create rec batch\n",
    "        sort_inds = np.argsort(y_score_site)\n",
    "        # TODO need to compute ranks if there are ties; for now, we'll assume there aren't any ties\n",
    "        # in the case of ties, not clear what order argsort prefers\n",
    "        #ranks = rankdata(-1 * y_score_site, method='max')\n",
    "\n",
    "        #highest_scores = y_score_site[sort_inds[-cov_helper.n:]]\n",
    "        highest_score_site_ids = site_id_arr[sort_inds[-cov_helper.n:]]\n",
    "        recs.append({\n",
    "            'metadata_id': metadata_id,\n",
    "            'recced_site_ids': list(highest_score_site_ids),\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = create_rec_df(model_coverage_scores, cov_helper)\n",
    "len(rec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_initiated_user_ids = set()\n",
    "ds = []\n",
    "for md in md_list:\n",
    "    if md['type'] == 'ineligible':\n",
    "        continue\n",
    "    if md['type'] == 'predict':\n",
    "        break\n",
    "    has_already_initiated = md['source_user_id'] in already_initiated_user_ids\n",
    "    if not has_already_initiated:\n",
    "        already_initiated_user_ids.add(md['source_user_id'])\n",
    "len(already_initiated_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_id_to_user_list = {}\n",
    "for md in md_list:\n",
    "    if md['type'] != 'predict':\n",
    "        continue\n",
    "    metadata_id_to_user_list[md['metadata_id']] = md['source_user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df['user_id'] = rec_df.metadata_id.map(lambda mdid: metadata_id_to_user_list[mdid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df['has_already_initiated'] = rec_df.user_id.map(lambda uid: uid in already_initiated_user_ids)\n",
    "rec_df.has_already_initiated.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for has_already_initiated, group in rec_df.groupby('has_already_initiated'):\n",
    "    recced_site_ids = group.recced_site_ids\n",
    "    unique_sites_recced = set()\n",
    "    for l in recced_site_ids:\n",
    "        unique_sites_recced.update(l)\n",
    "    print(has_already_initiated, len(unique_sites_recced))\n",
    "    \n",
    "    site_ids = []\n",
    "    for l in group.recced_site_ids:\n",
    "        site_ids.extend(l)\n",
    "    value_counts = pd.Series(site_ids).value_counts()\n",
    "    ax = axes[0] if has_already_initiated == True else axes[1]\n",
    "    ax.hist(value_counts, bins=np.arange(0, 100, 5), log=True)\n",
    "    ax.set_xlabel(\"Number of times recommended\")\n",
    "    ax.set_ylabel(\"Number of sites\")\n",
    "    ax.set_title(f\"{has_already_initiated=}\")\n",
    "    print(f\"{len(value_counts)} {value_counts.median()} {value_counts.mean():.2f} {(value_counts == 1).sum() / len(value_counts):.2%}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpuonly",
   "language": "python",
   "name": "pytorch-cpuonly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
