{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import dateutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging; only run this cell once\n",
    "import logging\n",
    "use_cbrec_logging = True\n",
    "if not use_cbrec_logging:\n",
    "    # this is a demo of how to set up logging\n",
    "    # since we use cbrec logging below, this will be done for us when we call set_up_logging.\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.DEBUG)\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    root.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "\n",
    "import cbrec.genconfig\n",
    "\n",
    "# create a config, which is needed by lots of the components for resolving paths, etc.\n",
    "config = cbrec.genconfig.Config()\n",
    "\n",
    "import cbrec.evaluation\n",
    "import cbrec.reccontext\n",
    "import cbrec.featuredb\n",
    "import cbrec.torchmodel\n",
    "import cbrec.utils\n",
    "import cbrec.logutils\n",
    "import cbrec.feature_loader\n",
    "\n",
    "cbrec.logutils.set_up_logging()\n",
    "\n",
    "# turn off matplotlib logging\n",
    "# which can be quite verbose and usually is not useful\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filepath = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/adam_randomsearch_experiment_20220213194145/outputs/adam_randomsearch_experiment_test_metadata.ndjson\"\n",
    "assert os.path.exists(metadata_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list = []\n",
    "with open(metadata_filepath, 'r') as infile:\n",
    "    for line in infile:\n",
    "        md = json.loads(line)\n",
    "        md_list.append(md)\n",
    "len(md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(md_list, columns=['metadata_id', \n",
    "                                     'timestamp',\n",
    "                                     'source_user_id',\n",
    "                                     'target_site_id',\n",
    "                                     'is_test_period',\n",
    "                                     'n_source_sites',\n",
    "                                     'n_target_users',\n",
    "                                     'source_user_is_existing',\n",
    "                                     'n_existing_users_on_target_site',\n",
    "                                     'source_user_is_eligible',\n",
    "                                     'target_site_has_eligible_user',\n",
    "                                     'is_self_initiation',\n",
    "                                     'is_initiation_eligible',  \n",
    "                                     # and the features that come with being initiation eligible...\n",
    "                                     'n_eligible_users',\n",
    "                                     'n_eligible_coauthors',\n",
    "                                     'n_source_usps',\n",
    "                                     'n_active_user_ids',\n",
    "                                     'source_user_is_active',\n",
    "                                     'n_active_target_users',\n",
    "                                     'n_target_usps',\n",
    "                                     'n_eligible_inactive_users',\n",
    "                                     'n_existing_initiations_from_source_user_id',\n",
    "                                     'n_candidate_user_ids',\n",
    "                                     'n_candidate_usps',\n",
    "                                     # test-only features\n",
    "                                     'test_target_usp_adjustment',\n",
    "                                     'source_user_initiated_in_train_period', \n",
    "                                     'target_site_initiated_with_in_train_period',\n",
    "                                   ]\n",
    ")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [key[:-8] for key in md_list[0].keys() if key.endswith('_metrics') and key != 'baseline_metrics']\n",
    "metrics_list = []\n",
    "for model_name in model_names:\n",
    "    for md in md_list:\n",
    "        metrics = md[model_name + '_metrics']\n",
    "        metrics['model_name'] = model_name\n",
    "        metrics['metadata_id'] = md['metadata_id']\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "# also include the baseline results\n",
    "for md in tqdm(md_list):\n",
    "    baseline_models = md['baseline_metrics']\n",
    "    for model_name, metrics in baseline_models.items():\n",
    "        metrics['model_name'] = model_name\n",
    "        metrics['metadata_id'] = md['metadata_id']\n",
    "        metrics_list.append(metrics)\n",
    "        \n",
    "mdf = pd.DataFrame(metrics_list)\n",
    "len(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [key[:] for key in md_list[0].keys() if key.endswith('_metrics') and key != 'baseline_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['reciprocal_rank'] = 1 / mdf.target_rank\n",
    "for k in [1, 3, 5, 50]:\n",
    "    mdf[f'hr@{k}'] = mdf.target_rank <= k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.model_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.groupby('model_name')[['reciprocal_rank', 'hr@1', 'hr@3', 'hr@5', 'hr@50']].mean().rename(columns={'reciprocal_rank': 'mrr'}).sort_values(by='mrr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model: adam_randomsearch_experiment_40\n",
    "# configs:\n",
    "#   train_Adam_beta1: 0.8114556965716483\n",
    "#   train_Adam_beta2: 0.9027771890512277\n",
    "#   train_Adam_eps: 0.037702454558030354\n",
    "#   train_lr_init: 0.09156343147932729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = mdf.groupby('model_name')[['reciprocal_rank', 'hr@1', 'hr@3', 'hr@5', 'hr@50']].mean().rename(columns={'reciprocal_rank': 'mrr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 20\n",
    "new_list = new['mrr']\n",
    "n, bins, patches = plt.hist(new_list, num_bins, facecolor='blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 20\n",
    "new_list = new['hr@1']\n",
    "n, bins, patches = plt.hist(new_list, num_bins, facecolor='blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 20\n",
    "new_list = new['hr@3']\n",
    "n, bins, patches = plt.hist(new_list, num_bins, facecolor='blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 20\n",
    "new_list = new['hr@5']\n",
    "n, bins, patches = plt.hist(new_list, num_bins, facecolor='blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 20\n",
    "new_list = new['hr@50']\n",
    "n, bins, patches = plt.hist(new_list, num_bins, facecolor='blue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph config parameters\n",
    "# difference between stuff that doesn't\n",
    "adam_models = new[9:109]\n",
    "adam_models = adam_models['mrr']\n",
    "print(adam_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/adam_randomsearch_experiment_20220213194145/configs\"\n",
    "beta1_lst = []\n",
    "beta2_lst = []\n",
    "eps_lst = []\n",
    "lr_init_lst = []\n",
    "\n",
    "# creating a list of indexes that are lexigraphically sorted in correlation with model_name\n",
    "values = list(range(100))\n",
    "value_strings = []\n",
    "for val in values:\n",
    "    value_strings.append(str(val))\n",
    "value_strings.sort()\n",
    "sorted_indexes = []\n",
    "for val in value_strings:\n",
    "    sorted_indexes.append(int(val))\n",
    "    \n",
    "for i in sorted_indexes:\n",
    "    with open(f\"{config_path}/adam_randomsearch_experiment_{i}.json\", 'r') as fp:\n",
    "        config = json.load(fp)\n",
    "        beta1_lst.append(config['train_Adam_beta1'])\n",
    "        beta2_lst.append(config['train_Adam_beta2'])\n",
    "        eps_lst.append(config['train_Adam_eps'])\n",
    "        lr_init_lst.append(config['train_lr_init'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(beta1_lst, adam_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(beta2_lst, adam_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(eps_lst, adam_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lr_init_lst, adam_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpuonly",
   "language": "python",
   "name": "pytorch-cpuonly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
