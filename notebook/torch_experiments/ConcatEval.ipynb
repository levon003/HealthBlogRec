{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import dateutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging; only run this cell once\n",
    "import logging\n",
    "use_cbrec_logging = True\n",
    "if not use_cbrec_logging:\n",
    "    # this is a demo of how to set up logging\n",
    "    # since we use cbrec logging below, this will be done for us when we call set_up_logging.\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.DEBUG)\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    root.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "import cbrec.genconfig\n",
    "config = cbrec.genconfig.Config()\n",
    "import cbrec.evaluation\n",
    "import cbrec.reccontext\n",
    "import cbrec.featuredb\n",
    "import cbrec.torchmodel\n",
    "import cbrec.utils\n",
    "import cbrec.logutils\n",
    "import cbrec.feature_loader\n",
    "cbrec.logutils.set_up_logging()\n",
    "# turn off matplotlib logging\n",
    "# which can be quite verbose and usually is not useful\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "output_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/torch_experiments/modeling/concatnet_20220329151435/outputs\"\n",
    "assert os.path.exists(output_dir)\n",
    "def identify_model_filepaths(model_dir):\n",
    "    logger = logging.getLogger(\"cbrec.modeling.submitEvalFromDirectory.identify_model_filepaths\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        raise ValueError(f\"Dir '{model_dir}' does not exist.\")\n",
    "    model_filepaths = []\n",
    "    for model_filepath in glob(os.path.join(model_dir, '*.ndjson')):\n",
    "        model_filepaths.append(model_filepath)\n",
    "    if len(model_filepaths) == 0:\n",
    "        raise ValueError(f\"No .ndjson files in dir '{model_dir}'.\")\n",
    "    logger.info(f\"Identified {len(model_filepaths)} model filepaths in dir {model_dir}.\")\n",
    "    return model_filepaths\n",
    "model_filepaths = identify_model_filepaths(output_dir)\n",
    "all_dfs = []\n",
    "metrics_list = []\n",
    "count = 0\n",
    "num_files = len(model_filepaths)\n",
    "\n",
    "for i in tqdm(range(num_files)):\n",
    "    metadata_filepath = model_filepaths[i]\n",
    "    assert os.path.exists(metadata_filepath)\n",
    "    md_list = []\n",
    "    with open(metadata_filepath, 'r') as infile:\n",
    "        for line in infile:\n",
    "            md = json.loads(line)\n",
    "            md_list.append(md)\n",
    "        model_names = [key[:-8] for key in md_list[0].keys() if key.endswith('_metrics') and key != 'baseline_metrics']\n",
    "        for model_name in model_names:\n",
    "            for md in md_list:\n",
    "                metrics = md[model_name + '_metrics']\n",
    "                metrics['model_name'] = model_name\n",
    "                metrics['metadata_id'] = md['metadata_id']\n",
    "                metrics_list.append(metrics)       \n",
    "# also include the baseline results\n",
    "        if count == 0:\n",
    "            count = count + 1\n",
    "            for md in tqdm(md_list):\n",
    "                baseline_models = md['baseline_metrics']\n",
    "                for model_name, metrics in baseline_models.items():\n",
    "                    metrics['model_name'] = model_name\n",
    "                    metrics['metadata_id'] = md['metadata_id']\n",
    "                    metrics_list.append(metrics)\n",
    "mdf = pd.DataFrame(metrics_list)\n",
    "len(mdf)\n",
    "mdf['mrr'] = 1 / mdf.target_rank\n",
    "for k in [1, 3, 5, 50]:\n",
    "    mdf[f'hr@{k}'] = mdf.target_rank <= k\n",
    "mdf.groupby('model_name').mrr.mean().sort_values(ascending=False)\n",
    "mdf.groupby('model_name')[['mrr', 'hr@1', 'hr@3', 'hr@5', 'hr@50']].mean().sort_values(by='mrr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.groupby('model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.groupby('model_name')[\"vals\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.groupby('model_name').mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mdf.groupby('model_name').mrr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.groupby('model_name').mrr.mean().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpuonly",
   "language": "python",
   "name": "pytorch-cpuonly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
