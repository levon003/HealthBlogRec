{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction Extraction\n",
    "===\n",
    "\n",
    "This file processes all interactions on CaringBridge into files.\n",
    "\n",
    "It processes:\n",
    " - Journal updates\n",
    " - Comments (standalone and from guestbooks)\n",
    " - Guestbooks\n",
    " - Amps (from updates, comments, and guestbooks)\n",
    "\n",
    "The fields in the resulting data-frame:\n",
    "\n",
    " - user_id\n",
    " - site_id\n",
    " - interaction_type (update, comment, guestbook, amp)\n",
    " - interaction_oid\n",
    " - parent_type (update, comment, guestbook)\n",
    " - parent_id (depending on parent, either a journal_oid, a comment_oid, or a gb_oid) \n",
    " - created_at\n",
    " - updated_at\n",
    "\n",
    "Other keys:\n",
    "- ancestor_type (either an update or a guestbook) - do we need this for any reason? I think not\n",
    "\n",
    "Question: do we need to do this for journal updates, or do we essentially already have that data?\n",
    "I think we essentially have it. But, so we can straightforwardly stream the file, I'm generating it anyway?\n",
    "\n",
    "The pseudocode process for using this data looks like:\n",
    " - Load interaction\n",
    " - Update new eligible authors based on time elapsed\n",
    "   - Special case: if no time elapsed (because this interaction occurred at the literal same timestamp as the last), do nothing and re-use e.g. eligible author pools without recomputing them\n",
    "   - Identify any user/site pairs with an \"initial join\" time (i.e. their 3rd published update on some site)\n",
    "   - Do this by just slowly chewing through a stack sorted by user/site \"initial join\" time\n",
    "   - Update site and author maps: add new authors to sites, and sites to authors.  Q: is this something that depends on FIRST update on a site, or THIRD update? In the CSCW project, we used existing + future links based on author type (being a patient). Here, seems like we just want to use current authorship i.e. THIRD update.\n",
    " - Update activity map\n",
    "   - Add any intervening journal updates as activity\n",
    "   - Remove any \"timedout\" users\n",
    " - Identify user/site sources and targets\n",
    " - Negative sample alt author(s) for each user/site source/target combination\n",
    "   - Note: need a network of authors to sites they've interacted with (independent from the author-author network)\n",
    " - Generate activity features for all implicated\n",
    " - Generate network features for all implicated\n",
    " - Retrieve other cached features (such as from journal texts)\n",
    " - Store triples in database\n",
    " - Update network connections\n",
    "   - Use site maps: add link to authors that are in the current site map. see Q above.\n",
    "   - Point of confusion: u1 is active on s1 and u2 is active on s2 at t, u3 interacts with s2 at t+1, u1 is active on s2 at t+2. \n",
    "   What should happen? at t+1, u3->u2 exists. At t+2, u3->u1 exists.  \n",
    "   How to identify that this edge should exist?  Well, could just look at in-bound edges of u2 if u1 becomes active on u2s site, then duplicate those edges to refer to u1.  But, the problem is that not all connections to u2 are because of s2.  \n",
    "   Solution: store (u1,s1) pairs as nodes in the network. Problem: u1 has interacted with a site before any eligible person exists on that site, or u1 interacts before having any eligible sites themselves. Possibility: create \"dummy\" nodes, e.g. (u1,s1)->(*,s2) and (u1,*)->(u2,s2) respectively, that are special nodes when user info is not yet available. \n",
    "   Second thought: this is clearly absurd.  In particular, we KNOW when a user becomes active on a site (in the user_site_df).  \n",
    "   So thought 1 is two separate networks: user->site, site->user\n",
    "   Thought 2: just a user->site network. At time t, can just resolve each site to 0 or more users (if it resolves to 0 users, then we have updated the network with the new edge but can't generate any new sites. If it resolves to 2+ users, then we generate one new interaction for each users.)\n",
    "   But this is clearly a disaster; to determine something like weakly-connected component, you need to resolve all the sites into nodes. In other words, this is just a bipartite graph by another face.\n",
    "   Thought 3: user->user network.  But need a separate store of user->site links, just as a list or dict or whatever, that can be used when a user becomes active on a site; we retrieve all existing user->site links, then construct the corresponding user->user links in the graph.  (This idea seems really reasonable to me!)\n",
    "   Basically, do two activities:\n",
    "   1. When interaction u1->s2 happens, put {s2: u1} in dictionary, get current eligible users of s2 (get d[s2] where d is site_id->set(user_id), add edges (u1,uX for X in d[s2]).\n",
    "   2. When u3 becomes active on s2, d[s2].add(u3), for each user uX in edge dict[s2], create edge (uX, u3).\n",
    "   It's okay to have user nodes in the graph that aren't yet active! (TODO check this assumption)\n",
    "   \n",
    " - Add to activity map with the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import sqlite3\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['font.family'] = \"serif\"\n",
    "\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbcore.data.paths as paths\n",
    "import cbcore.data.dates as dates\n",
    "import cbcore.data.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = paths.raw_data_filepath\n",
    "raw_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_dir = os.path.join(paths.derived_data_filepath, 'interactions')\n",
    "interactions_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {interactions_dir}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /home/lana/shared/caringbridge/data/derived/interactions/reaction.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data\"\n",
    "assert os.path.exists(working_dir)\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of valid user/site pairs\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "user_site_df = pd.read_csv(os.path.join(model_data_dir, 'user_site_df.csv'))\n",
    "valid_user_ids = set(user_site_df.user_id)\n",
    "valid_site_ids = set(user_site_df.site_id)\n",
    "print(f\"Read {len(user_site_df)} rows ({len(valid_user_ids)} unique users, {len(valid_site_ids)} unique sites) in {datetime.now() - s}.\")\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = os.path.join(working_dir, 'ints_filtered.csv')\n",
    "with open(output_filepath, 'w') as outfile:\n",
    "    for filename in ['reaction.csv', 'amps.csv', 'comment.csv', 'guestbook.csv']:\n",
    "        input_filepath = os.path.join(interactions_dir, filename)\n",
    "        both_valid_count = 0\n",
    "        neither_valid_count = 0\n",
    "        author_valid_count = 0\n",
    "        site_valid_count = 0\n",
    "        with open(input_filepath, 'r') as infile:\n",
    "            for line in tqdm(infile, desc=filename):\n",
    "                # columns: user_id, site_id, interaction_type, interaction_oid, parent_type, parent_id, ancestor_type, ancestor_id, created_at, updated_at\n",
    "                tokens = line.strip().split(\",\")\n",
    "                user_id = int(tokens[0])\n",
    "                site_id = int(tokens[1])\n",
    "                \n",
    "                is_user_valid = user_id in valid_user_ids\n",
    "                is_site_valid = site_id in valid_site_ids\n",
    "                if is_user_valid and is_site_valid:\n",
    "                    outfile.write(line)\n",
    "                    both_valid_count += 1\n",
    "                elif is_user_valid and not is_site_valid:\n",
    "                    author_valid_count += 1\n",
    "                elif not is_user_valid and is_site_valid:\n",
    "                    site_valid_count += 1\n",
    "                else:\n",
    "                    neither_valid_count += 1\n",
    "        print(f\"{filename:>15} ; Both valid = {both_valid_count:>8} ; Author valid = {author_valid_count:>8} ; Site valid = {site_valid_count:>8} ; Neither valid = {neither_valid_count:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {output_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user_id', 'site_id', 'interaction_type', 'interaction_oid', 'parent_type', 'parent_oid', 'ancestor_type', 'ancestor_oid', 'created_at', 'updated_at']\n",
    "s = datetime.now()\n",
    "ints_filepath = output_filepath\n",
    "ints_df = pd.read_csv(ints_filepath, header=None, names=cols)\n",
    "print(datetime.now() - s)\n",
    "len(ints_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "ints_df = ints_df.sort_values(by='created_at')\n",
    "print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "ints_df.reset_index(drop=True).to_feather(os.path.join(working_dir, 'ints_df.feather'))\n",
    "print(datetime.now() - s)\n",
    "#s = datetime.now()\n",
    "#ints_df.to_csv(os.path.join(working_dir, 'ints_df.csv'), index=False)\n",
    "#print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read interactions dataframe\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "ints_df = pd.read_feather(os.path.join(model_data_dir, 'ints_df.feather'))\n",
    "print(f\"Read {len(ints_df)} rows ({len(set(ints_df.user_id))} unique users) in {datetime.now() - s}.\")\n",
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ints_df.interaction_type.value_counts().rename('interaction_type_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ints_df.parent_type.value_counts(dropna=False).rename('parent_type_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(ints_df[['interaction_type', 'parent_type', 'ancestor_type']].value_counts().rename('count'))\n",
    "#ints_df[['interaction_type', 'parent_type', 'ancestor_type']].value_counts(dropna=False)\n",
    "# this doesn't work due to NoneType objects...\n",
    "pd.crosstab(ints_df.interaction_type, [ints_df.parent_type, ints_df.ancestor_type], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "start_time = datetime.strptime('2005-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.utcfromtimestamp(np.max(ints_df.created_at) / 1000).replace(tzinfo=pytz.UTC) #datetime.strptime('2021-07-15', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type.str.startswith('amp')].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Amps')\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'guestbook'].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks')\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'comment'].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Comments')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.axvline(datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "#plt.axvline(datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Interactions per month\")\n",
    "plt.title(f\"{len(ints_df):,} interactions from {len(set(ints_df.user_id)):,} unique users on {len(set(ints_df.site_id)):,} unique sites\")\n",
    "\n",
    "x_dates = [start_time + relativedelta(years=i) for i in range(18)]\n",
    "ax.set_xticks([d.timestamp() * 1000 for d in x_dates])\n",
    "ax.set_xticklabels([f\"Jan\\n\" + d.strftime('%Y')[2:] for i, d in enumerate(x_dates)])\n",
    "\n",
    "#newline = '\\n'\n",
    "#xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").replace(tzinfo=pytz.UTC).timestamp() for i in range((2020 - 2005) + 2)]\n",
    "#plt.xticks(\n",
    "#    xticks, \n",
    "#    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timestamp = datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "#end_timestamp = datetime.fromisoformat(f\"2021-09-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "#sdf = ints_df[(ints_df.created_at >= start_timestamp)&(ints_df.created_at <= end_timestamp)]\n",
    "sdf = ints_df[ints_df.created_at >= start_timestamp]\n",
    "len(sdf), len(ints_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf[['interaction_type', 'parent_type', 'ancestor_type']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "start_time = datetime.strptime('2021-05-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "sdf = ints_df[ints_df.created_at >= start_time.timestamp() * 1000]\n",
    "\n",
    "curr_time = start_time\n",
    "end_time = datetime.strptime('2021-12-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(days=1)\n",
    "bins.append(int(curr_time.timestamp() * 1000))\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(sdf[sdf.interaction_type.str.startswith('amp')].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Amps')\n",
    "total_counts, bin_edges = np.histogram(sdf[sdf.interaction_type == 'guestbook'].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks')\n",
    "total_counts, bin_edges = np.histogram(sdf[sdf.interaction_type == 'comment'].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Comments')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "#plt.axvline(datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "#plt.axvline(datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Interactions per day\")\n",
    "plt.title(f\"{len(sdf):,} interactions from {len(set(sdf.user_id)):,} unique users on {len(set(sdf.site_id)):,} unique sites\")\n",
    "\n",
    "curr_time = start_time\n",
    "tick_bins = []\n",
    "while curr_time < end_time:\n",
    "    tick_bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "tick_bins.append(int(curr_time.timestamp() * 1000))\n",
    "\n",
    "ax.set_xticks(tick_bins)\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: f\"{datetime.utcfromtimestamp(x / 1000).strftime('%m-%d')}\"))\n",
    "\n",
    "#ax.set_xticklabels([f\"Jan\\n\" + d.strftime('%Y')[2:] for i, d in enumerate(x_dates)])\n",
    "\n",
    "#newline = '\\n'\n",
    "#xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").replace(tzinfo=pytz.UTC).timestamp() for i in range((2020 - 2005) + 2)]\n",
    "#plt.xticks(\n",
    "#    xticks, \n",
    "#    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal dataframe with the index\n",
    "# this is all the new journal data\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(journal_df.created_at.quantile(0.0001) / 1000).isoformat(),\\\n",
    "datetime.utcfromtimestamp(journal_df.created_at.quantile(0.999999) / 1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# journal updates over time, by month\n",
    "\n",
    "start_time = datetime.strptime('2005-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.strptime('2021-09-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "total_counts, bin_edges = np.histogram(journal_df.created_at, bins=bins)\n",
    "ax.axhline(0, color='gray', alpha=0.4)\n",
    "ax.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2)\n",
    "\n",
    "# start of analysis period\n",
    "ax.axvline(datetime.fromisoformat(\"2014-01-01\").timestamp() * 1000, color='gray', linestyle='--', alpha=0.4)\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick / 1000).strftime('%b %Y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "start_time = datetime.strptime('2005-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.utcfromtimestamp(np.max(ints_df.created_at) / 1000).replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type.str.startswith('amp')].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Amps')\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'guestbook'].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks')\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'comment'].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Comments')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(journal_df.created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Journals')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.axvline(datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "#plt.axvline(datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Interactions per month\")\n",
    "plt.title(f\"{len(ints_df):,} interactions from {len(set(ints_df.user_id)):,} unique users on {len(set(ints_df.site_id)):,} unique sites\\n{len(journal_df):,} journals from {len(set(journal_df.user_id)):,} unique users on {len(set(journal_df.site_id)):,} unique sites\")\n",
    "\n",
    "x_dates = [start_time + relativedelta(years=i) for i in range(18)]\n",
    "ax.set_xticks([d.timestamp() * 1000 for d in x_dates])\n",
    "ax.set_xticklabels([f\"Jan\\n\" + d.strftime('%Y')[2:] for i, d in enumerate(x_dates)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For journals, createdAt vs publishedAt\n",
    "# this is the percentage of the time creation date == publication date\n",
    "np.sum(journal_df.published_at == journal_df.created_at) / len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating timestamp issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = ints_df[(ints_df.interaction_type == 'comment')&(ints_df.parent_type == 'journal')]\n",
    "len(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:\n",
    "    quantile = np.quantile(cdf.created_at, q)\n",
    "    dt = datetime.utcfromtimestamp(int(quantile) / 1000).isoformat()\n",
    "    print(f\"{q:.3f} {quantile:.3f} {dt}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = journal_df[journal_df.published_at.notna()]\n",
    "for q in [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:\n",
    "    quantile = np.quantile(sdf.published_at, q)\n",
    "    dt = datetime.utcfromtimestamp(int(quantile) / 1000).isoformat()\n",
    "    print(f\"{q:.3f} {quantile:.3f} {dt}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = journal_df[journal_df.created_at.notna()]\n",
    "published_at_index = sdf.set_index('journal_oid').created_at\n",
    "len(published_at_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = ints_df[(ints_df.interaction_type == 'comment')&(ints_df.parent_type == 'journal')]\n",
    "print(len(cdf))\n",
    "cdf = cdf[cdf.parent_oid.isin(published_at_index.index)]\n",
    "len(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf['journal_published_at'] = cdf.parent_oid.map(lambda journal_oid: published_at_index.at[journal_oid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf['time_to_comment'] = cdf.created_at - cdf.journal_published_at\n",
    "np.min(cdf.time_to_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:\n",
    "    quantile = np.quantile(cdf.time_to_comment, q)\n",
    "    dt = quantile / 1000 / 60 / 60\n",
    "    print(f\"{q:.3f} {quantile:.3f} {dt:.3f}hrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seconds_since_midnight(ts):\n",
    "    dt = datetime.fromtimestamp(int(ts) / 1000).astimezone(pytz.UTC)\n",
    "    dts = dt.strftime('%H%M%S')\n",
    "    hour, minute, second = int(dts[0:2]), int(dts[2:4]), int(dts[4:6])\n",
    "    seconds_since_midnight = second + minute * 60 + 60 * 60 * hour\n",
    "    return seconds_since_midnight\n",
    "\n",
    "get_seconds_since_midnight(1637262305000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_since_midnight = cdf.created_at.sample(n=100000).map(get_seconds_since_midnight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = np.arange(0, 24 * 60 * 60, 60 * 60)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "for int_type in ['reaction', 'comment', 'guestbook', 'journal']:\n",
    "    if int_type == 'guestbook':\n",
    "        created_at = ints_df[ints_df.interaction_type == int_type].created_at.sample(n=1000000)\n",
    "    elif int_type == 'journal':\n",
    "        created_at = journal_df[journal_df.published_at.notna()].published_at.sample(n=1000000)\n",
    "    elif int_type == 'reaction':\n",
    "        created_at = ints_df[(ints_df.interaction_type.str.startswith('amp_'))&(ints_df.parent_type == 'journal')].created_at\n",
    "    else:\n",
    "        created_at = ints_df[(ints_df.interaction_type == int_type)&(ints_df.parent_type == 'journal')].created_at.sample(n=1000000)\n",
    "    seconds_since_midnight = created_at.map(get_seconds_since_midnight)\n",
    "    counts, _ = np.histogram(seconds_since_midnight, hours)\n",
    "    pcts = counts / np.sum(counts)\n",
    "    ax.plot(hours[1:], pcts, label=int_type)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xticks(hours)\n",
    "ax.set_xticklabels([f\"{hour / 60 / 60:.0f}\" for hour in hours])\n",
    "\n",
    "ax.set_xlabel(\"Hour of day (UTC)\")\n",
    "ax.set_ylabel(\"% of ints during this hour\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bson.objectid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated_at_from_uuid(uuid):\n",
    "    generated_at = int(bson.objectid.ObjectId(uuid).generation_time.timestamp())\n",
    "    return generated_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = journal_df.sample(n=100000)\n",
    "generated_at_list = []\n",
    "for row in sdf.itertuples():\n",
    "    generated_at = get_generated_at_from_uuid(row.journal_oid)\n",
    "    generated_at_list.append(generated_at)\n",
    "generated_at = pd.Series(data=generated_at_list, name='generated_at')\n",
    "generated_at.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "start_time = datetime.fromtimestamp(generated_at.min()).replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.fromtimestamp(generated_at.max()).replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(generated_at * 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Generated At')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(sdf.created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Created At')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(sdf.published_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Published At')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "#plt.axvline(datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "#plt.axvline(datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Journals per year\")\n",
    "\n",
    "#x_dates = [start_time + relativedelta(years=i) for i in range(25)]\n",
    "#ax.set_xticks([d.timestamp() * 1000 for d in x_dates])\n",
    "#ax.set_xticklabels([f\"Jan\\n\" + d.strftime('%Y')[2:] for i, d in enumerate(x_dates)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_at.map(lambda ts: datetime.fromtimestamp(int(ts / 60 / 60) * 60 * 60).isoformat()).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = (np.array(generated_at) * 1000) - np.array(sdf.created_at)\n",
    "plt.hist(diffs / 1000 / 60 / 60 / 24, bins=np.arange(-20, 20), log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "start_time = datetime.fromtimestamp(generated_at.min()).replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.fromtimestamp(generated_at.max()).replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(generated_at * 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Generated At')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(sdf.created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Created At')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(sdf.published_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Published At')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "#plt.axvline(datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "#plt.axvline(datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Journals per year\")\n",
    "\n",
    "#x_dates = [start_time + relativedelta(years=i) for i in range(25)]\n",
    "#ax.set_xticks([d.timestamp() * 1000 for d in x_dates])\n",
    "#ax.set_xticklabels([f\"Jan\\n\" + d.strftime('%Y')[2:] for i, d in enumerate(x_dates)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment dates analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import time\n",
    "input_filepath = os.path.join(paths.raw_data_dir, 'comment_scrubbed.json.gz')\n",
    "with gzip.open(input_filepath, 'rt', encoding='utf-8') as infile, open('comment_dates.csv', 'w') as outfile:\n",
    "    for i, line in tqdm(enumerate(infile), total=48494407):\n",
    "        comment = json.loads(line)\n",
    "        comment_oid = comment['_id']['$oid']\n",
    "        created_at = comment['createdAt']\n",
    "        time_str = created_at['$date']\n",
    "        \n",
    "        if type(time_str) is str and \"Z\" in time_str:\n",
    "            if time_str.startswith(\"-\"):  # e.g. -0001\n",
    "                date_type = 'invalid_date'\n",
    "                created_at = None\n",
    "            else:\n",
    "                naive = datetime.strptime(time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                if bool(time.localtime(naive.timestamp()).tm_isdst):\n",
    "                    local = (naive.timestamp() * 1000) - 18000000\n",
    "                    date_type = 'dst_datestring'\n",
    "                else:\n",
    "                    local = (naive.timestamp() * 1000) - 21600000\n",
    "                    date_type = 'nondst_datestring'\n",
    "                created_at = int(local)\n",
    "        else:\n",
    "            created_at = int(time_str)\n",
    "            date_type = 'int'\n",
    "        outfile.write(comment_oid + ',' + date_type + ',' + str(created_at) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head comment_dates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "type_counts = defaultdict(int)\n",
    "with open('comment_dates.csv', 'r') as infile:\n",
    "    for line in tqdm(infile, total=48494407):\n",
    "        tokens = line.split(\",\")\n",
    "        if len(tokens) == 3:\n",
    "            date_type = tokens[1]\n",
    "            type_counts[date_type] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_comment_dates():\n",
    "    with open('comment_dates.csv', 'r') as infile:\n",
    "        for line in tqdm(infile, total=48494407):\n",
    "            tokens = line.strip().split(\",\")\n",
    "            if len(tokens) == 3:\n",
    "                generated_at = get_generated_at_from_uuid(tokens[0])\n",
    "                date_type = tokens[1]\n",
    "                created_at = int(tokens[2])\n",
    "                yield date_type, created_at, generated_at\n",
    "\n",
    "            \n",
    "df = pd.DataFrame(generate_comment_dates(), columns=['date_type','created_at', 'generated_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['generated_at'] = df.generated_at * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp_match'] = df.created_at == df.generated_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp_diff'] = df.created_at - df.generated_at\n",
    "df.timestamp_diff.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.timestamp_match, df.date_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df[~df.timestamp_match]\n",
    "diffs = sdf.created_at - sdf.generated_at\n",
    "plt.hist(diffs / 1000 / 60 / 60, bins=np.arange(-20, 20, step=0.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "start_time = datetime.fromtimestamp(df.generated_at.min() / 1000).replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.fromtimestamp(df.generated_at.max() / 1000).replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "index = (df.timestamp_diff >= -1000)&(df.timestamp_diff <= 1000)\n",
    "total_counts, bin_edges = np.histogram(df[index].generated_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'CreatedAt/UUID match ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "index = df.timestamp_diff.isin([-18000000, -21600000, -17999000, -21599000])\n",
    "total_counts, bin_edges = np.histogram(df[index].generated_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'UTC/CST offset ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "index = ((df.timestamp_diff < -1000)|(df.timestamp_diff > 1000))&(~df.timestamp_diff.isin([-18000000, -21600000, -17999000, -21599000]))\n",
    "total_counts, bin_edges = np.histogram(df[index].generated_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'Other offset ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "#plt.axvline(datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "#plt.axvline(datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000, color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Comments per month\")\n",
    "\n",
    "x_dates = [start_time + relativedelta(years=i) for i in range((len(bins) // 12) + 1)]\n",
    "ax.set_xticks([d.timestamp() * 1000 for d in x_dates])\n",
    "ax.set_xticklabels([f\"Jan\\n'\" + d.strftime('%Y')[2:] for i, d in enumerate(x_dates)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like 1-hour offsets are somewhat common\n",
    "index = ((df.timestamp_diff < -1000)|(df.timestamp_diff > 1000))&(~df.timestamp_diff.isin([-18000000, -21600000, -17999000, -21599000]))\n",
    "other_diffs = df[index].timestamp_diff / 1000 / 60 / 60\n",
    "plt.hist(other_diffs, log=True, bins=np.linspace(other_diffs.min(), other_diffs.max()))\n",
    "plt.xlabel(\"Difference between creation and generation timestamp (hours)\")\n",
    "plt.show()\n",
    "other_diffs.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "start_time = datetime.fromisoformat(f\"2018-02-01\").replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.fromisoformat(f\"2019-02-15\").replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(days=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "index = (df.timestamp_diff >= -1000)&(df.timestamp_diff <= 1000)\n",
    "total_counts, bin_edges = np.histogram(df[index].generated_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'CreatedAt/UUID match ({np.sum(index) / len(index):.2%})')\n",
    "match_counts = total_counts\n",
    "\n",
    "index = df.timestamp_diff.isin([-18000000, -21600000, -17999000, -21599000])\n",
    "total_counts, bin_edges = np.histogram(df[index].generated_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'UTC/CST offset ({np.sum(index) / len(index):.2%})')\n",
    "utc_offset_counts = total_counts\n",
    "\n",
    "index = ((df.timestamp_diff < -1000)|(df.timestamp_diff > 1000))&(~df.timestamp_diff.isin([-18000000, -21600000, -17999000, -21599000]))\n",
    "total_counts, bin_edges = np.histogram(df[index].generated_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'Other offset ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\"Comments per month\")\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: datetime.utcfromtimestamp(x / 1000).replace(tzinfo=pytz.timezone('US/Central')).strftime(\"%Y\\n%m/%d\")))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I visually inspected the index where match_counts spiked, in order to identify the day\n",
    "match_counts[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, I figured out which day it was\n",
    "datetime.utcfromtimestamp(bin_edges[56] / 1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.sort_values(by='generated_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, I manually identified the \"switchover\" point\n",
    "sdf[sdf.generated_at >= bin_edges[56] + 1000 * 60 * 60 * 3.23].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on visual inspection, the switch-over generated_at timestamp is Thursday, March 29, 2018 3:14:41 AM\n",
    "comments_correct_generated_at_timestamp = 1522293281000\n",
    "comments_correct_created_at_timestamp = 1522275282000\n",
    "comments_correct_generated_at_timestamp - comments_correct_created_at_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the fix works...\n",
    "hours = np.arange(0, 24 * 60 * 60, 60 * 60)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "for int_type in ['reaction', 'comment', 'guestbook', 'journal']:\n",
    "    if int_type == 'guestbook':\n",
    "        created_at = ints_df[ints_df.interaction_type == int_type].created_at.sample(n=1000000)\n",
    "    elif int_type == 'journal':\n",
    "        created_at = journal_df[journal_df.published_at.notna()].published_at.sample(n=1000000)\n",
    "    elif int_type == 'reaction':\n",
    "        created_at = ints_df[(ints_df.interaction_type.str.startswith('amp_'))&(ints_df.parent_type == 'journal')].created_at\n",
    "    else:\n",
    "        created_at = ints_df[(ints_df.interaction_type == int_type)&(ints_df.parent_type == 'journal')].created_at.sample(n=1000000)\n",
    "        created_at[created_at <= comments_correct_created_at_timestamp] += 18000000\n",
    "    seconds_since_midnight = created_at.map(get_seconds_since_midnight)\n",
    "    counts, _ = np.histogram(seconds_since_midnight, hours)\n",
    "    pcts = counts / np.sum(counts)\n",
    "    ax.plot(hours[1:], pcts, label=int_type)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xticks(hours)\n",
    "ax.set_xticklabels([f\"{hour / 60 / 60:.0f}\" for hour in hours])\n",
    "\n",
    "ax.set_xlabel(\"Hour of day (UTC)\")\n",
    "ax.set_ylabel(\"% of ints during this hour\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import time\n",
    "input_filepath = os.path.join(paths.raw_data_dir, 'guestbook_scrubbed.json.gz')\n",
    "with gzip.open(input_filepath, 'rt', encoding='utf-8') as infile, open('gb_dates.csv', 'w') as outfile:\n",
    "    for i, line in tqdm(enumerate(infile), total=82680259):\n",
    "        if i < 4002:\n",
    "            continue\n",
    "        gb = json.loads(line)\n",
    "        if '_id' in gb and '$oid' in gb['_id']:\n",
    "            guestbook_oid = gb['_id']['$oid']\n",
    "        else:\n",
    "            continue\n",
    "        if 'createdAt' not in gb:\n",
    "            continue\n",
    "        created_at = gb['createdAt']\n",
    "        time_str = created_at['$date']\n",
    "        \n",
    "        if type(time_str) is str and \"Z\" in time_str:\n",
    "            if time_str.startswith(\"-\"):  # e.g. -0001\n",
    "                date_type = 'invalid_date'\n",
    "                created_at = None\n",
    "            else:\n",
    "                naive = datetime.strptime(time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                if bool(time.localtime(naive.timestamp()).tm_isdst):\n",
    "                    local = (naive.timestamp() * 1000) - 18000000\n",
    "                    date_type = 'dst_datestring'\n",
    "                else:\n",
    "                    local = (naive.timestamp() * 1000) - 21600000\n",
    "                    date_type = 'nondst_datestring'\n",
    "                created_at = int(local)\n",
    "        else:\n",
    "            created_at = int(time_str)\n",
    "            date_type = 'int'\n",
    "        outfile.write(guestbook_oid + ',' + date_type + ',' + str(created_at) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "type_counts = defaultdict(int)\n",
    "with open('gb_dates.csv', 'r') as infile:\n",
    "    for line in tqdm(infile, total=82686297):\n",
    "        tokens = line.split(\",\")\n",
    "        if len(tokens) == 3:\n",
    "            date_type = tokens[1]\n",
    "            type_counts[date_type] += 1\n",
    "type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gb_dates(limit=10000000):\n",
    "    with open('gb_dates_shuffled.csv', 'r') as infile:\n",
    "        i = 0\n",
    "        for line in tqdm(infile, total=min(82686297, limit)):\n",
    "            tokens = line.strip().split(\",\")\n",
    "            if len(tokens) == 3:\n",
    "                i += 1\n",
    "                generated_at = get_generated_at_from_uuid(tokens[0])\n",
    "                date_type = tokens[1]\n",
    "                created_at = int(tokens[2])\n",
    "                yield date_type, created_at, generated_at\n",
    "                if i >= limit:\n",
    "                    return\n",
    "\n",
    "            \n",
    "df = pd.DataFrame(generate_gb_dates(), columns=['date_type','created_at', 'generated_at'])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['generated_at'] = df.generated_at * 1000\n",
    "df['timestamp_match'] = df.created_at == df.generated_at\n",
    "df['timestamp_diff'] = df.created_at - df.generated_at\n",
    "df.timestamp_diff.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "start_time = datetime.fromtimestamp(df.generated_at.min() / 1000).replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.fromtimestamp(df.generated_at.max() / 1000).replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "index = (df.timestamp_diff >= -1000)&(df.timestamp_diff <= 1000)\n",
    "total_counts, bin_edges = np.histogram(df[index].generated_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'CreatedAt/UUID match ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "index = df.timestamp_diff.isin([-18000000, -21600000, -17999000, -21599000])\n",
    "total_counts, bin_edges = np.histogram(df[index].generated_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'UTC/CST offset ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "index = ((df.timestamp_diff < -1000)|(df.timestamp_diff > 1000))&(~df.timestamp_diff.isin([-18000000, -21600000, -17999000, -21599000]))\n",
    "total_counts, bin_edges = np.histogram(df[index].generated_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'Other offset ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Guestbooks per month\")\n",
    "\n",
    "x_dates = [start_time + relativedelta(years=i) for i in range((len(bins) // 12) + 1)]\n",
    "ax.set_xticks([d.timestamp() * 1000 for d in x_dates])\n",
    "ax.set_xticklabels([f\"Jan\\n'\" + d.strftime('%Y')[2:] for i, d in enumerate(x_dates)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above, but plotting the created_at dates\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "start_time = datetime.fromtimestamp(df.created_at.min() / 1000).replace(tzinfo=pytz.UTC)\n",
    "curr_time = start_time\n",
    "end_time = datetime.fromtimestamp(df.created_at.max() / 1000).replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(months=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "index = (df.timestamp_diff >= -1000)&(df.timestamp_diff <= 1000)\n",
    "total_counts, bin_edges = np.histogram(df[index].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'CreatedAt/UUID match ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "index = df.timestamp_diff.isin([-18000000, -21600000, -17999000, -21599000])\n",
    "total_counts, bin_edges = np.histogram(df[index].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'UTC/CST offset ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "index = ((df.timestamp_diff < -1000)|(df.timestamp_diff > 1000))&(~df.timestamp_diff.isin([-18000000, -21600000, -17999000, -21599000]))\n",
    "total_counts, bin_edges = np.histogram(df[index].created_at, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label=f'Other offset ({np.sum(index) / len(index):.2%})')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Guestbooks per month\")\n",
    "\n",
    "x_dates = [start_time + relativedelta(years=i) for i in range((len(bins) // 12) + 1)]\n",
    "ax.set_xticks([d.timestamp() * 1000 for d in x_dates])\n",
    "ax.set_xticklabels([f\"Jan\\n'\" + d.strftime('%Y')[2:] for i, d in enumerate(x_dates)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO investigate journals more closely as well, to verify expected patterns between journals and guestbooks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating new amps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are amps getting double-counted?\n",
    "# amps are not getting double-counted, since amp_hearts are only happening on photos\n",
    "ints_df[ints_df.interaction_type == 'amp_heart'].parent_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions_launch_date = datetime.strptime('2021-02-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "sdf = ints_df[(ints_df.parent_type == 'journal')&(ints_df.interaction_type.str.startswith('amp'))&(ints_df.created_at >= int(reactions_launch_date.timestamp() * 1000))]\n",
    "print(len(sdf))\n",
    "amps_by_journal = sdf.groupby(by='parent_oid')\n",
    "amps_by_journal_df = amps_by_journal.agg({'site_id': len, 'user_id': 'nunique', 'interaction_type': 'nunique'}).rename(columns={'site_id': 'amp_count', 'user_id': 'user_count', 'interaction_type': 'amp_type_count'})\n",
    "len(amps_by_journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amps_by_journal_df = amps_by_journal_df.rename(columns={'site_id': 'amp_count', 'user_id': 'user_count', 'interaction_type': 'amp_type_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the site data\n",
    "s = datetime.now()\n",
    "site_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/site_metadata\"\n",
    "site_metadata_filepath = os.path.join(site_metadata_dir, \"site_metadata.feather\")\n",
    "site_df = pd.read_feather(site_metadata_filepath)\n",
    "print(f\"Read {len(site_df)} site_df rows in {datetime.now() - s}.\")\n",
    "site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab((amps_by_journal_df.amp_count > 1).rename('>1 amp?'), amps_by_journal_df.amp_type_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of journal updates with difference between total amps and unique amping users\n",
    "# this occurs due to a caringbridge bug where a single user is able to leave an original heart and a new reaction type\n",
    "np.sum(amps_by_journal_df.amp_count != amps_by_journal_df.user_count), np.sum(amps_by_journal_df.amp_count > amps_by_journal_df.user_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for journal_oid in amps_by_journal_df[amps_by_journal_df.amp_count != amps_by_journal_df.user_count].sample(n=5, random_state=0).index:\n",
    "    row = journal_df[journal_df.journal_oid == journal_oid].iloc[0]\n",
    "    journal_publish_date = datetime.utcfromtimestamp(row.published_at / 1000)\n",
    "    site_id = row.site_id\n",
    "    row = site_df[site_df.site_id == site_id].iloc[0]\n",
    "    print(row.site_id, row['name'], row.title, datetime.utcfromtimestamp(row.created_at / 1000), row.privacy, journal_publish_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions_launch_date = datetime.strptime('2020-11-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "sdf = ints_df[(ints_df.interaction_type.str.startswith('amp'))&(ints_df.created_at >= int(reactions_launch_date.timestamp() * 1000))]\n",
    "sdf['created_at_day'] = sdf.created_at.map(lambda ts: datetime.utcfromtimestamp(ts / 1000).replace(tzinfo=pytz.UTC).strftime('%Y-%m-%d'))\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_reactions_launch_date = datetime.utcfromtimestamp(np.min(sdf[sdf.interaction_type != 'amp'].created_at) / 1000).replace(tzinfo=pytz.UTC)\n",
    "str(real_reactions_launch_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch started on January 27, 2021\n",
    "sdf[sdf.interaction_type != 'amp'].created_at_day.value_counts().sort_index().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.interaction_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_type = sdf[sdf.created_at >= int(real_reactions_launch_date.timestamp() * 1000)].interaction_type\n",
    "pd.concat([int_type.value_counts(normalize=False).rename(f'counts since {real_reactions_launch_date.strftime(\"%Y-%m-%d %H:%m\")}'), int_type.value_counts(normalize=True).rename(f'% of total')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot query_df queries over time\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "may12 = datetime.strptime('2021-05-12', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "median_daily_pre = np.median(sdf[sdf.created_at_day <= '2021-05-12'].groupby('created_at_day').nunique().interaction_oid)\n",
    "median_daily_post = np.median(sdf[sdf.created_at_day > '2021-05-12'].groupby('created_at_day').nunique().interaction_oid)\n",
    "\n",
    "ax = axes[0]\n",
    "start_time = reactions_launch_date\n",
    "curr_time = start_time\n",
    "end_time = datetime.utcfromtimestamp(np.max(sdf.created_at) / 1000).replace(tzinfo=pytz.UTC)\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(days=1)\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "\n",
    "counts, bin_edges = np.histogram(sdf.created_at, bins=bins)\n",
    "ax.plot(bin_edges[:-1], counts, label=\"All reactions\")\n",
    "day_totals = counts\n",
    "\n",
    "counts, bin_edges = np.histogram(sdf[sdf.interaction_type != 'amp'].created_at, bins=bins)\n",
    "ax.plot(bin_edges[:-1], counts, label=\"New reactions\")\n",
    "\n",
    "#ax.axvline(\n",
    "#    may12.timestamp() * 1000,\n",
    "#    linestyle='--', color='gray', alpha=0.8, label='May 12, 2021'\n",
    "#)\n",
    "\n",
    "ax.axvline(\n",
    "    real_reactions_launch_date.timestamp() * 1000,\n",
    "    linestyle='--', color='gray', alpha=0.8, label='New Reactions Launch'\n",
    ")\n",
    "\n",
    "#ax.hlines(median_daily_pre, start_time.timestamp() * 1000, may12.timestamp() * 1000, linestyle='dotted', color='black', label=f'Pre-May-12th median ({median_daily_pre} per day)', zorder=100)\n",
    "#ax.hlines(median_daily_post, may12.timestamp() * 1000, end_time.timestamp() * 1000, linestyle='dashdot', color='black', label=f'Post-May-12th median ({median_daily_post} per day)', zorder=100)\n",
    "\n",
    "ax.set_ylabel(f\"Reactions per day\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title(f\"Reactions (n={len(sdf):,}) in Mongo snapshot from July 15, 2021\")\n",
    "\n",
    "#ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: datetime.utcfromtimestamp(x / 1000).replace(tzinfo=pytz.timezone('US/Central')).strftime(\"%Y\\n%m %d\").replace(\" 0\", \" \")))\n",
    "#start = datetime.strptime('2005-01-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "#x_dates = [start + relativedelta(years=i) for i in range(18)]\n",
    "#ax.set_xticks([d.timestamp() * 1000 for d in x_dates])\n",
    "#nl = '\\n'\n",
    "#ax.set_xticklabels([f\"{nl if i % 2 == 1 else ''}'\" + d.strftime('%Y')[2:] for i, d in enumerate(x_dates)])\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: datetime.utcfromtimestamp(x / 1000).replace(tzinfo=pytz.timezone('US/Central')).strftime(\"%h %d\").replace(\" 0\", \" \")))\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "for interaction_type, type_repr in zip(['amp', 'amp_folded_hands', 'amp_heart', 'amp_happy', 'amp_sad'], ['Original Amp', 'Folded Hands', 'Heart', 'Happy Face', 'Sad Face']):\n",
    "    counts, bin_edges = np.histogram(sdf[sdf.interaction_type == interaction_type].created_at, bins=bins)\n",
    "    pcts = counts / day_totals\n",
    "    if interaction_type == 'amp':\n",
    "        continue\n",
    "    ax.plot(bin_edges[:-1], pcts, label=f\"{type_repr}\")\n",
    "\n",
    "#ax.axvline(\n",
    "#    may12.timestamp() * 1000,\n",
    "#    linestyle='--', color='gray', alpha=0.8, label='May 12, 2021'\n",
    "#)\n",
    "\n",
    "ax.axvline(\n",
    "    real_reactions_launch_date.timestamp() * 1000,\n",
    "    linestyle='--', color='gray', alpha=0.8, label='New Reactions Launch'\n",
    ")\n",
    "\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: datetime.utcfromtimestamp(x / 1000).replace(tzinfo=pytz.timezone('US/Central')).strftime(\"%h %d\").replace(\" 0\", \" \")))\n",
    "ax.legend()\n",
    "ax.set_ylabel(f\"% of total daily reactions\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title(f\"Reactions by type\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shared-conda)",
   "language": "python",
   "name": "shared-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
