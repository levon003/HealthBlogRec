{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triple Generation Scratchpad\n",
    "===\n",
    "\n",
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import sqlite3\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbcore.data.paths as paths\n",
    "import cbcore.data.dates as dates\n",
    "import cbcore.data.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(git_root_dir, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport cbrec, cbrec.triple_generation, cbrec.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.triple_generation as tg\n",
    "import cbrec.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_df, ints_df, journal_df = tg.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim out journal updates that are trivial (short or machine-generated)\n",
    "journal_df = journal_df[journal_df.is_nontrivial]\n",
    "# trim out journal updates with invalid dates\n",
    "invalid_start_date = datetime.fromisoformat('2005-01-01').replace(tzinfo=pytz.UTC)\n",
    "invalid_end_date = datetime.fromisoformat('2019-02-11').replace(tzinfo=pytz.UTC)\n",
    "print(f\"Keeping journals between {invalid_start_date.isoformat()} and {invalid_end_date.isoformat()}.\")\n",
    "invalid_start_timestamp = invalid_start_date.timestamp() * 1000\n",
    "invalid_end_timestamp = invalid_end_date.timestamp() * 1000\n",
    "journal_df = journal_df[(journal_df.created_at>=invalid_start_timestamp)&(journal_df.created_at<=invalid_end_timestamp)]\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df = ints_df[(ints_df.created_at>=invalid_start_timestamp)&(ints_df.created_at<=invalid_end_timestamp)]\n",
    "len(ints_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes 3.5 minutes.\n",
    "# feels like it might be worth saving this as a new pickle file?\n",
    "# but on reflection, we actually don't want to do this!\n",
    "s = datetime.now()\n",
    "valid_tuples = user_site_df[['user_id', 'site_id']].apply(tuple, 1)\n",
    "valid_user_ids = set(user_site_df.user_id)\n",
    "filtered_journal_df = journal_df[journal_df.user_id.isin(valid_user_ids)]\n",
    "filtered_journal_tuples = filtered_journal_df[['user_id', 'site_id']].apply(tuple, 1)\n",
    "filtered_journal_df = filtered_journal_df[filtered_journal_tuples.isin(valid_tuples)]\n",
    "len(filtered_journal_df), str(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only journal updates authored by users who will become eligible\n",
    "# note this includes updates in (user_id, site_id) pairs that aren't in valid_usps\n",
    "# but we still want to record this activity for feature extraction reasons\n",
    "s = datetime.now()\n",
    "valid_usps = user_site_df[['user_id', 'site_id']].apply(tuple, 1)\n",
    "valid_user_ids = set(user_site_df.user_id)\n",
    "filtered_journal_df = journal_df[journal_df.user_id.isin(valid_user_ids)]\n",
    "len(filtered_journal_df), str(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of created_at -> (user_id, site_id, journal_oid)\n",
    "journal_dict = OrderedDict()\n",
    "for row in tqdm(filtered_journal_df.itertuples(), total=len(filtered_journal_df), desc='Populating journal dict'):\n",
    "    journal_dict[row.created_at] = (row.user_id, row.site_id, row.journal_oid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df.interaction_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of created_at -> (user_id, site_id)\n",
    "ts_to_first_update_dict = OrderedDict()\n",
    "ts_to_third_update_dict = OrderedDict()\n",
    "for row in tqdm(user_site_df.sort_values(by='user_first_update_timestamp', ascending=True).itertuples(), total=len(user_site_df), desc='Populating user/site eligibility dict'):\n",
    "    ts_to_first_update_dict[row.user_first_update_timestamp] = (row.user_id, row.site_id)\n",
    "for row in tqdm(user_site_df.sort_values(by='user_third_update_timestamp', ascending=True).itertuples(), total=len(user_site_df), desc='Populating user/site eligibility dict'):\n",
    "    ts_to_third_update_dict[row.user_third_update_timestamp] = (row.user_id, row.site_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecentActivityManager():\n",
    "    def __init__(self):\n",
    "        activity_count_duration_ms = config.get('ACTIVITY_COUNT_DURATION_MS')\n",
    "        assert activity_count_duration_ms is not None\n",
    "        self.activity_counter_dict = {\n",
    "            'journal_user': RecentActivityCounter(activity_count_duration_ms),\n",
    "            'journal_site': RecentActivityCounter(activity_count_duration_ms),\n",
    "            'amp': RecentActivityCounter(activity_count_duration_ms),\n",
    "            'comment': RecentActivityCounter(activity_count_duration_ms),\n",
    "            'guestbook': RecentActivityCounter(activity_count_duration_ms),\n",
    "        }\n",
    "        self.user_activity_keys = set(['journal_user', 'amp', 'comment', 'guestbook'])\n",
    "        self.active_user_ids = set()\n",
    "    \n",
    "    def update_counts(self, current_timestamp):\n",
    "        # update activity counters to the current moment\n",
    "        all_removed_user_ids = set()\n",
    "        for key, rac in self.activity_counter_dict.items():\n",
    "            removed_user_ids = rac.update_counts(current_timestamp)\n",
    "            if key in self.user_activity_keys:\n",
    "                all_removed_user_ids |= removed_user_ids\n",
    "        if len(all_removed_user_ids) > 0:\n",
    "            # recompute active user ids\n",
    "            self.active_user_ids = set()\n",
    "            self.active_user_ids.update(*[\n",
    "                self.activity_counter_dict[key].get_active_ids() \n",
    "                for key in self.user_activity_keys\n",
    "            ])\n",
    "\n",
    "            \n",
    "    def get_active_user_ids(self):\n",
    "        return self.active_user_ids\n",
    "    \n",
    "    \n",
    "    def add_interaction(self, interaction_type, user_id, created_at):\n",
    "        self.activity_counter_dict[interaction_type].add_interaction(user_id, created_at)\n",
    "        self.active_user_ids.add(user_id)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        summary = f\"{len(self.activity_counter_dict)} activity counters ({len(self.user_activity_keys)} for users). Tracking {len(self.active_user_ids)} active users.\\n\"\n",
    "        for int_type, rac in self.activity_counter_dict.items():\n",
    "            summary += f\"{int_type} recent activity: {len(rac.activity_count_dict)} unique users with {np.sum(list(rac.activity_count_dict.values()))} total interactions.\\n\"\n",
    "        return summary\n",
    "    \n",
    "    def __str__(self):\n",
    "        return __repr__(self)\n",
    "        \n",
    "            \n",
    "class RecentActivityCounter():\n",
    "    \"\"\"\n",
    "    Written for user_ids, but supports any form of hashable id, e.g. site_ids or (user_id, site_id) tuples\n",
    "    \"\"\"\n",
    "    def __init__(self, activity_count_duration_ms):\n",
    "        # map of created_at -> list(user_id)\n",
    "        # tracks one type of activity\n",
    "        # user_id is a list and not a set because a user may have multiple e.g. amps at the same timestamp\n",
    "        self.ts_to_user_ids = OrderedDict()\n",
    "        self.activity_count_dict = {}  # map of user_id -> int\n",
    "        self.activity_count_duration_ms = activity_count_duration_ms\n",
    "        self.active_ids = set()\n",
    "    \n",
    "    def update_counts(self, current_timestamp):\n",
    "        \"\"\"\n",
    "        Set the timestamp from which to give counts to current_timestamp,\n",
    "        which has the effect of removing any old activity and updating the counts accordingly.\n",
    "        Note: current_timestamp must be >= any previous calls to update_counts().\n",
    "        \n",
    "        Returns user_ids no longer considered active.\n",
    "        \"\"\"\n",
    "        expired_timestamp = current_timestamp - self.activity_count_duration_ms\n",
    "        removed_ids = set()\n",
    "        while len(self.ts_to_user_ids) > 0:\n",
    "            if next(iter(self.ts_to_user_ids)) < expired_timestamp:\n",
    "                # this activity has expired\n",
    "                _, user_id_list = self.ts_to_user_ids.popitem(last=False)\n",
    "                # update the counts to account for the removal\n",
    "                for user_id in user_id_list:\n",
    "                    self.activity_count_dict[user_id] -= 1\n",
    "                    # delete old keys when count hits 0\n",
    "                    if self.activity_count_dict[user_id] == 0:\n",
    "                        del self.activity_count_dict[user_id]\n",
    "                        self.active_ids.remove(user_id)\n",
    "                        removed_ids.add(user_id)\n",
    "            else:\n",
    "                break\n",
    "        return removed_ids\n",
    "    \n",
    "    def add_interaction(self, user_id, created_at):\n",
    "        \"\"\"\n",
    "        Add an interaction from user_id at time created_at to the activity tracker.\n",
    "        \n",
    "        \"\"\"\n",
    "        if created_at in self.ts_to_user_ids:\n",
    "            user_id_list = self.ts_to_user_ids[created_at]\n",
    "        else:\n",
    "            user_id_list = []\n",
    "            self.ts_to_user_ids[created_at] = user_id_list\n",
    "        user_id_list.append(user_id)\n",
    "        if user_id in self.activity_count_dict:\n",
    "            self.activity_count_dict[user_id] += 1\n",
    "        else:\n",
    "            self.activity_count_dict[user_id] = 1\n",
    "            self.active_ids.add(user_id)\n",
    "    \n",
    "    def get_count(self, user_id):\n",
    "        \"\"\"\n",
    "        Activity count for a user_id in the last self.activity_count_duration_ms milliseconds\n",
    "        \"\"\"\n",
    "        if user_id not in self.activity_count_dict:\n",
    "            return 0\n",
    "        return self.activity_count_dict[user_id]\n",
    "    \n",
    "    def get_active_ids(self):\n",
    "        return self.active_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rac = RecentActivityCounter(100)\n",
    "rac.add_interaction(1, 0)\n",
    "assert rac.get_count(1) == 1\n",
    "rac.add_interaction(1, 50)\n",
    "assert rac.get_count(1) == 2\n",
    "rac.update_counts(51)\n",
    "assert rac.get_count(1) == 2\n",
    "rac.update_counts(101)\n",
    "assert rac.get_count(1) == 1\n",
    "rac.update_counts(151)\n",
    "assert rac.get_count(1) == 0\n",
    "rac.ts_to_user_ids, rac.activity_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_df[user_site_df.user_id == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_journal_df[filtered_journal_df.user_id == 1].created_at.sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(\n",
    "    ints_df.interaction_type, \n",
    "    ints_df.user_id, \n",
    "    ints_df.site_id, \n",
    "    ints_df.head(n=50).created_at.map(lambda ca: str(datetime.utcfromtimestamp(ca / 1000)))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usp = user/site pair\n",
    "# existing vs eligible vs active\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "\n",
    "\n",
    "eligible_usps = set()  # this set isn't really used...\n",
    "existing_user_site_map = defaultdict(set)  # map of user_id -> set(site_id), where user_id is an existing author on the set of sites\n",
    "eligible_user_site_map = defaultdict(set)  # map of user_id -> set(site_id), where user_id is an eligible author on the set of sites\n",
    "existing_site_user_map = defaultdict(set)  # map of site_id -> set(user_id), where site_id has the set of existing authors\n",
    "eligible_site_user_map = defaultdict(set)  # map of site_id -> set(user_id), where site_id has the set of eligible authors\n",
    "\n",
    "site_user_int_dict = defaultdict(set)  # map of site_id -> set(user_id), where each user_id has interacted with this site_id\n",
    "\n",
    "# map of (user_id, site_id) -> journal_oid of most recent update\n",
    "# this will be the user's third update once they become eligible\n",
    "# note we COULD save the THREE most recent journal_oids easily, but it seems like they may not be needed\n",
    "usp_most_recent_update = {}\n",
    "\n",
    "activity_manager = RecentActivityManager()\n",
    "\n",
    "# burnin_start_timestamp is when we should start tracking activity data\n",
    "# note: it may be that there's no computational benefit to using this\n",
    "# as the expensive stuff is the graph creation anyway. Seems like it will though\n",
    "burnin_start_timestamp = datetime.fromisoformat('2013-12-01').replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "generation_start_timestamp = datetime.fromisoformat('2014-01-01').replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "generation_stop_timestamp = datetime.fromisoformat('2019-01-01').replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "\n",
    "for row in tqdm(ints_df.itertuples(), total=len(ints_df), desc='Computing interactions'):\n",
    "    int_created_at = row.created_at\n",
    "    int_user_id = row.user_id\n",
    "    int_site_id = row.site_id\n",
    "    interaction_type = row.interaction_type\n",
    "    \n",
    "    if int_created_at > generation_stop_timestamp:\n",
    "        # no reason to continue past the generation period\n",
    "        break\n",
    "    \n",
    "    # is_initiation if this is the first time this user_id has interacted with this site_id\n",
    "    is_initiation = int_user_id not in site_user_int_dict[int_site_id]\n",
    "    \n",
    "    # update existing users\n",
    "    while len(ts_to_first_update_dict) > 0:\n",
    "        if next(iter(ts_to_first_update_dict)) < int_created_at:\n",
    "            # this is a new existing user_id\n",
    "            _, user_site_tup = ts_to_first_update_dict.popitem(last=False)\n",
    "            user_id, site_id = user_site_tup\n",
    "            existing_user_site_map[user_id].add(site_id)\n",
    "            existing_site_user_map[site_id].add(user_id)\n",
    "            # need to update the graph, as a new user is eligible on this site\n",
    "            # so, get all users who have previously interacted with this site\n",
    "            prev_user_ids = site_user_int_dict[site_id]\n",
    "            for prev_user_id in prev_user_ids:\n",
    "                # add edge prev_user_id -> user_id\n",
    "                pass\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # update eligible users\n",
    "    while len(ts_to_third_update_dict) > 0:\n",
    "        if next(iter(ts_to_third_update_dict)) < int_created_at:\n",
    "            # this is a new ELIGIBLE (previously existing) user_id\n",
    "            _, user_site_tup = ts_to_third_update_dict.popitem(last=False)\n",
    "            user_id, site_id = user_site_tup\n",
    "            eligible_usps.add(user_site_tup)\n",
    "            eligible_user_site_map[user_id].add(site_id)\n",
    "            eligible_site_user_map[site_id].add(user_id)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # update journal activity\n",
    "    while len(journal_dict) > 0:\n",
    "        if next(iter(journal_dict)) < int_created_at:\n",
    "            # this is a journal update that happened before this interaction\n",
    "            journal_created_at, user_site_oid_tup = journal_dict.popitem(last=False)\n",
    "            journal_user_id, journal_site_id, journal_oid = user_site_oid_tup\n",
    "            activity_manager.add_interaction('journal_user', journal_user_id, journal_created_at)\n",
    "            activity_manager.add_interaction('journal_site', journal_site_id, journal_created_at)\n",
    "            usp_most_recent_update[(journal_user_id, journal_site_id)] = journal_oid\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # update activity counters to the current moment\n",
    "    activity_manager.update_counts(int_created_at)\n",
    "        \n",
    "    # for initiations after some time period, we generate features\n",
    "    if int_created_at >= generation_start_timestamp and is_initiation:\n",
    "        # identify usp sources and targets\n",
    "        # sources is every usp that has this user_id\n",
    "        source_site_ids = eligible_user_site_map[int_user_id]\n",
    "        sources = [(int_user_id, site_id) for site_id in source_site_ids]\n",
    "        # targets is every usp that has this site_id\n",
    "        target_user_ids = eligible_site_user_map[int_site_id]\n",
    "        targets = [(user_id, int_site_id) for user_id in target_user_ids]\n",
    "        \n",
    "        # to select alternatives, we need to identify ACTIVE eligible users\n",
    "        # active means \"amp, comment, guestbook, or journal\" in last X milliseconds\n",
    "        active_user_ids = activity_manager.get_active_user_ids()\n",
    "        # and here we filter to eligible active users only\n",
    "        active_user_ids &= set(eligible_user_site_map.keys())\n",
    "        \n",
    "        # finally, remove from active any user_ids invalid for this source user_id\n",
    "        # invalid users include:\n",
    "        #  - any target user_id\n",
    "        #  - any user_id that int_user_id has previously connected with\n",
    "        # TODO get previous connections from graph\n",
    "        invalid_user_ids = target_user_ids # | graph.get_edge_targets_for_user_id(int_user_id)\n",
    "        active_user_ids -= invalid_user_ids\n",
    "        \n",
    "        # generate the usps from these active users\n",
    "        # we use an array to make sampling faster\n",
    "        active_usps = np.array([\n",
    "            (active_user_id, site_id)\n",
    "            for active_user_id in active_user_ids\n",
    "            for site_id in eligible_user_site_map[active_user_id]\n",
    "        ])\n",
    "        if len(active_usps) == 0:\n",
    "            raise ValueError(\"No active usps! That means no alternatives, which should never happen.\")\n",
    "            \n",
    "        # create all combinations of source and target usps\n",
    "        for source in sources:\n",
    "            for target in targets:\n",
    "                # select an alternative (alt) usp\n",
    "                alt = rng.choice(active_usps)\n",
    "                \n",
    "                # generate and save features for this usp triple\n",
    "                # feature_writer.save_triple(source, target, alt)\n",
    "\n",
    "    \n",
    "    \n",
    "    # update network\n",
    "    if is_initiation:\n",
    "        site_user_int_dict[int_site_id].add(int_user_id)\n",
    "        # who has int_user_id interacted with in the graph?\n",
    "        # any user who has previously authored an update on this site\n",
    "        if int_site_id in existing_site_user_map:\n",
    "            curr_existing_users_on_site = existing_site_user_map[int_site_id]\n",
    "            for curr_user_id in curr_existing_users_on_site:\n",
    "                # add edge int_user_id -> curr_user_id\n",
    "                pass\n",
    "            \n",
    "    # update activity counters\n",
    "    activity_manager.add_interaction(interaction_type, int_user_id, int_created_at)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(datetime.utcfromtimestamp(next(iter(journal_dict)) / 1000)), \\\n",
    "str(datetime.utcfromtimestamp(int_created_at / 1000)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(datetime.utcfromtimestamp(next(iter(ts_to_first_update_dict)) / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(journal_dict)\n",
    "for i in range(10):\n",
    "    print(str(datetime.utcfromtimestamp(next(it) / 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eligible_usps), len(existing_user_site_map), len(existing_site_user_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(active_user_ids), len(active_usps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for int_type, rac in activity_counter_dict.items():\n",
    "    print(f\"{int_type} recent activity: {len(rac.activity_count_dict)} unique users with {np.sum(list(rac.activity_count_dict.values()))} total interactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = f\"{len(activity_manager.activity_counter_dict)} activity counters ({len(activity_manager.user_activity_keys)} for users). Tracking {len(activity_manager.active_user_ids)} active users.\\n\"\n",
    "for int_type, rac in activity_manager.activity_counter_dict.items():\n",
    "    summary += f\"{int_type} recent activity: {len(rac.activity_count_dict)} unique users with {np.sum(list(rac.activity_count_dict.values()))} total interactions.\\n\"\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_cond_filepath = os.path.join(\"/home/lana/shared/caringbridge/data/projects/sna-social-support/user_metadata\", \"assigned_health_conditions.feather\")\n",
    "user_health_conds_df = pd.read_feather(health_cond_filepath)\n",
    "print(len(user_health_conds_df))\n",
    "user_health_conds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal dataframe\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.df\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(f\"Read {len(journal_df)} rows in {datetime.now() - s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a quick fix for invalid dates in journals, when created_at is 0 we use the updated_at instead\n",
    "# note that only 41 updates have this issue\n",
    "invalid_created_at = journal_df.created_at <= 0\n",
    "journal_df.loc[invalid_created_at, 'created_at'] = journal_df.loc[invalid_created_at, 'updated_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the journal metadata with author type info added\n",
    "s = datetime.now()\n",
    "author_type_dir = \"/home/lana/shared/caringbridge/data/projects/sna-social-support/author_type\"\n",
    "journal_metadata_filepath = os.path.join(author_type_dir, \"journal_metadata_with_author_type.df\")\n",
    "at_journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(at_journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a quick fix for invalid dates in journals, when created_at is 0 we use the updated_at instead\n",
    "# note that only 41 updates have this issue\n",
    "#invalid_created_at = at_journal_df.created_at <= 0\n",
    "#at_journal_df.loc[invalid_created_at, 'created_at'] = at_journal_df.loc[invalid_created_at, 'updated_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of valid user/site pairs\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "user_site_df = pd.read_csv(os.path.join(model_data_dir, 'user_site_df.csv'))\n",
    "valid_user_ids = set(user_site_df.user_id)\n",
    "valid_site_ids = set(user_site_df.site_id)\n",
    "print(f\"Read {len(user_site_df)} rows ({len(valid_user_ids)} unique users, {len(valid_site_ids)} unique sites) in {datetime.now() - s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(user_site_df.user_third_update_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_creation_times = defaultdict(set)\n",
    "for key, group in tqdm(journal_df.groupby(['user_id', 'site_id'], sort=False)):\n",
    "    journal_creation_times[key] = set(group.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_journal_creation_times = defaultdict(set)\n",
    "for key, group in tqdm(at_journal_df.groupby(['user_id', 'site_id'], sort=False)):\n",
    "    at_journal_creation_times[key] = set(group.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_missing_pred_count = 0\n",
    "at_journal_missing_pred_count = 0\n",
    "for _, row in tqdm(user_site_df.iterrows(), total=len(user_site_df)):\n",
    "    user_id = row['user_id']\n",
    "    site_id = row['site_id']\n",
    "    creation_times = journal_creation_times[(user_id, site_id)]\n",
    "    if row.user_first_update_timestamp not in creation_times:\n",
    "        journal_missing_pred_count += 1\n",
    "    if row.user_third_update_timestamp not in creation_times:\n",
    "        journal_missing_pred_count += 1\n",
    "    \n",
    "    creation_times = at_journal_creation_times[(user_id, site_id)]\n",
    "    if row.user_first_update_timestamp not in creation_times:\n",
    "        at_journal_missing_pred_count += 1\n",
    "    if row.user_third_update_timestamp not in creation_times:\n",
    "        at_journal_missing_pred_count += 1\n",
    "journal_missing_pred_count, at_journal_missing_pred_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = paths.raw_data_2019_filepath\n",
    "raw_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data\"\n",
    "assert os.path.exists(working_dir)\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of valid user/site pairs\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "user_site_df = pd.read_csv(os.path.join(model_data_dir, 'user_site_df.csv'))\n",
    "valid_user_ids = set(user_site_df.user_id)\n",
    "valid_site_ids = set(user_site_df.site_id)\n",
    "print(f\"Read {len(user_site_df)} rows ({len(valid_user_ids)} unique users, {len(valid_site_ids)} unique sites) in {datetime.now() - s}.\")\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guestbook_filepath = os.path.join(raw_data_dir, 'guestbook_scrubbed.json')\n",
    "output_filepath = os.path.join(working_dir, \"guestbook_filtered.csv\")\n",
    "both_valid_count = 0\n",
    "neither_valid_count = 0\n",
    "author_valid_count = 0\n",
    "site_valid_count = 0\n",
    "with open(output_filepath, 'w') as outfile:\n",
    "    with open(guestbook_filepath, encoding='utf-8') as infile:\n",
    "        processed_count = 0\n",
    "        for i, line in tqdm(enumerate(infile), total=82858710):\n",
    "            if i < 4002:\n",
    "                continue\n",
    "            try:\n",
    "                gb = json.loads(line)\n",
    "            except:\n",
    "                continue\n",
    "            gb_oid = gb['_id']['$oid']\n",
    "            site_id = utils.extract_long(gb['siteId'])\n",
    "            user_id = utils.extract_long(gb['userId'])\n",
    "            created_at = dates.get_date_from_json_value(gb['createdAt']) if 'createdAt' in gb else 0\n",
    "            updated_at = dates.get_date_from_json_value(gb['updatedAt']) if 'updatedAt' in gb else 0\n",
    "            \n",
    "            if 'amps' in gb and type(gb['amps']) == list:\n",
    "                # we write out any amps as separate lines\n",
    "                for amp in gb['amps']:\n",
    "                    amp_user_id = utils.extract_long(amp)\n",
    "                    is_user_valid = amp_user_id in valid_user_ids\n",
    "                    is_site_valid = site_id in valid_site_ids\n",
    "                    if is_user_valid and is_site_valid:\n",
    "                        outfile.write(f\"{amp_user_id},{site_id},amp,{gb_oid}|{amp_user_id},guestbook,{gb_oid},guestbook,{gb_oid},{created_at},{updated_at}\\n\")\n",
    "                        both_valid_count += 1\n",
    "                    elif is_user_valid and not is_site_valid:\n",
    "                        author_valid_count += 1\n",
    "                    elif not is_user_valid and is_site_valid:\n",
    "                        site_valid_count += 1\n",
    "                    else:\n",
    "                        neither_valid_count += 1\n",
    "            is_user_valid = user_id in valid_user_ids\n",
    "            is_site_valid = site_id in valid_site_ids\n",
    "            if is_user_valid and is_site_valid:\n",
    "                # columns: user_id, site_id, interaction_type, interaction_oid, parent_type, parent_id, ancestor_type, ancestor_id, created_at, updated_at\n",
    "                outfile.write(f\"{user_id},{site_id},guestbook,{gb_oid},None,None,None,None,{created_at},{updated_at}\\n\")\n",
    "                both_valid_count += 1\n",
    "            elif is_user_valid and not is_site_valid:\n",
    "                author_valid_count += 1\n",
    "            elif not is_user_valid and is_site_valid:\n",
    "                site_valid_count += 1\n",
    "            else:\n",
    "                neither_valid_count += 1\n",
    "            processed_count += 1\n",
    "processed_count, both_valid_count, neither_valid_count, author_valid_count, site_valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_filepath = os.path.join(raw_data_dir, 'comment_scrubbed.json')\n",
    "output_filepath = os.path.join(working_dir, \"comment_filtered.csv\")\n",
    "both_valid_count = 0\n",
    "neither_valid_count = 0\n",
    "author_valid_count = 0\n",
    "site_valid_count = 0\n",
    "with open(output_filepath, 'w') as outfile:\n",
    "    with open(comments_filepath, encoding='utf-8') as infile:\n",
    "        for line in tqdm(infile, total=31052715):\n",
    "            comment = json.loads(line)\n",
    "            comment_oid = comment['_id']['$oid']\n",
    "            parent_type = comment['parentType']  # either 'journal' or 'comment'\n",
    "            parent_oid = comment['parentId']\n",
    "            journal_oid = comment['ancestorId']  # ancestorType is never guestbook; we seemingly don't have any of the guestbook comment data\n",
    "            site_id = utils.extract_long(comment['siteId'])\n",
    "            is_site_valid = site_id in valid_site_ids\n",
    "            user_id = utils.extract_long(comment['userId'])\n",
    "            created_at = dates.get_date_from_json_value(comment['createdAt'])\n",
    "            updated_at = dates.get_date_from_json_value(comment['updatedAt'])\n",
    "            \n",
    "            if 'amps' in comment and type(comment['amps']) == list:\n",
    "                # we write out any amps as separate lines\n",
    "                for amp in comment['amps']:\n",
    "                    amp_user_id = utils.extract_long(amp)\n",
    "                    is_user_valid = amp_user_id in valid_user_ids\n",
    "                    if is_user_valid and is_site_valid:\n",
    "                        outfile.write(f\"{amp_user_id},{site_id},amp,{comment_oid}|{amp_user_id},comment,{comment_oid},journal,{journal_oid},{created_at},{updated_at}\\n\")\n",
    "                        both_valid_count += 1\n",
    "                    elif is_user_valid and not is_site_valid:\n",
    "                        author_valid_count += 1\n",
    "                    elif not is_user_valid and is_site_valid:\n",
    "                        site_valid_count += 1\n",
    "                    else:\n",
    "                        neither_valid_count += 1\n",
    "            \n",
    "            is_user_valid = user_id in valid_user_ids\n",
    "            if is_user_valid and is_site_valid:\n",
    "                # columns: user_id, site_id, interaction_type, interaction_oid, parent_type, parent_id, ancestor_type, ancestor_id, created_at, updated_at\n",
    "                outfile.write(f\"{user_id},{site_id},comment,{comment_oid},{parent_type},{parent_oid},journal,{journal_oid},{created_at},{updated_at}\\n\")\n",
    "                both_valid_count += 1\n",
    "            elif is_user_valid and not is_site_valid:\n",
    "                author_valid_count += 1\n",
    "            elif not is_user_valid and is_site_valid:\n",
    "                site_valid_count += 1\n",
    "            else:\n",
    "                neither_valid_count += 1\n",
    "both_valid_count, neither_valid_count, author_valid_count, site_valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_filepath = os.path.join(raw_data_dir, 'journal.json')\n",
    "output_filepath = os.path.join(working_dir, \"journal_filtered.csv\")\n",
    "both_valid_count = 0\n",
    "neither_valid_count = 0\n",
    "author_valid_count = 0\n",
    "site_valid_count = 0\n",
    "with open(output_filepath, 'w') as outfile:\n",
    "    with open(journal_filepath, encoding='utf-8') as infile:\n",
    "        for line in tqdm(infile, total=19137078):\n",
    "            journal = json.loads(line)\n",
    "            \n",
    "            if 'amps' not in journal:\n",
    "                continue\n",
    "            amps = journal['amps']\n",
    "            if type(amps) != list:\n",
    "                continue\n",
    "                \n",
    "            journal_oid = journal['_id']['$oid']\n",
    "            site_id = utils.extract_long(journal['siteId'])\n",
    "            is_site_valid = site_id in valid_site_ids\n",
    "            user_id = utils.extract_long(journal['userId'])\n",
    "            \n",
    "            created_at = dates.get_date_from_json_value(journal['createdAt'])\n",
    "            updated_at = dates.get_date_from_json_value(journal['updatedAt'])\n",
    "            \n",
    "            for amp in amps:\n",
    "                amp_user_id = utils.extract_long(amp)\n",
    "                is_user_valid = amp_user_id in valid_user_ids\n",
    "                if is_user_valid and is_site_valid:\n",
    "                    outfile.write(f\"{amp_user_id},{site_id},amp,{journal_oid}|{amp_user_id},journal,{journal_oid},journal,{journal_oid},{created_at},{updated_at}\\n\")\n",
    "                    both_valid_count += 1\n",
    "                elif is_user_valid and not is_site_valid:\n",
    "                    author_valid_count += 1\n",
    "                elif not is_user_valid and is_site_valid:\n",
    "                    site_valid_count += 1\n",
    "                else:\n",
    "                    neither_valid_count += 1\n",
    "            \n",
    "            #is_user_valid = user_id in valid_user_ids\n",
    "            #if is_user_valid and is_site_valid:\n",
    "            #    # columns: user_id, site_id, interaction_type, interaction_oid, parent_type, parent_id, ancestor_type, ancestor_id, created_at, updated_at\n",
    "            #    outfile.write(f\"{user_id},{site_id},journal,{journal_oid},None,None,None,None,{created_at},{updated_at}\\n\")\n",
    "            #    both_valid_count += 1\n",
    "            #elif is_user_valid and not is_site_valid:\n",
    "            #    author_valid_count += 1\n",
    "            #elif not is_user_valid and is_site_valid:\n",
    "            #    site_valid_count += 1\n",
    "            #else:\n",
    "            #    neither_valid_count += 1\n",
    "both_valid_count, neither_valid_count, author_valid_count, site_valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user_id', 'site_id', 'interaction_type', 'interaction_oid', 'parent_type', 'parent_oid', 'ancestor_type', 'ancestor_oid', 'created_at', 'updated_at']\n",
    "s = datetime.now()\n",
    "gb_filepath = os.path.join(working_dir, \"guestbook_filtered.csv\")\n",
    "gb_df = pd.read_csv(gb_filepath, header=None, names=cols)\n",
    "print(datetime.now() - s)\n",
    "\n",
    "s = datetime.now()\n",
    "comment_filepath = os.path.join(working_dir, \"comment_filtered.csv\")\n",
    "comment_df = pd.read_csv(comment_filepath, header=None, names=cols)\n",
    "print(datetime.now() - s)\n",
    "\n",
    "s = datetime.now()\n",
    "journal_filepath = os.path.join(working_dir, \"journal_filtered.csv\")\n",
    "journal_df = pd.read_csv(journal_filepath, header=None, names=cols)\n",
    "print(datetime.now() - s)\n",
    "\n",
    "len(gb_df), len(comment_df), len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df = pd.concat([gb_df, comment_df, journal_df], sort=False)\n",
    "ints_df.reset_index(drop=True, inplace=True)\n",
    "print(len(ints_df))\n",
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "ints_df = ints_df.sort_values(by='created_at')\n",
    "print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ints_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "ints_df.reset_index(drop=True).to_feather(os.path.join(working_dir, 'ints_df.feather'))\n",
    "print(datetime.now() - s)\n",
    "s = datetime.now()\n",
    "ints_df.to_csv(os.path.join(working_dir, 'ints_df.csv'), index=False)\n",
    "print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read interactions dataframe\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "ints_df = pd.read_feather(os.path.join(model_data_dir, 'ints_df.feather'))\n",
    "print(f\"Read {len(ints_df)} rows ({len(set(ints_df.user_id))} unique users) in {datetime.now() - s}.\")\n",
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df[['interaction_type', 'parent_type', 'ancestor_type']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2005\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").replace(tzinfo=pytz.UTC).timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'amp'].created_at / 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Amps')\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'guestbook'].created_at / 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks')\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'comment'].created_at / 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Comments')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.axvline(datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "plt.axvline(datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Interactions per month\")\n",
    "plt.title(f\"{len(ints_df):,} interactions from {len(set(ints_df.user_id)):,} unique users on {len(set(ints_df.site_id)):,} unique sites\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").replace(tzinfo=pytz.UTC).timestamp() for i in range((2020 - 2005) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timestamp = datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "end_timestamp = datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "sdf = ints_df[(ints_df.created_at >= start_timestamp)&(ints_df.created_at <= end_timestamp)]\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf[['interaction_type', 'parent_type', 'ancestor_type']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal dataframe with the index\n",
    "# this is all the new journal data\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.df\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(journal_df.created_at.quantile(0.0001) / 1000).isoformat(),\\\n",
    "datetime.utcfromtimestamp(journal_df.created_at.quantile(0.999999) / 1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# journal updates over time, by month\n",
    "\n",
    "start_date = \"2002-04-01\"\n",
    "end_date = \"2019-03-01\"\n",
    "sdate = datetime.fromisoformat(start_date)\n",
    "edate = datetime.fromisoformat(end_date)\n",
    "delta = edate - sdate\n",
    "delta = relativedelta(edate, sdate)\n",
    "bins = []\n",
    "for i in range((delta.years*12) + delta.months + 1):\n",
    "    day = sdate + relativedelta(months=i) #timedelta(months=i)\n",
    "    bins.append(day.timestamp())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "total_counts, bin_edges = np.histogram(journal_df.created_at / 1000, bins=bins)\n",
    "ax.axhline(0, color='gray', alpha=0.4)\n",
    "ax.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2)\n",
    "\n",
    "# 5 year analysis period of relative normality, 2014-2019\n",
    "ax.axvline(datetime.fromisoformat(\"2014-01-01\").timestamp(), color='gray', linestyle='--', alpha=0.4)\n",
    "ax.axvline(datetime.fromisoformat(\"2019-01-01\").timestamp(), color='gray', linestyle='--', alpha=0.4)\n",
    "\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick).strftime('%b%y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing createdAt of guestbooks\n",
    "\n",
    "`new_guestbook_createdAt.txt` created via `cut -f4 -d, new_guestbook_metadata_raw.csv > new_guestbook_createdAt.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr = np.zeros(82854708)\n",
    "with open(os.path.join(working_dir, \"new_guestbook_createdAt.txt\"), 'r') as infile:\n",
    "    error_count = 0\n",
    "    for i, line in tqdm(enumerate(infile), total=82854708):\n",
    "        try:\n",
    "            ca_arr[i] = int(line.strip())\n",
    "        except:\n",
    "            error_count += 1\n",
    "            continue\n",
    "error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr = ca_arr / 1000\n",
    "ca_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(ca_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ca_arr.shape)\n",
    "ca_arr = ca_arr[ca_arr > 0]\n",
    "print(ca_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr_old = np.zeros(82980359)\n",
    "with open(os.path.join(working_dir, \"old_guestbook_createdAt.txt\"), 'r') as infile:\n",
    "    error_count = 0\n",
    "    for i, line in tqdm(enumerate(infile), total=82854708):\n",
    "        try:\n",
    "            ca_arr_old[i] = int(line.strip())\n",
    "        except:\n",
    "            error_count += 1\n",
    "            continue\n",
    "error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr_old = ca_arr_old / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2005\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2019 data)')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr_old, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2016 data)')\n",
    "\n",
    "plt.axvline(datetime.fromisoformat(f\"2016-06-01\").timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Guestbook count\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").timestamp() for i in range((2020 - 2005) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "     \n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.margins(0,0)\n",
    "#plt.savefig(os.path.join(figures_dir, 'initiation_types_timeline.pdf'), dpi=200, pad_inches=0)\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2005\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr, bins=bins)\n",
    "total_counts_old, bin_edges = np.histogram(ca_arr_old, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts - total_counts_old, linestyle='-', linewidth=2, label='Guestbooks (2019 - 2016 data)')\n",
    "\n",
    "plt.axvline(datetime.fromisoformat(f\"2016-06-01\").timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Guestbook count\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").timestamp() for i in range((2020 - 2005) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "     \n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.margins(0,0)\n",
    "#plt.savefig(os.path.join(figures_dir, 'initiation_types_timeline.pdf'), dpi=200, pad_inches=0)\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2016\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2019 data)')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr_old, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2016 data)')\n",
    "\n",
    "plt.axvline(datetime.fromisoformat(f\"2016-06-01\").timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Guestbook count\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2016 + i}-01-01\").timestamp() for i in range((2020 - 2016) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "     \n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.margins(0,0)\n",
    "#plt.savefig(os.path.join(figures_dir, 'initiation_types_timeline.pdf'), dpi=200, pad_inches=0)\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO look for match on guestbook_oid, site_id, and created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
