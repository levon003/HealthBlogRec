{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valid Author Selection\n",
    "===\n",
    "\n",
    "This process generates three outcomes:\n",
    " - List of authors\n",
    " - List of valid authors\n",
    " - List of valid sites\n",
    "\n",
    "Note on requirements: all authored updates most be non-trivial.\n",
    "\n",
    "Requirements for authors:\n",
    " - 1+ update\n",
    " - Never authored an update on a spam site\n",
    "\n",
    "Requirement for valid authors:\n",
    " - 3+ updates authored on SOME site\n",
    " - Never authored an update on a spam site\n",
    "\n",
    "Requirements for valid sites:\n",
    " - 3+ updates from a valid author\n",
    " \n",
    "Data to record about authors:\n",
    " - Update count\n",
    " - Unique site count\n",
    " - Most updates on one site\n",
    " - First to last update time\n",
    " - Is valid\n",
    " - Primary site (if valid)\n",
    " - Number of eligible sites\n",
    " - List of eligible sites\n",
    " \n",
    "Note that what we actually have are valid author/site pairs. Data about them:\n",
    " - user_id\n",
    " - site_id\n",
    " - total_updates\n",
    " - user_total_updates\n",
    " - first_update_timestamp\n",
    " - user_first_update_timestamp\n",
    " - user_third_update_timestamp\n",
    " - user_valid_site_count  `# total valid sites i.e. how many user/site pairs contain this user`\n",
    " - site_valid_user_count  `# total valid users i.e. how many user/site pairs contain this site`\n",
    " \n",
    "Note: the sna-social-support project required \"Most updates on one site\" >= 2 and 24 hours apart.\n",
    "\n",
    "**Question: should we generate recommendations for authors who have published 1 update on a site with 3+ updates?**\n",
    "The intuition behind recommnendation based on author type, etc. is that your writings reveal who you want by encoding information about who you are. So, I think we want to compute embeddings for authors based on first three updates *by that author*.  \n",
    "But this is weird, since we are really recommending sites to authors. The sites are focused on a particular person, but multiple people could be eligible \"authors\" for a site.  We could compute an embedding/interaction features for potentially 2+ authors.  So each author has a \"primary\" site that we use as the basis for recommendation?  Or we just have (user,site) pairs in all cases, and when an author SEEKS a recommendation we select for them the (user,site) pair such that the most recent/large/relevant one is used as the basis for feature extraction?\n",
    "\n",
    "One question: depending on definition, how often would an authors' primary site actually shift?  Maybe we just ALWAYS use a person's first site where they hit 3+ journal updates, and then disregard all other sites? How many sites would that omit? (And how inaccurately would we portray those users by using the \"older\" site for features?)\n",
    "\n",
    "\n",
    "Specifically for embedding feature extraction, it seems like one needs to do so for all (user,site) pairs where user is a valid author and site is an eligible site for that author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "from nltk import word_tokenize\n",
    "from html.parser import HTMLParser\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this imports a number of utility functions related to data annotation & the web client\n",
    "import sys\n",
    "sys.path.append(\"/home/lana/levon003/repos/qual-health-journeys/annotation_data\")\n",
    "import journal as journal_utils\n",
    "import db as db_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all derived data in the data_selection folder\n",
    "working_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data\"\n",
    "os.makedirs(working_dir, exist_ok=True)\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the site metadata dataframe\n",
    "# this is created in caringbridge_core from the new data\n",
    "site_metadata_working_dir = \"/home/lana/shared/caringbridge/data/derived/site_metadata\"\n",
    "s = datetime.now()\n",
    "site_metadata_filepath = os.path.join(site_metadata_working_dir, \"site_metadata.feather\")\n",
    "site_info_df = pd.read_feather(site_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(site_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if np.sum(site_info_df.site_id.duplicated()) > 0:\n",
    "site_info_df[site_info_df.site_id.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any duplicates\n",
    "site_info_df = site_info_df.drop_duplicates(subset='site_id', keep='first')\n",
    "len(site_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(site_info_df.created_at.quantile(0.0001) / 1000).isoformat(),\\\n",
    "datetime.utcfromtimestamp(site_info_df.created_at.max() / 1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal dataframe with the index\n",
    "# this is all the new journal data\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(journal_df.created_at.quantile(0.0001) / 1000).isoformat(),\\\n",
    "datetime.utcfromtimestamp(journal_df.created_at.quantile(0.999999) / 1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vast majority of sites with journals also have site-level metadata\n",
    "# these 16 missing sites might be related to e.g. incomplete deletions on the part of CaringBridge\n",
    "# or, more likely for new journals, the site collection was snapshotted before the journal collection, and new sites were created in the intervening period\n",
    "len(set(journal_df[~journal_df.is_deleted].site_id) - set(site_info_df.site_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim out journal updates that are trivial (short or machine-generated) and deleted\n",
    "print(len(journal_df))\n",
    "journal_df = journal_df[(journal_df.is_nontrivial)&(~journal_df.is_deleted)]\n",
    "print(len(journal_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim out journal updates with invalid dates\n",
    "# (which includes journals without a published_at date)\n",
    "invalid_start_date = datetime.fromisoformat('2005-01-01').replace(tzinfo=pytz.UTC)\n",
    "invalid_end_date = datetime.fromisoformat('2022-01-01').replace(tzinfo=pytz.UTC)\n",
    "print(f\"Keeping journals between {invalid_start_date.isoformat()} and {invalid_end_date.isoformat()}.\")\n",
    "invalid_start_timestamp = invalid_start_date.timestamp() * 1000\n",
    "invalid_end_timestamp = invalid_end_date.timestamp() * 1000\n",
    "print(len(journal_df), np.sum(journal_df.published_at.isna()))\n",
    "journal_df = journal_df[(journal_df.published_at >= invalid_start_timestamp)&(journal_df.published_at <= invalid_end_timestamp)]\n",
    "print(len(journal_df), np.sum(journal_df.published_at.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataframe where each site has a list of user_ids on that site and the total number of non-trivial journals\n",
    "site_proportions = []\n",
    "for site_id, group in tqdm(journal_df.groupby(by='site_id', sort=False)):\n",
    "    total_journals = len(group)\n",
    "    user_ids = set(group.user_id)\n",
    "    site_proportion = {\n",
    "        'site_id': site_id,\n",
    "        'user_ids': user_ids,\n",
    "        'total_journals': total_journals\n",
    "    }\n",
    "    site_proportions.append(site_proportion)\n",
    "len(site_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_proportions_df = pd.DataFrame(site_proportions)\n",
    "len(site_proportions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_proportions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes so that we have more detailed site-level info\n",
    "s = datetime.now()\n",
    "site_df = pd.merge(site_info_df, site_proportions_df, on='site_id', validate='one_to_one')\n",
    "print(datetime.now() - s)\n",
    "len(site_df), len(site_df) / len(site_info_df), len(site_df) / len(site_proportions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.sum(site_df.total_journals == site_df.numJournals) / len(site_df) * 100:.2f}% of sites ({len(site_df)} / {len(site_info_df)} total sites) have a correct 'numJournals' entry.\")\n",
    "site_df[site_df.total_journals != site_df.numJournals].sample(n=10)[['site_id', 'name', 'title', 'privacy', 'publish_date', 'created_at', 'numJournals', 'total_journals', 'isDeactivated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = site_df[~site_df.isDeactivated]\n",
    "print(f\"{np.sum(sdf.total_journals == sdf.numJournals) / len(sdf) * 100:.2f}% of non-deactivated sites ({len(sdf)} / {len(site_df)} sites with 1+ updates) have a correct 'numJournals' entry.\")\n",
    "\n",
    "plt.hist(sdf.numJournals - sdf.total_journals, bins=np.linspace(-100, 100), log=True)\n",
    "plt.xlabel(\"Additional journals in total count but not in subset\")\n",
    "plt.ylabel(\"Site count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(site_info_df.created_at.quantile(0.0001), site_info_df.created_at.max())\n",
    "totals, bin_edges = np.histogram(sdf.created_at, bins=bins)\n",
    "\n",
    "counts, bin_edges = np.histogram(sdf[sdf.numJournals != sdf.total_journals].created_at, bins=bins)\n",
    "pcts = counts / totals\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,6))\n",
    "ax.plot(bin_edges[:-1], pcts)\n",
    "ax.set_title(\"Count mismatches are related to original site creation time\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"% of sites with mismatch between official and actual Journal counts\")\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: datetime.utcfromtimestamp(x / 1000).replace(tzinfo=pytz.timezone('US/Central')).strftime(\"%Y\\n%m/%d\").replace(\" 0\", \" \")))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_counts = journal_df[['user_id', 'site_id']].value_counts()\n",
    "user_site_df = user_site_counts.to_frame(name='update_count').reset_index()\n",
    "print(len(user_site_df))\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_users = []\n",
    "for spam_users in site_df[site_df.isSpam == 1].user_ids:\n",
    "    for spam_user in spam_users:\n",
    "        invalid_users.append(spam_user)\n",
    "print(len(invalid_users))\n",
    "\n",
    "# Manual removal of users who are invalid for other reasons\n",
    "invalid_users.extend([\n",
    "    0,  # Test user\n",
    "    15159562,  # Test user account run by CaringBridge Customer Experience team\n",
    "    46,  # Seems to be run at least in part by CaringBridge team for testing\n",
    "    13896060,  # Seems like another customer care rep\n",
    "    594,  # Seems like a customer care rep, but also seems like it may include some legitimate sites? (see e.g. site 559205)\n",
    "    7393709, #Junk and test posts\n",
    "    25036137, #Repeated test text\n",
    "    8192483, #Mostly test posts, but one genuine post about patient\n",
    "    17956362, #Test posts\n",
    "    16648084, #Test posts (and some good poetry)\n",
    "    31761432, # Doctor's ad\n",
    "    32764680, # Payday lending ad\n",
    "    30457719, # 3D visualization company ad\n",
    "    32538830, # Car supplies ad\n",
    "    32757690, # Fashion ad\n",
    "    32757739, # Clothing brand ad\n",
    "    1043681, # Leasing furniture ad\n",
    "    28132146, # Farm company ad\n",
    "    31477721, # Lenders ad\n",
    "    31879875, # Payday lender ad\n",
    "    31799168, # Credit company ad\n",
    "    32428328, # SEO ad\n",
    "    31684805, # Various ads\n",
    "    30165532, # Various ads about black magic\n",
    "    31833912, # Job hunting spam\n",
    "    32753111, # Arabic text (possibly spam)\n",
    "    32732132 # Turkish text (spam)\n",
    "])\n",
    "print(len(invalid_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_df = user_site_df[~user_site_df.user_id.isin(invalid_users)].reset_index(drop=True)\n",
    "len(user_site_df), len(set(user_site_df.user_id)), len(set(user_site_df.site_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any sites that are deactivated\n",
    "# note that we do this as a separate step, rather than removing all users who have published on deactivated sites, since we want to remove deleted sites but don't consider it to be \"author poison\" the way publishing on a spam site is\n",
    "deactivated_sites = set(site_df[site_df.isDeactivated].site_id)\n",
    "print(len(deactivated_sites))\n",
    "user_site_df = user_site_df[~user_site_df.site_id.isin(deactivated_sites)].reset_index(drop=True)\n",
    "len(user_site_df), len(set(user_site_df.user_id)), len(set(user_site_df.site_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "user_site_df.to_feather(os.path.join(working_dir, 'user_site_df.feather'))\n",
    "user_site_df.to_csv(os.path.join(working_dir, 'user_site_df.csv'), index=False)\n",
    "print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_counts = journal_df[['user_id', 'site_id']].value_counts()\n",
    "user_site_df = user_site_counts[user_site_counts >= 3].to_frame(name='update_count')\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_site_df.index.to_frame(index=False)\n",
    "user_site_df = user_site_df.reset_index()\n",
    "user_site_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12,813 authors with multiple sites\n",
    "user_eligible_site_count = user_site_df['user_id'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "bins = np.arange(1, 20)\n",
    "ax.hist(user_eligible_site_count, bins=bins, log=True)\n",
    "\n",
    "ax.axvline(2, linestyle='--', color='black')\n",
    "ax.text(0.12, 0.97, f'{np.sum(user_eligible_site_count > 1):,} ({np.sum(user_eligible_site_count > 1) / len(user_eligible_site_count) * 100:.2f}%) authors have > 1 eligible site', transform=ax.transAxes, va='top', ha='left')\n",
    "\n",
    "ax.set_xlabel(\"Number of sites with 3+ journal updates\")\n",
    "ax.set_ylabel(\"User count\")\n",
    "ax.set_title(\"Some authors have 3+ non-trivial updates on multiple sites\")\n",
    "\n",
    "ax.set_xticks(bins)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_site_df contains only users with 3+ journal updates on at least one site\n",
    "valid_users = set(user_site_df.user_id)\n",
    "len(valid_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 in valid_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_for_spam = 0\n",
    "for spam_users in tqdm(site_df[~site_df.isSpam.isna()].user_ids):\n",
    "    for spam_user in spam_users:\n",
    "        if spam_user in valid_users:\n",
    "            valid_users.remove(spam_user)\n",
    "            removed_for_spam += 1\n",
    "removed_for_spam, len(valid_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual removal of users who are invalid for other reasons\n",
    "invalid_users = [\n",
    "    0,  # Test user\n",
    "    15159562,  # Test user account run by CaringBridge Customer Experience team\n",
    "    46,  # Seems to be run at least in part by CaringBridge team for testing\n",
    "    13896060,  # Seems like another customer care rep\n",
    "    594,  # Seems like a customer care rep, but also seems like it may include some legitimate sites? (see e.g. site 559205)\n",
    "    7393709, #Junk and test posts\n",
    "    25036137, #Repeated test text\n",
    "    8192483, #Mostly test posts, but one genuine post about patient\n",
    "    17956362, #Test posts\n",
    "    16648084, #Test posts (and some good poetry)\n",
    "    31761432, # Doctor's ad\n",
    "    32764680, # Payday lending ad\n",
    "    30457719, # 3D visualization company ad\n",
    "    32538830, # Car supplies ad\n",
    "    32757690, # Fashion ad\n",
    "    32757739, # Clothing brand ad\n",
    "    1043681, # Leasing furniture ad\n",
    "    28132146, # Farm company ad\n",
    "    31477721, # Lenders ad\n",
    "    31879875, # Payday lender ad\n",
    "    31799168, # Credit company ad\n",
    "    32428328, # SEO ad\n",
    "    31684805, # Various ads\n",
    "    30165532, # Various ads about black magic\n",
    "    31833912, # Job hunting spam\n",
    "    32753111, # Arabic text (possibly spam)\n",
    "    32732132 # Turkish text (spam)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_manually = 0\n",
    "for invalid_user in invalid_users:\n",
    "    if invalid_user in valid_users:\n",
    "        valid_users.remove(invalid_user)\n",
    "        removed_manually += 1\n",
    "removed_manually, len(valid_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what percent of users remain?\n",
    "len(valid_users) / len(set(journal_df.user_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out valid users to text file\n",
    "with open(os.path.join(working_dir, \"valid_user_ids.txt\"), 'w') as outfile:\n",
    "    for user_id in valid_users:\n",
    "        outfile.write(str(user_id) + \"\\n\")\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read valid users\n",
    "valid_users = set()\n",
    "with open(os.path.join(working_dir, \"valid_user_ids.txt\"), 'r') as infile:\n",
    "    for line in infile:\n",
    "        user_id = line.strip()\n",
    "        if user_id == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            valid_users.add(int(user_id))\n",
    "len(valid_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site analysis & filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(user_site_df))\n",
    "user_site_df = user_site_df[user_site_df.user_id.isin(valid_users)]\n",
    "print(len(user_site_df))\n",
    "valid_sites = set(user_site_df.site_id)\n",
    "len(valid_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_df.sample(n=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_site_ids = valid_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_site_ids), len(valid_site_ids) / len(set(site_df.site_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there any spam sites still in the sample?\n",
    "# no, as expected\n",
    "len(site_df[(~site_df.isSpam.isna())&(site_df.site_id.isin(valid_site_ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many sites included in this sample have only a single (substantive) update?\n",
    "# None! We have changed the criteria compared to the CSCW paper\n",
    "total_single_update_sites = len(site_df[(site_df.site_id.isin(valid_site_ids))&(site_df.total_journals == 1)])\n",
    "total_single_update_sites, total_single_update_sites / len(valid_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_update_site_ids = np.array(site_df[(site_df.site_id.isin(valid_site_ids))&(site_df.total_journals == 1)].site_id)\n",
    "len(single_update_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out valid sites to text file\n",
    "with open(os.path.join(working_dir, \"valid_site_ids.txt\"), 'w') as outfile:\n",
    "    for site_id in valid_site_ids:\n",
    "        outfile.write(str(site_id) + \"\\n\")\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read valid sites\n",
    "valid_site_ids = set()\n",
    "with open(os.path.join(working_dir, \"valid_site_ids.txt\"), 'r') as infile:\n",
    "    for line in infile:\n",
    "        site_id = line.strip()\n",
    "        if site_id == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            valid_site_ids.add(int(site_id))\n",
    "len(valid_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(site_df[site_df.site_id.isin(valid_site_ids)].visits, log=True, bins=range(500))\n",
    "plt.title(\"Distribution of selected sites by number of visits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create valid user/site dataframe\n",
    "\n",
    " - user_id\n",
    " - site_id\n",
    " - total_updates\n",
    " - user_total_updates\n",
    " - first_update_timestamp\n",
    " - user_first_update_timestamp\n",
    " - user_third_update_timestamp\n",
    " - user_valid_site_count  `# total valid sites i.e. how many user/site pairs contain this user`\n",
    " - site_valid_user_count  `# total valid users i.e. how many user/site pairs contain this site`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_site_df = user_site_counts[user_site_counts >= 3].to_frame(name='update_count').reset_index()\n",
    "user_site_df = user_site_df[user_site_df.user_id.isin(valid_users)]\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add total_updates column\n",
    "site_counts = journal_df.site_id.value_counts().to_frame(name='total_updates').rename_axis(index='site_id').reset_index()\n",
    "user_site_df = pd.merge(user_site_df, site_counts, how='left', on='site_id')\n",
    "user_site_df = user_site_df.rename(columns={'update_count': 'user_total_updates'})\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_valid_site_count = user_site_df.groupby('user_id').site_id.nunique()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "bins=np.arange(1, 12)\n",
    "ax.hist(user_valid_site_count, log=True, bins=bins)\n",
    "ax.set_xticks(bins)\n",
    "ax.axvline(2, linestyle='--', color='black')\n",
    "ax.text(0.15, 0.97, f'{np.sum(user_valid_site_count > 1):,} ({np.sum(user_valid_site_count > 1) / len(user_valid_site_count) * 100:.2f}%) authors have > 1 eligible site', transform=ax.transAxes, va='top', ha='left')\n",
    "ax.set_xlabel(\"Number of sites with 3+ journal updates\")\n",
    "ax.set_ylabel(\"User count\")\n",
    "ax.set_title(\"Some authors have 3+ non-trivial updates on multiple sites\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_valid_user_count = user_site_df.groupby('site_id').user_id.nunique()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "bins=np.arange(1, 12)\n",
    "ax.hist(site_valid_user_count, log=True, bins=bins)\n",
    "ax.set_xticks(bins)\n",
    "ax.axvline(2, linestyle='--', color='black')\n",
    "ax.text(0.15, 0.97, f'{np.sum(site_valid_user_count > 1):,} ({np.sum(site_valid_user_count > 1) / len(site_valid_user_count) * 100:.2f}%) valid sites have > 1 eligible author', transform=ax.transAxes, va='top', ha='left')\n",
    "ax.set_xlabel(\"Number of valid authors with 3+ updates on site\")\n",
    "ax.set_ylabel(\"Site count\")\n",
    "ax.set_title(\"Some sites have multiple authors with 3+ updates\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_valid_site_count = user_valid_site_count.to_frame('user_valid_site_count').reset_index()\n",
    "site_valid_user_count = site_valid_user_count.to_frame('site_valid_user_count').reset_index()\n",
    "user_site_df = pd.merge(user_site_df, user_valid_site_count, how='left', on='user_id')\n",
    "user_site_df = pd.merge(user_site_df, site_valid_user_count, how='left', on='site_id')\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "user_first_update_timestamp_dict = {}\n",
    "user_third_update_timestamp_dict = {}\n",
    "#df[[\"A\",\"B\"]].apply(tuple, 1).isin(AB_col)\n",
    "valid_tuples = user_site_df[['user_id', 'site_id']].apply(tuple, 1)\n",
    "filtered_journals = journal_df[journal_df.user_id.isin(valid_users)]\n",
    "filtered_journal_tuples = filtered_journals[['user_id', 'site_id']].apply(tuple, 1)\n",
    "filtered_journals = filtered_journals[filtered_journal_tuples.isin(valid_tuples)]\n",
    "print(f\"Starting groupby after {datetime.now() - s}.\")\n",
    "for key, group in tqdm(filtered_journals[['user_id', 'site_id', 'created_at']].groupby(['user_id', 'site_id'])):\n",
    "    created_at = group.created_at.sort_values(ascending=True)\n",
    "    user_first_update_timestamp_dict[key] = created_at.iloc[0]\n",
    "    user_third_update_timestamp_dict[key] = created_at.iloc[2]\n",
    "print(f\"Finished groupby after {datetime.now() - s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_first_update_timestamp_list = []\n",
    "user_third_update_timestamp_list = []\n",
    "for user_id, site_id in zip(user_site_df.user_id, user_site_df.site_id):\n",
    "    user_first_update_timestamp = user_first_update_timestamp_dict[(user_id, site_id)]\n",
    "    user_first_update_timestamp_list.append(user_first_update_timestamp)\n",
    "    user_third_update_timestamp = user_third_update_timestamp_dict[(user_id, site_id)]\n",
    "    user_third_update_timestamp_list.append(user_third_update_timestamp)\n",
    "user_site_df = user_site_df.assign(user_first_update_timestamp=user_first_update_timestamp_list, user_third_update_timestamp=user_third_update_timestamp_list)\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_first_update_df = journal_df[['site_id', 'created_at']].groupby('site_id').min().reset_index().rename(columns={'created_at': 'first_update_timestamp'})\n",
    "user_site_df = pd.merge(user_site_df, site_first_update_df, how='left', on='site_id')\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user's first update should always be >= the first update on the site\n",
    "assert np.sum(user_site_df.first_update_timestamp <= user_site_df.user_first_update_timestamp) == len(user_site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this number should be very large\n",
    "# (in fact, the two should be equal on any site where total_updates == user_total_updates\n",
    "np.sum(user_site_df.first_update_timestamp == user_site_df.user_first_update_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = user_site_df[user_site_df.total_updates == user_site_df.user_total_updates]\n",
    "print(f\"{len(sdf)} ({len(sdf) / len(user_site_df)*100:.2f}%) user/site pairs are on single-author sites.\")\n",
    "assert np.all(sdf.first_update_timestamp == sdf.user_first_update_timestamp)\n",
    "assert np.all(sdf.site_valid_user_count == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "user_site_df.to_feather(os.path.join(working_dir, 'user_site_df.feather'))\n",
    "user_site_df.to_csv(os.path.join(working_dir, 'user_site_df.csv'), index=False)\n",
    "print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate valid authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of valid user/site pairs\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "user_site_df = pd.read_csv(os.path.join(model_data_dir, 'user_site_df.csv'))\n",
    "valid_user_ids = set(user_site_df.user_id)\n",
    "print(f\"Read {len(user_site_df)} rows ({len(valid_user_ids)} unique users) in {datetime.now() - s}.\")\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2005\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(user_site_df.user_third_update_timestamp / 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='All valid user/site pairs')\n",
    "total_counts, bin_edges = np.histogram(user_site_df[user_site_df.user_valid_site_count == 1].user_third_update_timestamp / 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Single-site authors')\n",
    "total_counts, bin_edges = np.histogram(user_site_df[user_site_df.site_valid_user_count == 1].user_third_update_timestamp / 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Single-author sites')\n",
    "plt.legend()\n",
    "plt.axvline(datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "plt.axvline(datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"New user/site pairs\")\n",
    "plt.title(f\"{len(user_site_df):,} valid user/site pairs containing {len(set(user_site_df.user_id)):,} unique users and {len(set(user_site_df.site_id)):,} unique sites\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").timestamp() for i in range((2020 - 2005) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users with the largest number of valid sites\n",
    "user_df = user_site_df.drop_duplicates(subset='user_id')\n",
    "user_df[['user_id', 'user_valid_site_count']].sort_values(by='user_valid_site_count', ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites with the largest number of valid users\n",
    "site_df = user_site_df.drop_duplicates(subset='site_id')\n",
    "site_df[['site_id', 'site_valid_user_count']].sort_values(by='site_valid_user_count', ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user/site pairs with the largest number of updates on a single site\n",
    "user_site_df.sort_values(by='user_total_updates', ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shared-conda)",
   "language": "python",
   "name": "shared-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
