{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Guestbook Extraction\n",
    "===\n",
    "\n",
    "This script processes the json guestbooks in the new (2019) dataset to a CSV file containing the barebones interaction info.\n",
    "\n",
    "This code has been entirely superceded by the InteractionExtraction notebook and the caringbridge_core import scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import sqlite3\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbcore.data.paths as paths\n",
    "import cbcore.data.dates as dates\n",
    "import cbcore.data.utils as utils\n",
    "from cbcore.data.utils import extract_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = paths.raw_data_2019_filepath\n",
    "raw_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data\"\n",
    "assert os.path.exists(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guestbook_filepath = os.path.join(raw_data_dir, 'guestbook_scrubbed.json')\n",
    "output_filepath = os.path.join(working_dir, \"guestbook_all.tsv\")\n",
    "with open(output_filepath, 'w') as outfile:\n",
    "    with open(guestbook_filepath, encoding='utf-8') as infile:\n",
    "        processed_count = 0\n",
    "        for i, line in tqdm(enumerate(infile), total=82858710):\n",
    "            if i < 4002:\n",
    "                continue\n",
    "            try:\n",
    "                gb = json.loads(line)\n",
    "            except:\n",
    "                continue\n",
    "            gb_oid = gb['_id']['$oid']\n",
    "            site_id = extract_long(gb['siteId'])\n",
    "            user_id = extract_long(gb['userId'])\n",
    "\n",
    "            platform = gb['platform'] if 'platform' in gb else None\n",
    "            userAgent = gb['userAgent'] if 'userAgent' in gb else None\n",
    "            isDeleted = gb['isDeleted'] if 'isDeleted' in gb else None\n",
    "            ip = gb['ip'] if 'ip' in gb else None\n",
    "            fromTribute = gb['fromTribute'] if 'fromTribute' in gb else None\n",
    "\n",
    "            body = gb['body'] if 'body' in gb and gb['body'] is not None else ''\n",
    "            signature = gb['signature'] if 'signature' in gb and gb['signature'] is not None else ''\n",
    "\n",
    "            created_at = dates.get_date_from_json_value(gb['createdAt']) if 'createdAt' in gb else 0\n",
    "            updated_at = dates.get_date_from_json_value(gb['updatedAt']) if 'updatedAt' in gb else 0\n",
    "            if created_at is None:\n",
    "                created_at = 0\n",
    "            if updated_at is None:\n",
    "                updated_at = 0\n",
    "\n",
    "            photo_count = 0\n",
    "            if 'photos' in gb:\n",
    "                photo_count = len(gb['photos'])\n",
    "\n",
    "            amps_count = 0\n",
    "            amps = None\n",
    "            if 'amps' in gb and type(gb['amps']) == list:\n",
    "                amps_count = len(gb['amps'])\n",
    "                # just represent the amps as a string, which can be evaled by subsequent processing code to extract the list\n",
    "                # but we do a simple step to extract the underlying long values first, if using the \"new\" dict format for the literal userId values in the list\n",
    "                amps = \"[]\"\n",
    "                if amps_count > 0:\n",
    "                    if type(gb['amps'][0]) == dict and '$numberLong' in gb['amps'][0]:\n",
    "                        amps = str([v['$numberLong'] for v in gb['amps']])\n",
    "                    else:\n",
    "                        amps = str(gb['amps'])\n",
    "\n",
    "            result = (gb_oid, site_id, user_id, created_at, updated_at, body, signature, isDeleted, platform, userAgent, ip, fromTribute, photo_count, amps_count, amps)\n",
    "            result = [str(val).replace('\\t', '\\\\t').replace('\\n', '\\\\n') if val is not None else '' for val in result]\n",
    "            outfile.write('\\t'.join(result)+'\\n')\n",
    "            \n",
    "            #outfile.write(f\"{user_id},{site_id},guestbook,{created_at},{updated_at}\\n\")\n",
    "            processed_count += 1\n",
    "processed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db(db_filename):\n",
    "    db = sqlite3.connect(\n",
    "            db_filename,\n",
    "            detect_types=sqlite3.PARSE_DECLTYPES\n",
    "        )\n",
    "    db.row_factory = sqlite3.Row\n",
    "    return db\n",
    "\n",
    "\n",
    "def create_table(db, drop_table=True):\n",
    "    if drop_table:\n",
    "        db.execute(\"DROP TABLE IF EXISTS guestbook\")\n",
    "    create_table_command = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS guestbook (\n",
    "          id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "          gb_oid TEXT NOT NULL,\n",
    "          site_id INTEGER NOT NULL,\n",
    "          user_id INTEGER NOT NULL,\n",
    "          created_at INTEGER NOT NULL,\n",
    "          updated_at INTEGER NOT NULL,\n",
    "          body TEXT,\n",
    "          signature TEXT,\n",
    "          platform TEXT,\n",
    "          userAgent TEXT,\n",
    "          isDeleted TEXT,\n",
    "          ip TEXT,\n",
    "          fromTribute TEXT,\n",
    "          photo_count INTEGER NOT NULL,\n",
    "          amps_count INTEGER NOT NULL,\n",
    "          amps TEXT\n",
    "        )\n",
    "    \"\"\"\n",
    "    db.execute(create_table_command)\n",
    "    db.commit()\n",
    "\n",
    "guestbook_filepath = os.path.join(raw_data_dir, 'guestbook_scrubbed.json')\n",
    "output_filepath = os.path.join(working_dir, \"guestbook_scrubbed.sqlite\")\n",
    "try:\n",
    "    db = get_db(output_filepath)\n",
    "    create_table(db)\n",
    "    with open(guestbook_filepath, encoding='utf-8') as infile:\n",
    "        processed_count = 0\n",
    "        s = datetime.now()\n",
    "        for i, line in tqdm(enumerate(infile), total=82858710):\n",
    "            if i < 4002:\n",
    "                continue\n",
    "            try:\n",
    "                gb = json.loads(line)\n",
    "            except:\n",
    "                continue\n",
    "            gb_oid = gb['_id']['$oid']\n",
    "            site_id = extract_long(gb['siteId'])\n",
    "            user_id = extract_long(gb['userId'])\n",
    "\n",
    "            platform = gb['platform'] if 'platform' in gb else None\n",
    "            userAgent = gb['userAgent'] if 'userAgent' in gb else None\n",
    "            isDeleted = gb['isDeleted'] if 'isDeleted' in gb else None\n",
    "            ip = gb['ip'] if 'ip' in gb else None\n",
    "            fromTribute = gb['fromTribute'] if 'fromTribute' in gb else None\n",
    "\n",
    "            body = gb['body'] if 'body' in gb and gb['body'] is not None else ''\n",
    "            signature = gb['signature'] if 'signature' in gb and gb['signature'] is not None else ''\n",
    "\n",
    "            created_at = dates.get_date_from_json_value(gb['createdAt']) if 'createdAt' in gb else 0\n",
    "            updated_at = dates.get_date_from_json_value(gb['updatedAt']) if 'updatedAt' in gb else 0\n",
    "            if created_at is None:\n",
    "                created_at = 0\n",
    "            if updated_at is None:\n",
    "                updated_at = 0\n",
    "\n",
    "            photo_count = 0\n",
    "            if 'photos' in gb:\n",
    "                photo_count = len(gb['photos'])\n",
    "\n",
    "            amps_count = 0\n",
    "            amps = None\n",
    "            if 'amps' in gb and type(gb['amps']) == list:\n",
    "                amps_count = len(gb['amps'])\n",
    "                # just represent the amps as a string, which can be evaled by subsequent processing code to extract the list\n",
    "                # but we do a simple step to extract the underlying long values first, if using the \"new\" dict format for the literal userId values in the list\n",
    "                amps = \"[]\"\n",
    "                if amps_count > 0:\n",
    "                    if type(gb['amps'][0]) == dict and '$numberLong' in gb['amps'][0]:\n",
    "                        amps = str([v['$numberLong'] for v in gb['amps']])\n",
    "                    else:\n",
    "                        amps = str(gb['amps'])\n",
    "            \n",
    "            db.execute(\n",
    "                    'INSERT OR IGNORE INTO guestbook (gb_oid, site_id, user_id, created_at, updated_at, body, signature, isDeleted, platform, userAgent, ip, fromTribute, photo_count, amps_count, amps) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)',\n",
    "                    (gb_oid, site_id, user_id, created_at, updated_at, body, signature, isDeleted, platform, userAgent, ip, fromTribute, photo_count, amps_count, amps)\n",
    "                )\n",
    "            processed_count += 1\n",
    "            if processed_count % 500000 == 0:\n",
    "                db.commit()\n",
    "                print(f\"Rows committed after {datetime.now() - s}. ({processed_count} total)\")\n",
    "        db.commit()\n",
    "        print(f\"Final rows committed after {datetime.now() - s}. ({processed_count} total)\")\n",
    "finally:\n",
    "    db.close()\n",
    "processed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = os.path.join(working_dir, \"guestbook_metadata.csv\")\n",
    "guestbook_filepath = os.path.join(raw_data_dir, 'guestbook_scrubbed.json')\n",
    "with open(output_filepath, 'w') as outfile:\n",
    "    with open(guestbook_filepath, encoding='utf-8') as infile:\n",
    "        processed_count = 0\n",
    "        for i, line in tqdm(enumerate(infile), total=82858710):\n",
    "            if i < 4002:\n",
    "                continue\n",
    "            try:\n",
    "                gb = json.loads(line)\n",
    "            except:\n",
    "                continue\n",
    "            gb_oid = gb['_id']['$oid']\n",
    "            site_id = utils.extract_long(gb['siteId'])\n",
    "            user_id = utils.extract_long(gb['userId'])\n",
    "            created_at = dates.get_date_from_json_value(gb['createdAt']) if 'createdAt' in gb else 0\n",
    "            updated_at = dates.get_date_from_json_value(gb['updatedAt']) if 'updatedAt' in gb else 0\n",
    "            \n",
    "            outfile.write(f\"{user_id},{site_id},guestbook,{created_at},{updated_at}\\n\")\n",
    "            processed_count += 1\n",
    "processed_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing createdAt of guestbooks\n",
    "\n",
    "`new_guestbook_createdAt.txt` created via `cut -f4 -d, new_guestbook_metadata_raw.csv > new_guestbook_createdAt.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr = np.zeros(82854708)\n",
    "with open(os.path.join(working_dir, \"new_guestbook_createdAt.txt\"), 'r') as infile:\n",
    "    error_count = 0\n",
    "    for i, line in tqdm(enumerate(infile), total=82854708):\n",
    "        try:\n",
    "            ca_arr[i] = int(line.strip())\n",
    "        except:\n",
    "            error_count += 1\n",
    "            continue\n",
    "error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr = ca_arr / 1000\n",
    "ca_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(ca_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ca_arr.shape)\n",
    "ca_arr = ca_arr[ca_arr > 0]\n",
    "print(ca_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr_old = np.zeros(82980359)\n",
    "with open(os.path.join(working_dir, \"old_guestbook_createdAt.txt\"), 'r') as infile:\n",
    "    error_count = 0\n",
    "    for i, line in tqdm(enumerate(infile), total=82854708):\n",
    "        try:\n",
    "            ca_arr_old[i] = int(line.strip())\n",
    "        except:\n",
    "            error_count += 1\n",
    "            continue\n",
    "error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr_old = ca_arr_old / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2005\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2019 data)')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr_old, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2016 data)')\n",
    "\n",
    "plt.axvline(datetime.fromisoformat(f\"2016-06-01\").timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Guestbook count\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").timestamp() for i in range((2020 - 2005) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "     \n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.margins(0,0)\n",
    "#plt.savefig(os.path.join(figures_dir, 'initiation_types_timeline.pdf'), dpi=200, pad_inches=0)\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2005\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr, bins=bins)\n",
    "total_counts_old, bin_edges = np.histogram(ca_arr_old, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts - total_counts_old, linestyle='-', linewidth=2, label='Guestbooks (2019 - 2016 data)')\n",
    "\n",
    "plt.axvline(datetime.fromisoformat(f\"2016-06-01\").timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Guestbook count\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").timestamp() for i in range((2020 - 2005) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "     \n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.margins(0,0)\n",
    "#plt.savefig(os.path.join(figures_dir, 'initiation_types_timeline.pdf'), dpi=200, pad_inches=0)\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2016\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2019 data)')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr_old, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2016 data)')\n",
    "\n",
    "plt.axvline(datetime.fromisoformat(f\"2016-06-01\").timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Guestbook count\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2016 + i}-01-01\").timestamp() for i in range((2020 - 2016) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "     \n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.margins(0,0)\n",
    "#plt.savefig(os.path.join(figures_dir, 'initiation_types_timeline.pdf'), dpi=200, pad_inches=0)\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO look for match on guestbook_oid, site_id, and created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
