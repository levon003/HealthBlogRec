{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction Extraction\n",
    "===\n",
    "\n",
    "This file processes all interactions on CaringBridge into files.\n",
    "\n",
    "It processes:\n",
    " - Journal updates\n",
    " - Comments (standalone and from guestbooks)\n",
    " - Guestbooks\n",
    " - Amps (from updates, comments, and guestbooks)\n",
    "\n",
    "The fields in the resulting data-frame:\n",
    "\n",
    " - user_id\n",
    " - site_id\n",
    " - interaction_type (update, comment, guestbook, amp)\n",
    " - interaction_oid\n",
    " - parent_type (update, comment, guestbook)\n",
    " - parent_id (depending on parent, either a journal_oid, a comment_oid, or a gb_oid) \n",
    " - created_at\n",
    " - updated_at\n",
    "\n",
    "Other keys:\n",
    "- ancestor_type (either an update or a guestbook) - do we need this for any reason? I think not\n",
    "\n",
    "Question: do we need to do this for journal updates, or do we essentially already have that data?\n",
    "I think we essentially have it. But, so we can straightforwardly stream the file, I'm generating it anyway?\n",
    "\n",
    "The pseudocode process for using this data looks like:\n",
    " - Load interaction\n",
    " - Update new eligible authors based on time elapsed\n",
    "   - Special case: if no time elapsed (because this interaction occurred at the literal same timestamp as the last), do nothing and re-use e.g. eligible author pools without recomputing them\n",
    "   - Identify any user/site pairs with an \"initial join\" time (i.e. their 3rd published update on some site)\n",
    "   - Do this by just slowly chewing through a stack sorted by user/site \"initial join\" time\n",
    "   - Update site and author maps: add new authors to sites, and sites to authors.  Q: is this something that depends on FIRST update on a site, or THIRD update? In the CSCW project, we used existing + future links based on author type (being a patient). Here, seems like we just want to use current authorship i.e. THIRD update.\n",
    " - Update activity map\n",
    "   - Add any intervening journal updates as activity\n",
    "   - Remove any \"timedout\" users\n",
    " - Identify user/site sources and targets\n",
    " - Negative sample alt author(s) for each user/site source/target combination\n",
    "   - Note: need a network of authors to sites they've interacted with (independent from the author-author network)\n",
    " - Generate activity features for all implicated\n",
    " - Generate network features for all implicated\n",
    " - Retrieve other cached features (such as from journal texts)\n",
    " - Store triples in database\n",
    " - Update network connections\n",
    "   - Use site maps: add link to authors that are in the current site map. see Q above.\n",
    "   - Point of confusion: u1 is active on s1 and u2 is active on s2 at t, u3 interacts with s2 at t+1, u1 is active on s2 at t+2. \n",
    "   What should happen? at t+1, u3->u2 exists. At t+2, u3->u1 exists.  \n",
    "   How to identify that this edge should exist?  Well, could just look at in-bound edges of u2 if u1 becomes active on u2s site, then duplicate those edges to refer to u1.  But, the problem is that not all connections to u2 are because of s2.  \n",
    "   Solution: store (u1,s1) pairs as nodes in the network. Problem: u1 has interacted with a site before any eligible person exists on that site, or u1 interacts before having any eligible sites themselves. Possibility: create \"dummy\" nodes, e.g. (u1,s1)->(*,s2) and (u1,*)->(u2,s2) respectively, that are special nodes when user info is not yet available. \n",
    "   Second thought: this is clearly absurd.  In particular, we KNOW when a user becomes active on a site (in the user_site_df).  \n",
    "   So thought 1 is two separate networks: user->site, site->user\n",
    "   Thought 2: just a user->site network. At time t, can just resolve each site to 0 or more users (if it resolves to 0 users, then we have updated the network with the new edge but can't generate any new sites. If it resolves to 2+ users, then we generate one new interaction for each users.)\n",
    "   But this is clearly a disaster; to determine something like weakly-connected component, you need to resolve all the sites into nodes. In other words, this is just a bipartite graph by another face.\n",
    "   Thought 3: user->user network.  But need a separate store of user->site links, just as a list or dict or whatever, that can be used when a user becomes active on a site; we retrieve all existing user->site links, then construct the corresponding user->user links in the graph.  (This idea seems really reasonable to me!)\n",
    "   Basically, do two activities:\n",
    "   1. When interaction u1->s2 happens, put {s2: u1} in dictionary, get current eligible users of s2 (get d[s2] where d is site_id->set(user_id), add edges (u1,uX for X in d[s2]).\n",
    "   2. When u3 becomes active on s2, d[s2].add(u3), for each user uX in edge dict[s2], create edge (uX, u3).\n",
    "   It's okay to have user nodes in the graph that aren't yet active! (TODO check this assumption)\n",
    "   \n",
    " - Add to activity map with the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import sqlite3\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbcore.data.paths as paths\n",
    "import cbcore.data.dates as dates\n",
    "import cbcore.data.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = paths.raw_data_2019_filepath\n",
    "raw_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data\"\n",
    "assert os.path.exists(working_dir)\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of valid user/site pairs\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "user_site_df = pd.read_csv(os.path.join(model_data_dir, 'user_site_df.csv'))\n",
    "valid_user_ids = set(user_site_df.user_id)\n",
    "valid_site_ids = set(user_site_df.site_id)\n",
    "print(f\"Read {len(user_site_df)} rows ({len(valid_user_ids)} unique users, {len(valid_site_ids)} unique sites) in {datetime.now() - s}.\")\n",
    "user_site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guestbook_filepath = os.path.join(raw_data_dir, 'guestbook_scrubbed.json')\n",
    "output_filepath = os.path.join(working_dir, \"guestbook_filtered.csv\")\n",
    "both_valid_count = 0\n",
    "neither_valid_count = 0\n",
    "author_valid_count = 0\n",
    "site_valid_count = 0\n",
    "with open(output_filepath, 'w') as outfile:\n",
    "    with open(guestbook_filepath, encoding='utf-8') as infile:\n",
    "        processed_count = 0\n",
    "        for i, line in tqdm(enumerate(infile), total=82858710):\n",
    "            if i < 4002:\n",
    "                continue\n",
    "            try:\n",
    "                gb = json.loads(line)\n",
    "            except:\n",
    "                continue\n",
    "            gb_oid = gb['_id']['$oid']\n",
    "            site_id = utils.extract_long(gb['siteId'])\n",
    "            user_id = utils.extract_long(gb['userId'])\n",
    "            created_at = dates.get_date_from_json_value(gb['createdAt']) if 'createdAt' in gb else 0\n",
    "            updated_at = dates.get_date_from_json_value(gb['updatedAt']) if 'updatedAt' in gb else 0\n",
    "            \n",
    "            if 'amps' in gb and type(gb['amps']) == list:\n",
    "                # we write out any amps as separate lines\n",
    "                for amp in gb['amps']:\n",
    "                    amp_user_id = utils.extract_long(amp)\n",
    "                    is_user_valid = amp_user_id in valid_user_ids\n",
    "                    is_site_valid = site_id in valid_site_ids\n",
    "                    if is_user_valid and is_site_valid:\n",
    "                        outfile.write(f\"{amp_user_id},{site_id},amp,{gb_oid}|{amp_user_id},guestbook,{gb_oid},guestbook,{gb_oid},{created_at},{updated_at}\\n\")\n",
    "                        both_valid_count += 1\n",
    "                    elif is_user_valid and not is_site_valid:\n",
    "                        author_valid_count += 1\n",
    "                    elif not is_user_valid and is_site_valid:\n",
    "                        site_valid_count += 1\n",
    "                    else:\n",
    "                        neither_valid_count += 1\n",
    "            is_user_valid = user_id in valid_user_ids\n",
    "            is_site_valid = site_id in valid_site_ids\n",
    "            if is_user_valid and is_site_valid:\n",
    "                # columns: user_id, site_id, interaction_type, interaction_oid, parent_type, parent_id, ancestor_type, ancestor_id, created_at, updated_at\n",
    "                outfile.write(f\"{user_id},{site_id},guestbook,{gb_oid},None,None,None,None,{created_at},{updated_at}\\n\")\n",
    "                both_valid_count += 1\n",
    "            elif is_user_valid and not is_site_valid:\n",
    "                author_valid_count += 1\n",
    "            elif not is_user_valid and is_site_valid:\n",
    "                site_valid_count += 1\n",
    "            else:\n",
    "                neither_valid_count += 1\n",
    "            processed_count += 1\n",
    "processed_count, both_valid_count, neither_valid_count, author_valid_count, site_valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_filepath = os.path.join(raw_data_dir, 'comment_scrubbed.json')\n",
    "output_filepath = os.path.join(working_dir, \"comment_filtered.csv\")\n",
    "both_valid_count = 0\n",
    "neither_valid_count = 0\n",
    "author_valid_count = 0\n",
    "site_valid_count = 0\n",
    "with open(output_filepath, 'w') as outfile:\n",
    "    with open(comments_filepath, encoding='utf-8') as infile:\n",
    "        for line in tqdm(infile, total=31052715):\n",
    "            comment = json.loads(line)\n",
    "            comment_oid = comment['_id']['$oid']\n",
    "            parent_type = comment['parentType']  # either 'journal' or 'comment'\n",
    "            parent_oid = comment['parentId']\n",
    "            journal_oid = comment['ancestorId']  # ancestorType is never guestbook; we seemingly don't have any of the guestbook comment data\n",
    "            site_id = utils.extract_long(comment['siteId'])\n",
    "            is_site_valid = site_id in valid_site_ids\n",
    "            user_id = utils.extract_long(comment['userId'])\n",
    "            created_at = dates.get_date_from_json_value(comment['createdAt'])\n",
    "            updated_at = dates.get_date_from_json_value(comment['updatedAt'])\n",
    "            \n",
    "            if 'amps' in comment and type(comment['amps']) == list:\n",
    "                # we write out any amps as separate lines\n",
    "                for amp in comment['amps']:\n",
    "                    amp_user_id = utils.extract_long(amp)\n",
    "                    is_user_valid = amp_user_id in valid_user_ids\n",
    "                    if is_user_valid and is_site_valid:\n",
    "                        outfile.write(f\"{amp_user_id},{site_id},amp,{comment_oid}|{amp_user_id},comment,{comment_oid},journal,{journal_oid},{created_at},{updated_at}\\n\")\n",
    "                        both_valid_count += 1\n",
    "                    elif is_user_valid and not is_site_valid:\n",
    "                        author_valid_count += 1\n",
    "                    elif not is_user_valid and is_site_valid:\n",
    "                        site_valid_count += 1\n",
    "                    else:\n",
    "                        neither_valid_count += 1\n",
    "            \n",
    "            is_user_valid = user_id in valid_user_ids\n",
    "            if is_user_valid and is_site_valid:\n",
    "                # columns: user_id, site_id, interaction_type, interaction_oid, parent_type, parent_id, ancestor_type, ancestor_id, created_at, updated_at\n",
    "                outfile.write(f\"{user_id},{site_id},comment,{comment_oid},{parent_type},{parent_oid},journal,{journal_oid},{created_at},{updated_at}\\n\")\n",
    "                both_valid_count += 1\n",
    "            elif is_user_valid and not is_site_valid:\n",
    "                author_valid_count += 1\n",
    "            elif not is_user_valid and is_site_valid:\n",
    "                site_valid_count += 1\n",
    "            else:\n",
    "                neither_valid_count += 1\n",
    "both_valid_count, neither_valid_count, author_valid_count, site_valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_filepath = os.path.join(raw_data_dir, 'journal.json')\n",
    "output_filepath = os.path.join(working_dir, \"journal_filtered.csv\")\n",
    "both_valid_count = 0\n",
    "neither_valid_count = 0\n",
    "author_valid_count = 0\n",
    "site_valid_count = 0\n",
    "with open(output_filepath, 'w') as outfile:\n",
    "    with open(journal_filepath, encoding='utf-8') as infile:\n",
    "        for line in tqdm(infile, total=19137078):\n",
    "            journal = json.loads(line)\n",
    "            \n",
    "            if 'amps' not in journal:\n",
    "                continue\n",
    "            amps = journal['amps']\n",
    "            if type(amps) != list:\n",
    "                continue\n",
    "                \n",
    "            journal_oid = journal['_id']['$oid']\n",
    "            site_id = utils.extract_long(journal['siteId'])\n",
    "            is_site_valid = site_id in valid_site_ids\n",
    "            user_id = utils.extract_long(journal['userId'])\n",
    "            \n",
    "            created_at = dates.get_date_from_json_value(journal['createdAt'])\n",
    "            updated_at = dates.get_date_from_json_value(journal['updatedAt'])\n",
    "            \n",
    "            for amp in amps:\n",
    "                amp_user_id = utils.extract_long(amp)\n",
    "                is_user_valid = amp_user_id in valid_user_ids\n",
    "                if is_user_valid and is_site_valid:\n",
    "                    outfile.write(f\"{amp_user_id},{site_id},amp,{journal_oid}|{amp_user_id},journal,{journal_oid},journal,{journal_oid},{created_at},{updated_at}\\n\")\n",
    "                    both_valid_count += 1\n",
    "                elif is_user_valid and not is_site_valid:\n",
    "                    author_valid_count += 1\n",
    "                elif not is_user_valid and is_site_valid:\n",
    "                    site_valid_count += 1\n",
    "                else:\n",
    "                    neither_valid_count += 1\n",
    "            \n",
    "            #is_user_valid = user_id in valid_user_ids\n",
    "            #if is_user_valid and is_site_valid:\n",
    "            #    # columns: user_id, site_id, interaction_type, interaction_oid, parent_type, parent_id, ancestor_type, ancestor_id, created_at, updated_at\n",
    "            #    outfile.write(f\"{user_id},{site_id},journal,{journal_oid},None,None,None,None,{created_at},{updated_at}\\n\")\n",
    "            #    both_valid_count += 1\n",
    "            #elif is_user_valid and not is_site_valid:\n",
    "            #    author_valid_count += 1\n",
    "            #elif not is_user_valid and is_site_valid:\n",
    "            #    site_valid_count += 1\n",
    "            #else:\n",
    "            #    neither_valid_count += 1\n",
    "both_valid_count, neither_valid_count, author_valid_count, site_valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user_id', 'site_id', 'interaction_type', 'interaction_oid', 'parent_type', 'parent_oid', 'ancestor_type', 'ancestor_oid', 'created_at', 'updated_at']\n",
    "s = datetime.now()\n",
    "gb_filepath = os.path.join(working_dir, \"guestbook_filtered.csv\")\n",
    "gb_df = pd.read_csv(gb_filepath, header=None, names=cols)\n",
    "print(datetime.now() - s)\n",
    "\n",
    "s = datetime.now()\n",
    "comment_filepath = os.path.join(working_dir, \"comment_filtered.csv\")\n",
    "comment_df = pd.read_csv(comment_filepath, header=None, names=cols)\n",
    "print(datetime.now() - s)\n",
    "\n",
    "s = datetime.now()\n",
    "journal_filepath = os.path.join(working_dir, \"journal_filtered.csv\")\n",
    "journal_df = pd.read_csv(journal_filepath, header=None, names=cols)\n",
    "print(datetime.now() - s)\n",
    "\n",
    "len(gb_df), len(comment_df), len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df = pd.concat([gb_df, comment_df, journal_df], sort=False)\n",
    "ints_df.reset_index(drop=True, inplace=True)\n",
    "print(len(ints_df))\n",
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "ints_df = ints_df.sort_values(by='created_at')\n",
    "print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ints_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "ints_df.reset_index(drop=True).to_feather(os.path.join(working_dir, 'ints_df.feather'))\n",
    "print(datetime.now() - s)\n",
    "s = datetime.now()\n",
    "ints_df.to_csv(os.path.join(working_dir, 'ints_df.csv'), index=False)\n",
    "print(datetime.now() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read interactions dataframe\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "ints_df = pd.read_feather(os.path.join(model_data_dir, 'ints_df.feather'))\n",
    "print(f\"Read {len(ints_df)} rows ({len(set(ints_df.user_id))} unique users) in {datetime.now() - s}.\")\n",
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_df[['interaction_type', 'parent_type', 'ancestor_type']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2005\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").replace(tzinfo=pytz.UTC).timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'amp'].created_at / 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Amps')\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'guestbook'].created_at / 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks')\n",
    "total_counts, bin_edges = np.histogram(ints_df[ints_df.interaction_type == 'comment'].created_at / 1000, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Comments')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.axvline(datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "plt.axvline(datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Interactions per month\")\n",
    "plt.title(f\"{len(ints_df):,} interactions from {len(set(ints_df.user_id)):,} unique users on {len(set(ints_df.site_id)):,} unique sites\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").replace(tzinfo=pytz.UTC).timestamp() for i in range((2020 - 2005) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timestamp = datetime.fromisoformat(f\"2014-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "end_timestamp = datetime.fromisoformat(f\"2019-01-01\").replace(tzinfo=pytz.UTC).timestamp() * 1000\n",
    "sdf = ints_df[(ints_df.created_at >= start_timestamp)&(ints_df.created_at <= end_timestamp)]\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf[['interaction_type', 'parent_type', 'ancestor_type']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal dataframe with the index\n",
    "# this is all the new journal data\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.df\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(journal_df.created_at.quantile(0.0001) / 1000).isoformat(),\\\n",
    "datetime.utcfromtimestamp(journal_df.created_at.quantile(0.999999) / 1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# journal updates over time, by month\n",
    "\n",
    "start_date = \"2002-04-01\"\n",
    "end_date = \"2019-03-01\"\n",
    "sdate = datetime.fromisoformat(start_date)\n",
    "edate = datetime.fromisoformat(end_date)\n",
    "delta = edate - sdate\n",
    "delta = relativedelta(edate, sdate)\n",
    "bins = []\n",
    "for i in range((delta.years*12) + delta.months + 1):\n",
    "    day = sdate + relativedelta(months=i) #timedelta(months=i)\n",
    "    bins.append(day.timestamp())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "total_counts, bin_edges = np.histogram(journal_df.created_at / 1000, bins=bins)\n",
    "ax.axhline(0, color='gray', alpha=0.4)\n",
    "ax.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2)\n",
    "\n",
    "# 5 year analysis period of relative normality, 2014-2019\n",
    "ax.axvline(datetime.fromisoformat(\"2014-01-01\").timestamp(), color='gray', linestyle='--', alpha=0.4)\n",
    "ax.axvline(datetime.fromisoformat(\"2019-01-01\").timestamp(), color='gray', linestyle='--', alpha=0.4)\n",
    "\n",
    "\n",
    "use_autoloc = True\n",
    "locs = bins\n",
    "if use_autoloc:\n",
    "    locs = ax.get_xticks()\n",
    "labels = []\n",
    "for xtick in locs:\n",
    "    label = f\"{datetime.utcfromtimestamp(xtick).strftime('%b%y')}\"\n",
    "    labels.append(label)\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing createdAt of guestbooks\n",
    "\n",
    "`new_guestbook_createdAt.txt` created via `cut -f4 -d, new_guestbook_metadata_raw.csv > new_guestbook_createdAt.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr = np.zeros(82854708)\n",
    "with open(os.path.join(working_dir, \"new_guestbook_createdAt.txt\"), 'r') as infile:\n",
    "    error_count = 0\n",
    "    for i, line in tqdm(enumerate(infile), total=82854708):\n",
    "        try:\n",
    "            ca_arr[i] = int(line.strip())\n",
    "        except:\n",
    "            error_count += 1\n",
    "            continue\n",
    "error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr = ca_arr / 1000\n",
    "ca_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(ca_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ca_arr.shape)\n",
    "ca_arr = ca_arr[ca_arr > 0]\n",
    "print(ca_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr_old = np.zeros(82980359)\n",
    "with open(os.path.join(working_dir, \"old_guestbook_createdAt.txt\"), 'r') as infile:\n",
    "    error_count = 0\n",
    "    for i, line in tqdm(enumerate(infile), total=82854708):\n",
    "        try:\n",
    "            ca_arr_old[i] = int(line.strip())\n",
    "        except:\n",
    "            error_count += 1\n",
    "            continue\n",
    "error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_arr_old = ca_arr_old / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2005\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2019 data)')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr_old, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2016 data)')\n",
    "\n",
    "plt.axvline(datetime.fromisoformat(f\"2016-06-01\").timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Guestbook count\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").timestamp() for i in range((2020 - 2005) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "     \n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.margins(0,0)\n",
    "#plt.savefig(os.path.join(figures_dir, 'initiation_types_timeline.pdf'), dpi=200, pad_inches=0)\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2005\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr, bins=bins)\n",
    "total_counts_old, bin_edges = np.histogram(ca_arr_old, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts - total_counts_old, linestyle='-', linewidth=2, label='Guestbooks (2019 - 2016 data)')\n",
    "\n",
    "plt.axvline(datetime.fromisoformat(f\"2016-06-01\").timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Guestbook count\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2005 + i}-01-01\").timestamp() for i in range((2020 - 2005) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "     \n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.margins(0,0)\n",
    "#plt.savefig(os.path.join(figures_dir, 'initiation_types_timeline.pdf'), dpi=200, pad_inches=0)\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "bins = []\n",
    "year = 2016\n",
    "month = 0\n",
    "while year != 2020:\n",
    "    if month == 12:\n",
    "        year += 1\n",
    "        month = 1\n",
    "    else:\n",
    "        month += 1\n",
    "    bins.append(datetime.fromisoformat(f\"{year}-{month:02}-01\").timestamp())\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2019 data)')\n",
    "\n",
    "total_counts, bin_edges = np.histogram(ca_arr_old, bins=bins)\n",
    "plt.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2, label='Guestbooks (2016 data)')\n",
    "\n",
    "plt.axvline(datetime.fromisoformat(f\"2016-06-01\").timestamp(), color='black', alpha=0.8, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.ylabel(\"Guestbook count\")\n",
    "\n",
    "newline = '\\n'\n",
    "xticks = [datetime.fromisoformat(f\"{2016 + i}-01-01\").timestamp() for i in range((2020 - 2016) + 2)]\n",
    "plt.xticks(\n",
    "    xticks, \n",
    "    [f\"{datetime.utcfromtimestamp(be).strftime('%Y')}\" for i, be in enumerate(xticks)])\n",
    "     \n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.margins(0,0)\n",
    "#plt.savefig(os.path.join(figures_dir, 'initiation_types_timeline.pdf'), dpi=200, pad_inches=0)\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO look for match on guestbook_oid, site_id, and created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
