{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Journal Text Feature Extraction\n",
    "===\n",
    "\n",
    "Use sklearn to train an author type classification model.\n",
    "\n",
    "Then, save the model to pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "from html.parser import HTMLParser\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import ftfy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/lana/levon003/repos/qual-health-journeys/annotation_data\")\n",
    "import journal as journal_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_working_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data/text\"\n",
    "vw_working_dir = os.path.join(general_working_dir, \"vw\")\n",
    "os.makedirs(vw_working_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "import cbrec.genconfig\n",
    "import cbrec.featuredb\n",
    "import cbrec.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cbrec.genconfig.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = transformers.RobertaModel.from_pretrained('roberta-base')\n",
    "roberta_model.eval()\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sentence.\"\n",
    "tokenized = roberta_tokenizer(text, \n",
    "                padding=False,\n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\")\n",
    "input_ids, attention_mask = tokenized['input_ids'], tokenized['attention_mask']\n",
    "\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = roberta_model(input_ids, attention_mask=attention_mask)\n",
    "    lhs = outputs['last_hidden_state'].numpy()\n",
    "    #cls = cls_states.numpy()\n",
    "last_hidden_state = lhs[0,:,:]\n",
    "mean_pool = np.mean(last_hidden_state, axis=0)\n",
    "text_feature_arr = mean_pool.astype(np.float32)\n",
    "text_feature_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res = roberta_model(input_ids, attention_mask=attention_mask)\n",
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal dataframe\n",
    "s = datetime.now()\n",
    "journal_metadata_filepath = os.path.join(config.journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(f\"Read {len(journal_df)} journal_df rows in {datetime.now() - s}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_oids = []\n",
    "with open(os.path.join(config.model_data_dir, 'required_journal_oids.txt'), 'r') as infile:\n",
    "    for line in tqdm(infile):\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "        journal_oids.append(line.strip())\n",
    "len(journal_oids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO filter journal_df and verify that all journal texts are in the db\n",
    "# TODO train the author type classifier\n",
    "# TODO instantiate the roberta model\n",
    "# TODO create a text feature db to store the outputs\n",
    "# TODO write harness to iterate through journals, compute, and store\n",
    "# Note: This is fundamentally parallelizable, with the one problem being combining the outputs into a database. Should probably write a script to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = cbrec.text.TextDatabase(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for journal_oid in tqdm(journal_oids):\n",
    "    text = td.get_clean_journal_text(journal_oid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cached_results = False  # set to true to load cached intermediate results from pickle rather than redoing the computations\n",
    "\n",
    "# Caching option specifically for instrumental support, since that's more intensive computationally\n",
    "use_cached_instrumental_support_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text_from_token_list(token_list):\n",
    "    cleaned_text = \" \".join(token_list).replace(':', 'COLON').replace('|', 'PIPE').replace(\"\\n\", \"NEWLINE \")\n",
    "    return cleaned_text\n",
    "\n",
    "def get_cleaned_text(text, lowercase=True):\n",
    "    tokens = text.split()\n",
    "    if lowercase:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    cleaned_text = get_cleaned_text_from_token_list(tokens)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load annotation client author types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_web_client_database = \"/home/lana/shared/caringbridge/data/projects/qual-health-journeys/instance/cbAnnotator.sqlite\"\n",
    "\n",
    "\n",
    "def get_annotation_db():\n",
    "    db = sqlite3.connect(\n",
    "            annotation_web_client_database,\n",
    "            detect_types=sqlite3.PARSE_DECLTYPES\n",
    "        )\n",
    "    db.row_factory = sqlite3.Row\n",
    "    return db\n",
    "\n",
    "\n",
    "def get_author_annotations():\n",
    "    try:\n",
    "        db = get_annotation_db()\n",
    "        cursor = db.execute(\n",
    "            \"\"\"SELECT site_id, journal_oid, username, data \n",
    "                FROM journalAnnotation\n",
    "                WHERE annotation_type = \"journal_author_type\"\n",
    "                GROUP BY site_id, journal_oid, username\n",
    "                ORDER BY id DESC\"\"\")\n",
    "        journal_author_annotations = cursor.fetchall()\n",
    "        annotation_strings = [{'site_id': a['site_id'], \n",
    "                               'journal_oid': a['journal_oid'], \n",
    "                               'username': a['username'],\n",
    "                               'data': a['data']}\n",
    "                              for a in journal_author_annotations if a['journal_oid'] != 'site']\n",
    "        return annotation_strings\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "# Test extraction of annotations\n",
    "get_author_annotations()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_client_annotations_raw = get_author_annotations()\n",
    "Counter([a['data'] for a in web_client_annotations_raw]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(web_client_annotations_raw)\n",
    "\n",
    "skip_count = 0\n",
    "arbitrary_selection_count = 0\n",
    "levon003_preferred_selection_count = 0\n",
    "\n",
    "conflict_resolved_author_type_annotations = []\n",
    "for key, group in df.groupby(by=['site_id', 'journal_oid'], sort=False):\n",
    "    if len(group) == 1:\n",
    "        row = group.iloc[0]\n",
    "        entry = {'site_id': row['site_id'], \n",
    "           'journal_oid': row['journal_oid'], \n",
    "           'data': row['data']}\n",
    "        conflict_resolved_author_type_annotations.append(entry)\n",
    "    else:\n",
    "        assert len(group) > 1\n",
    "        group_data = set(group.data)\n",
    "        if len(group_data) == 1:\n",
    "            # multiple annotators, but they all agree\n",
    "            row = group.iloc[0]\n",
    "            entry = {'site_id': row['site_id'], \n",
    "               'journal_oid': row['journal_oid'], \n",
    "               'data': row['data']}\n",
    "            conflict_resolved_author_type_annotations.append(entry)\n",
    "        else:\n",
    "            # multiple annotators, and there is some disagreement\n",
    "            data, count = Counter(group_data).most_common()[0]\n",
    "            data = str(data)\n",
    "            assert \"memory\" not in data\n",
    "            if count == 1:\n",
    "                if \"p\" in group_data and \"cg\" in group_data:\n",
    "                    # we ignore obviously confused situations\n",
    "                    skip_count += 1\n",
    "                    continue\n",
    "                if \"levon003\" in set(group.username):\n",
    "                    # prefer levon003's annotations...\n",
    "                    levon003_preferred_selection_count += 1\n",
    "                    data = str(group[group.username == 'levon003'].iloc[0]['data'])\n",
    "                else:  # make an arbitrary choice\n",
    "                    arbitrary_selection_count += 1\n",
    "                    site_id = group.iloc[0].site_id\n",
    "                    journal_oid = group.iloc[0].journal_oid\n",
    "                    print(f\"127.0.0.1:5000/siteId/{site_id}#{journal_oid}\")\n",
    "            row = group.iloc[0]\n",
    "            entry = {'site_id': row['site_id'], \n",
    "               'journal_oid': row['journal_oid'], \n",
    "               'data': data}\n",
    "            conflict_resolved_author_type_annotations.append(entry)\n",
    "print(f\"Made {arbitrary_selection_count} arbitrary selections in situations of conflict.\")\n",
    "print(f\"Preferred levon003's annotations in {levon003_preferred_selection_count} situations of conflict.\")\n",
    "print(f\"Skipped {skip_count} obviously confused situations.\")\n",
    "len(conflict_resolved_author_type_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([t['data'] for t in conflict_resolved_author_type_annotations]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreement analysis\n",
    "from collections import defaultdict\n",
    "\n",
    "df = pd.DataFrame(web_client_annotations_raw)\n",
    "\n",
    "levon003 = []\n",
    "luoxx498 = []  # NOTE: the actual username is set by luoxx498_un\n",
    "luoxx498_un = \"mill6273\"  # Hannah Miller Hillberg\n",
    "\n",
    "num_annotators = []\n",
    "annotation_list = []\n",
    "annotator_counts = defaultdict(int)\n",
    "for key, group in df.groupby(by=['site_id', 'journal_oid'], sort=False):\n",
    "    if len(group) == 1:\n",
    "        continue\n",
    "    else:  # this journal has multiple annotators\n",
    "        annotations = [1 if data == 'p' else 0 for data in group.data]\n",
    "        total_p = sum(annotations)\n",
    "        total_not_p = len(annotations) - total_p\n",
    "        annotation_list.append((total_p, total_not_p))\n",
    "        assert len(set(group.username)) == len(group)\n",
    "        num_annotators.append(len(group))\n",
    "        for username, annotation in zip(group.username, annotations):\n",
    "            annotator_counts[username] += 1\n",
    "            \n",
    "        usernames = set(group.username)\n",
    "        \n",
    "        if \"levon003\" in usernames and luoxx498_un in usernames:\n",
    "            assert len(group[group.username == \"levon003\"]) == 1\n",
    "            assert len(group[group.username == luoxx498_un]) == 1\n",
    "            levon003.append(1 if group[group.username == \"levon003\"].iloc[0]['data'] == 'p' else 0)\n",
    "            luoxx498.append(1 if group[group.username == luoxx498_un].iloc[0]['data'] == 'p' else 0)\n",
    "annotator_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "print(len(levon003), len(luoxx498))\n",
    "levon003, luoxx498 = np.array(levon003), np.array(luoxx498)\n",
    "print(np.sum(levon003 == luoxx498) / len(levon003))\n",
    "cohen_kappa_score(levon003, luoxx498)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(num_annotators, bins=range(1, 9), align='left')\n",
    "plt.title(\"Distribution of multiply-annotated updates for author type\")\n",
    "plt.xlabel(\"Number of annotators\")\n",
    "plt.ylabel(\"Update count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_client_annotation_list = []\n",
    "\n",
    "for annotation in tqdm(conflict_resolved_author_type_annotations):\n",
    "    site_id = annotation['site_id']\n",
    "    journal_oid = annotation['journal_oid']\n",
    "    title, body = td.get_raw_journal_text(journal_oid)\n",
    "    if len(body) < 50:\n",
    "        continue\n",
    "        \n",
    "    label = annotation['data']\n",
    "    # One consideration: do we want to include unknown journals in the training data?\n",
    "    #if label == \"unk\":\n",
    "    #    continue\n",
    "        \n",
    "    cleaned_body = get_cleaned_text(cbrec.text.clean_text(body))\n",
    "    \n",
    "    annotation_dict = {\n",
    "        \"site_id\": annotation['site_id'],\n",
    "        \"journal_oid\": annotation['journal_oid'], \n",
    "        \"cleaned_body\": cleaned_body,\n",
    "        \"label\": label\n",
    "    }\n",
    "    web_client_annotation_list.append(annotation_dict)\n",
    "\n",
    "len(web_client_annotation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_client_annotation_list_pickle_filepath = os.path.join(vw_working_dir, \"web_client_annotations.pkl\")\n",
    "\n",
    "if use_cached_results and os.path.exists(web_client_annotation_list_pickle_filepath):\n",
    "    print(\"Loading from pickled file.\")\n",
    "    with open(web_client_annotation_list_pickle_filepath, 'rb') as f:\n",
    "        web_client_annotation_list = pickle.load(f)\n",
    "else:  # should save the processed list\n",
    "    print(\"Saving to pickled file.\")\n",
    "    #df = pd.DataFrame(web_client_annotation_list)\n",
    "    #df.to_pickle(web_client_annotation_list_pickle_filepath)\n",
    "    with open(web_client_annotation_list_pickle_filepath, 'wb') as f:\n",
    "        pickle.dump(web_client_annotation_list, f)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cleaned_text(cbrec.text.clean_text(td.get_raw_journal_text(\"51bdfaee6ca004ae6400f06c\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_client_annotation_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_client_annotation_df = pd.DataFrame(web_client_annotation_list)\n",
    "len(web_client_annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(web_client_annotation_df['label']).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load instrumental support coding author types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appreciation_coding_filepath = \"/home/srivbane/levon003/repos/instrumental_support/appreciation_coding/collect_coding_results/certain_data/all_certain_data.csv\"\n",
    "df = pd.read_csv(appreciation_coding_filepath)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df['author_type']).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrumental_support_annotation_list = []\n",
    "skipped = 0\n",
    "if not use_cached_instrumental_support_results:\n",
    "    try:\n",
    "        db = get_db()\n",
    "        for i in tqdm(range(len(df))):\n",
    "            row = df.iloc[i]\n",
    "            site_id = int(row['site_id'])\n",
    "\n",
    "            if len(row['body_text']) < 50:\n",
    "                continue\n",
    "            comparison_body = row['body_text'][:500].lower()\n",
    "\n",
    "            cursor = db.execute(\"\"\"SELECT site_id, journal_oid, title, body\n",
    "                                    FROM journal\n",
    "                                    WHERE site_id = ?\n",
    "                                    GROUP BY site_id, journal_oid\n",
    "                                    ORDER BY createdAt\"\"\",\n",
    "                                (site_id,))\n",
    "            current_match = None\n",
    "            for res in cursor.fetchall():\n",
    "                if res['body'] is None or len(res['body']) < 50:\n",
    "                    continue\n",
    "                title = get_cleaned_text(res['title']) if res['title'] is not None else \"\"\n",
    "                body = get_cleaned_text(res['body'][:500-len(title)-1]).lower()\n",
    "\n",
    "                title_and_body = title + \" \" + body\n",
    "                distance = Levenshtein.distance(title_and_body, comparison_body)\n",
    "\n",
    "                if current_match is None or current_match['distance'] >= distance:\n",
    "                    #if current_match is not None and current_match['distance'] == distance:\n",
    "                    #    print(\"Warning: Equivalent match found in site %d.\" % site_id)\n",
    "                    #    print(\"  Old match text:\", current_match['title_body'][:200])\n",
    "                    #    print(\"  New match text:\", title_and_body[:200])\n",
    "                    current_match = {\n",
    "                        \"journal_oid\": res['journal_oid'],\n",
    "                        \"title_body\": title_and_body[:300],\n",
    "                        \"body\": get_cleaned_text(res['body']),\n",
    "                        \"distance\": distance\n",
    "                    }\n",
    "                    if distance <= 4:  # just assume that this is an exact match; no need to look at the other journals\n",
    "                        break\n",
    "            # print out particularly large differences for manual inspection\n",
    "            if current_match['distance'] > 220:\n",
    "                print(site_id, current_match['journal_oid'], current_match['distance'])\n",
    "                print(\"Matched Journal:\", current_match['title_body'][:200])\n",
    "                print(\"Dataframe Input:\", comparison_body[:200])\n",
    "                skipped += 1\n",
    "                continue  # discard this record, not including it in the list of annotations.\n",
    "\n",
    "            match_dict = {\n",
    "                \"site_id\": site_id,\n",
    "                \"journal_oid\": current_match['journal_oid'], \n",
    "                \"cleaned_body\": current_match['body'],\n",
    "                \"label\": row['author_type']\n",
    "            }\n",
    "            instrumental_support_annotation_list.append(match_dict)\n",
    "    finally:\n",
    "        db.close()\n",
    "    \n",
    "len(instrumental_support_annotation_list), skipped  # Note: 10 or so records are skipped during computation due to bad matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrumental_support_annotation_list_pickle_filepath = os.path.join(vw_working_dir, \"instrumental_support_annotations.pkl\")\n",
    "\n",
    "if (use_cached_results or use_cached_instrumental_support_results) and os.path.exists(instrumental_support_annotation_list_pickle_filepath):\n",
    "    print(\"Loaded from file.\")\n",
    "    with open(instrumental_support_annotation_list_pickle_filepath, 'rb') as f:\n",
    "        instrumental_support_annotation_list = pickle.load(f)\n",
    "else:  # should save the processed list\n",
    "    with open(instrumental_support_annotation_list_pickle_filepath, 'wb') as f:\n",
    "        pickle.dump(instrumental_support_annotation_list, f)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the annotation sources into a common format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = web_client_annotation_list[:]\n",
    "should_merge = False  # this flag determines if the instrumental support annotations are incorporated\n",
    "if should_merge:\n",
    "    for instr_annotation in instrumental_support_annotation_list:\n",
    "        annotation_already_present = False\n",
    "        for web_annotation in web_client_annotation_list:\n",
    "            if instr_annotation['site_id'] == web_annotation['site_id'] \\\n",
    "                and instr_annotation['journal_oid'] == web_annotation['journal_oid']:\n",
    "                    annotation_already_present = True  # we prefer the web client annotations\n",
    "                    print(\"Duplicate:\", instr_annotation['label'], web_annotation['label'])\n",
    "        if not annotation_already_present:\n",
    "            annotations.append(instr_annotation)\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df = pd.DataFrame(annotations)\n",
    "annotation_df.drop_duplicates(subset=[\"site_id\", \"journal_oid\"], keep='first', inplace=True)\n",
    "len(annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in annotation_df.groupby(by=[\"site_id\", \"journal_oid\"]):\n",
    "    if len(group) > 1:\n",
    "        print(group)\n",
    "        print()\n",
    "        assert False, \"Unexpected duplicate.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all journal info and subset for sna-social-support project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/srivbane/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.df\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of valid users\n",
    "data_selection_working_dir = \"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/data_selection\"\n",
    "valid_user_ids = set()\n",
    "with open(os.path.join(data_selection_working_dir, \"valid_user_ids.txt\"), 'r') as infile:\n",
    "    for line in infile:\n",
    "        user_id = line.strip()\n",
    "        if user_id == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            valid_user_ids.add(int(user_id))\n",
    "len(valid_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of valid users\n",
    "data_selection_working_dir = \"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/data_selection\"\n",
    "valid_site_ids = set()\n",
    "with open(os.path.join(data_selection_working_dir, \"valid_site_ids.txt\"), 'r') as infile:\n",
    "    for line in infile:\n",
    "        site_id = line.strip()\n",
    "        if site_id == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            valid_site_ids.add(int(site_id))\n",
    "len(valid_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most journals are already authored by valid users\n",
    "np.sum(journal_df.user_id.isin(valid_user_ids)) / len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((journal_df.user_id.isin(valid_user_ids))|(journal_df.site_id.isin(valid_site_ids))) / len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we will restrict to only the valid users and journals they've authored\n",
    "# as well as to valid sites on which the valid users have authored\n",
    "original_length = len(journal_df)\n",
    "journal_df = journal_df[(journal_df.user_id.isin(valid_user_ids))|(journal_df.site_id.isin(valid_site_ids))]\n",
    "len(journal_df), original_length, f\"{len(journal_df) / original_length * 100 :.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_journal_df = journal_df[journal_df.is_nontrivial]\n",
    "len(unlabeled_journal_df), len(journal_df), f\"{len(unlabeled_journal_df) / len(journal_df) * 100 :.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a copy of the given journal_df with an added column with the journal update's text\n",
    "# journal_df should be a dataframe with a site_id and journal_oid column\n",
    "# a 'cleaned_body' column is added\n",
    "def add_body_text(journal_df):\n",
    "    journal_df = journal_df.copy()\n",
    "    bodies = []\n",
    "    for site_id, journal_oid in zip(journal_df.site_id, journal_df.journal_oid):\n",
    "        body = get_journal_text(site_id, journal_oid)\n",
    "        if body is None or len(body) < 50:\n",
    "            cleaned_body = \"\"\n",
    "        else:\n",
    "            cleaned_body = get_cleaned_text(body)\n",
    "        bodies.append(cleaned_body)\n",
    "    journal_df['cleaned_body'] = bodies\n",
    "    return journal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_body_text(journal_df.sample(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train and evaluate a few non-VW models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,2), min_df=3)),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', sklearn.linear_model.SGDClassifier(loss='log', alpha=0.001, tol=1e-3, max_iter=1000) ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_ids = set(annotation_df.site_id)\n",
    "len(site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_journal_df_subset = unlabeled_journal_df.sample(n=5000)\n",
    "unlabeled_journal_df_subset = add_body_text(unlabeled_journal_df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df['human_label'] = annotation_df['label'].map(lambda label: 'p' if label == 'p' else 'cg')\n",
    "annotation_df['pre_bbse_predicted_label'] = \"\"\n",
    "annotation_df['bbse_proportion_p'] = np.nan\n",
    "annotation_df['predicted_label'] = \"\"\n",
    "for site_id in tqdm(site_ids):\n",
    "    test = annotation_df[annotation_df.site_id == site_id]\n",
    "    \n",
    "    # eligible training data is all sites other than the held-out site\n",
    "    train = annotation_df[annotation_df.site_id != site_id]\n",
    "    \n",
    "    cg_idx = train[train.human_label == 'cg'].index\n",
    "    cg_total = len(cg_idx)\n",
    "    \n",
    "    p_idx = train[train.human_label == 'p'].sample(n=cg_total).index\n",
    "    p_total = len(p_idx)\n",
    "    \n",
    "    train_idx = cg_idx.union(p_idx)\n",
    "    assert len(train_idx) == cg_total + p_total\n",
    "    \n",
    "    # retrieve the subset of the eligible training data that will be used for estimation\n",
    "    train_subset = train.loc[train_idx]\n",
    "    \n",
    "    # within the labeled data eligible for training, create a train/validation split\n",
    "    train_subset = train_subset.sample(frac=1)\n",
    "    labeled_train_pct = 0.8\n",
    "    labeled_train_index = int(len(train_subset) * 0.8)\n",
    "    labeled_train = train_subset.iloc[:labeled_train_index]\n",
    "    labeled_valid = train_subset.iloc[labeled_train_index:]\n",
    "    \n",
    "    # train the model and predict on the validation data\n",
    "    md = text_clf.fit(labeled_train.cleaned_body, labeled_train.human_label)\n",
    "    labeled_valid_preds = md.predict(labeled_valid.cleaned_body)\n",
    "    \n",
    "    # predict on the test data pre-BBSC so we have a point of comparison\n",
    "    predicted = md.predict(test.cleaned_body)\n",
    "    annotation_df.loc[test.index, 'pre_bbse_predicted_label'] = predicted\n",
    "    \n",
    "    # computation the proportion of the two classes in the training dataset\n",
    "    # by design, this should be 50/50\n",
    "    labeled_p = np.sum(train_subset.human_label == 'p') / len(train_subset)\n",
    "    labeled_cg = 1 - labeled_p\n",
    "    v_est = np.array([labeled_p, labeled_cg])\n",
    "    \n",
    "    # C_est is the normalized confusion matrix on the validation data\n",
    "    C_est = np.zeros((2,2))\n",
    "    C_est[0,0] = np.sum((labeled_valid.human_label == 'p') & (labeled_valid_preds == 'p'))\n",
    "    C_est[0,1] = np.sum((labeled_valid.human_label != 'p') & (labeled_valid_preds == 'p'))\n",
    "    C_est[1,0] = np.sum((labeled_valid.human_label == 'p') & (labeled_valid_preds != 'p'))\n",
    "    C_est[1,1] = np.sum((labeled_valid.human_label != 'p') & (labeled_valid_preds != 'p'))\n",
    "    C_est = C_est / len(labeled_valid)  # normalize by dividing by the sample size of labeled validation data\n",
    "    \n",
    "    # Retrieve a sample of unlabeled data\n",
    "    journal_df_subset = unlabeled_journal_df_subset  # use the same unlabeled set for all iterations\n",
    "    #journal_df_subset = unlabeled_journal_df.sample(n=500)\n",
    "    #journal_df_subset = add_body_text(journal_df_subset)\n",
    "    \n",
    "    # Predict on the unlabeled data\n",
    "    unlabeled_preds = md.predict(journal_df_subset.cleaned_body)\n",
    "    \n",
    "    # compute estimator for true percentages in shifted distribution\n",
    "    target_predicted_p = np.sum(unlabeled_preds == 'p') / len(unlabeled_preds)\n",
    "    target_predicted_cg = 1 - target_predicted_p\n",
    "    mu_pred_est = np.array([target_predicted_p, target_predicted_cg])\n",
    "    w_est = np.matmul(np.linalg.inv(C_est), mu_pred_est)\n",
    "    mu_est = np.matmul(np.diag(v_est), w_est)\n",
    "    estimated_proportion_p = mu_est[0]  # the estimated proportion of patient updates in the target distribution\n",
    "    annotation_df.loc[test.index, 'bbse_proportion_p'] = estimated_proportion_p\n",
    "    \n",
    "    \n",
    "    # Black Box Shift Correction\n",
    "    # use w_est to produce a corrected classifier\n",
    "    # we train a new logistic regression classifier to solve the importance-weighted ERM problem\n",
    "    w_est_nn = w_est.clip(0)  # w_est_nn is the non-negative version of w_est, clipping class weights to 0\n",
    "    class_weights = {'p': w_est_nn[0], 'cg': w_est_nn[1]}\n",
    "    bbsc_clf = Pipeline([\n",
    "         ('vect', CountVectorizer(ngram_range=(1,2), min_df=3)),\n",
    "         ('tfidf', TfidfTransformer()),\n",
    "         ('bbsc_clf', sklearn.linear_model.LogisticRegression(solver='lbfgs', penalty='l2', class_weight=class_weights) ),\n",
    "    ])\n",
    "    bbsc_clf.fit(labeled_train.cleaned_body, labeled_train.human_label)\n",
    "    bbsc_prediction = bbsc_clf.predict(test.cleaned_body)\n",
    "    annotation_df.loc[test.index, 'predicted_label'] = bbsc_prediction\n",
    "        \n",
    "    # train the model and make predictions on the held-out test data\n",
    "    #md = text_clf.fit(train_subset.cleaned_body, train_subset.human_label)\n",
    "    #predicted = md.predict(test.cleaned_body)\n",
    "    #annotation_df.loc[test.index, 'predicted_label'] = predicted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_proportions = annotation_df.drop_duplicates(subset=['site_id',]).bbse_proportion_p\n",
    "#print(np.mean(patient_proportions), np.std(patient_proportions))\n",
    "print(f\"{np.mean(patient_proportions) * 100:.2f}%+-{np.std(patient_proportions)/np.sqrt(len(patient_proportions))*100:.2f}% of journal updates are predicted patient-authored (BBSE used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the estimate of the patient proportions from the unlabeled data\n",
    "# this estimate is based on a sample of 5000 journal updates from the unlabeled data\n",
    "# it should be (and is) lower than the BBSE estimate\n",
    "patient_proportions = annotation_df.drop_duplicates(subset=['site_id',]).bbse_proportion_p\n",
    "plt.hist(patient_proportions, bins=np.linspace(0, 1, num=100))\n",
    "plt.axvline(np.mean(patient_proportions), color='black', linestyle='--')\n",
    "print(np.mean(patient_proportions))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df['post_bbse_predicted_label'] = annotation_df.predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df.predicted_label = annotation_df.post_bbse_predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,2), min_df=3)),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', sklearn.linear_model.SGDClassifier(loss='hinge', alpha=0.001, tol=1e-3, max_iter=1000) ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df['human_label'] = annotation_df['label'].map(lambda label: 'p' if label == 'p' else 'cg')\n",
    "annotation_df['unlabeled_predicted_p'] = \"\"\n",
    "annotation_df['predicted_label'] = \"\"\n",
    "for site_id in tqdm(site_ids):\n",
    "    test = annotation_df[annotation_df.site_id == site_id]\n",
    "    \n",
    "    # eligible training data is all sites other than the held-out site\n",
    "    train = annotation_df[annotation_df.site_id != site_id]\n",
    "    \n",
    "    cg_idx = train[train.human_label == 'cg'].index\n",
    "    cg_total = len(cg_idx)\n",
    "    \n",
    "    p_idx = train[train.human_label == 'p'].sample(n=cg_total).index\n",
    "    p_total = len(p_idx)\n",
    "    \n",
    "    train_idx = cg_idx.union(p_idx)\n",
    "    assert len(train_idx) == cg_total + p_total\n",
    "    \n",
    "    # retrieve the subset of the eligible training data that will be used for estimation\n",
    "    train_subset = train.loc[train_idx]\n",
    "        \n",
    "    # train the model and make predictions on the held-out test data\n",
    "    md = text_clf.fit(train_subset.cleaned_body, train_subset.human_label)\n",
    "    predicted = md.predict(test.cleaned_body)\n",
    "    annotation_df.loc[test.index, 'predicted_label'] = predicted\n",
    "    \n",
    "    # estimate the empirical proportion of P/CG posts on an unlabeled data sample\n",
    "    unlabeled_preds = md.predict(unlabeled_journal_df_subset.cleaned_body)\n",
    "    target_predicted_p = np.sum(unlabeled_preds == 'p') / len(unlabeled_preds)\n",
    "    annotation_df.loc[test.index, 'unlabeled_predicted_p'] = target_predicted_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the estimate of the patient proportions from the unlabeled data\n",
    "patient_proportions = annotation_df.drop_duplicates(subset=['site_id',]).unlabeled_predicted_p\n",
    "print(f\"{np.mean(patient_proportions) * 100:.2f}%+-{np.std(patient_proportions)/np.sqrt(len(patient_proportions))*100:.2f}% of journal updates are predicted patient-authored (BBSE not used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(patient_proportions, bins=np.linspace(0, 1, num=100))\n",
    "plt.axvline(np.mean(patient_proportions), color='black', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(annotation_df.predicted_label == \"\")\n",
    "Counter(annotation_df.predicted_label).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.sum(annotation_df.human_label == annotation_df.predicted_label) / len(annotation_df)\n",
    "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
    "print()\n",
    "print(sklearn.metrics.classification_report(annotation_df.human_label, annotation_df.predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute site-level accuracy\n",
    "sites = []\n",
    "for site_id, group in tqdm(annotation_df.groupby('site_id')):\n",
    "    num_correct = np.sum(group.label == group.predicted_label)\n",
    "    site = {\n",
    "        'site_id': site_id,\n",
    "        'total_count': len(group),\n",
    "        'correct_count': num_correct,\n",
    "        'accuracy': num_correct / len(group)\n",
    "    }\n",
    "    sites.append(site)\n",
    "site_df = pd.DataFrame(sites)\n",
    "len(site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, num=20)\n",
    "plt.hist(site_df.accuracy, bins=bins, label='All sites with 1+ annotations')\n",
    "plt.hist(site_df[site_df.total_count >= 5].accuracy, bins=bins, label='Sites with 5+ annotated updates')\n",
    "plt.legend()\n",
    "plt.title(\"Site-level accuracy distribution\")\n",
    "plt.xlabel(\"Site-level accuracy\")\n",
    "plt.ylabel(\"Annotated site count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 56.4% of sites are classified entirely correctly\n",
    "np.sum(site_df.accuracy == 1) / len(site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the percentage of sites that are classified at least 90% correctly,\n",
    "# which suggests that our multiauthor heuristic will misclassify at the site-level at most (1 - this %) of the time\n",
    "proportion_nearly_correct = len(site_df[site_df.accuracy >= 0.9]) / len(site_df)\n",
    "proportion_nearly_correct, 1 - proportion_nearly_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the final sklearn model and apply it to all the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,2), min_df=3)),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', sklearn.linear_model.SGDClassifier(loss='hinge', alpha=0.001, tol=1e-4, max_iter=1000) ),\n",
    "])\n",
    "\n",
    "annotation_df['human_label'] = annotation_df['label'].map(lambda label: 'p' if label == 'p' else 'cg')\n",
    "\n",
    "# eligible training data is all labeled data for the final model\n",
    "train = annotation_df\n",
    "\n",
    "cg_idx = train[train.human_label == 'cg'].index\n",
    "cg_total = len(cg_idx)\n",
    "\n",
    "p_idx = train[train.human_label == 'p'].sample(n=cg_total).index\n",
    "p_total = len(p_idx)\n",
    "\n",
    "train_idx = cg_idx.union(p_idx)\n",
    "assert len(train_idx) == cg_total + p_total\n",
    "\n",
    "# retrieve the subset of the eligible training data that will be used for estimation\n",
    "train_subset = train.loc[train_idx]\n",
    "\n",
    "# train the model and make predictions on the held-out test data\n",
    "clf = text_clf.fit(train_subset.cleaned_body, train_subset.human_label)\n",
    "\n",
    "train_preds = text_clf.predict(train_subset.cleaned_body)\n",
    "acc = np.sum(train_preds == train_subset.human_label) / len(train_subset)\n",
    "print(f\"Train accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the trained model to the unlabled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this flag is true,\n",
    "# then predictions will only be made on not-yet-predicted rows in the journal_df\n",
    "# this might be a good idea if the underlying sample changes, or if a specific subset of the predictions\n",
    "# need to be thrown out and regenerated\n",
    "# We load in the existing label dataframe, merge with the journal_df, and then leave all unlabeled rows blank\n",
    "merge_existing_labels = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one time process\n",
    "if merge_existing_labels:\n",
    "    # read the journal metadata with author type info added\n",
    "    s = datetime.now()\n",
    "    author_type_dir = \"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/author_type\"\n",
    "    journal_metadata_filepath = os.path.join(author_type_dir, \"journal_metadata_with_author_type.df\")\n",
    "    journal_df_with_labels = pd.read_feather(journal_metadata_filepath)\n",
    "    print(datetime.now() - s)\n",
    "    len(journal_df_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df['predicted_label'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_existing_labels:\n",
    "    original_length = len(journal_df)\n",
    "    merged = pd.merge(journal_df, journal_df_with_labels[['site_id', 'journal_oid', 'predicted_label']], on=['site_id', 'journal_oid'], how='left')\n",
    "    merged.loc[merged.predicted_label_y.isna(), 'predicted_label_y'] = \"\"\n",
    "    merged['predicted_label'] = merged.predicted_label_y\n",
    "    merged = merged.drop(columns=['predicted_label_x', 'predicted_label_y'])\n",
    "    journal_df = merged\n",
    "    assert original_length == len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(journal_df.predicted_label).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "num_batches = len(journal_df) // batch_size + 1\n",
    "batch_size, num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this only runs the classifier on rows of journal_df with predicted_label == \"\"\n",
    "for batch_num in tqdm(range(num_batches)):\n",
    "    journal_df_subset = journal_df.iloc[batch_num * batch_size:batch_num * batch_size + batch_size]\n",
    "    journal_df_subset = journal_df_subset[journal_df_subset.predicted_label == \"\"]\n",
    "    if len(journal_df_subset) == 0:\n",
    "        continue\n",
    "    journal_df_subset = add_body_text(journal_df_subset)\n",
    "    preds = clf.predict(journal_df_subset.cleaned_body)\n",
    "    journal_df.loc[journal_df_subset.index, 'predicted_label'] = preds\n",
    "    if batch_num % 50 == 0:\n",
    "        # keep writing a checkpoint file at regular intervals\n",
    "        journal_metadata_filepath = os.path.join(general_working_dir, \"journal_metadata_with_author_type.df.checkpoint\")\n",
    "        journal_df.reset_index(drop=True).to_feather(journal_metadata_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now()\n",
    "journal_metadata_filepath = os.path.join(general_working_dir, \"journal_metadata_with_author_type.df\")\n",
    "journal_df.reset_index(drop=True).to_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of labeling all unlabeled data\n",
    "\n",
    "### Now, some analysis on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the journal metadata with author type info added\n",
    "s = datetime.now()\n",
    "author_type_dir = \"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/author_type\"\n",
    "journal_metadata_filepath = os.path.join(author_type_dir, \"journal_metadata_with_author_type.df\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = journal_df[journal_df.is_nontrivial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(journal_df.predicted_label).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the overall percentage that are patient-authored\n",
    "np.sum(journal_df.predicted_label == 'p') / len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proportions = journal_df.groupby(['user_id', 'site_id']).agg({'predicted_label': lambda group: np.sum(group == 'p') / len(group),\n",
    "                                                                  'site_index': lambda group: len(group)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proportions = user_proportions.rename(columns={'predicted_label': 'patient_proportion', 'site_index': 'site_total'})\n",
    "user_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_proportions.reset_index(level=user_proportions.index.names)\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df[user_df.user_id.isin(valid_user_ids)]\n",
    "len(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO probably split this into TWO designations: shared account/site, mixed site types\n",
    "\n",
    "def compute_author_type(group):\n",
    "    if len(group) == 1:\n",
    "        site = group.iloc[0]\n",
    "        patient_proportion = site.patient_proportion\n",
    "        if patient_proportion >= 0.8:\n",
    "            return 'p'\n",
    "        elif patient_proportion >= 0.2:\n",
    "            return 'shared'\n",
    "        else:\n",
    "            return 'cg'\n",
    "    else:  # multiple sites on which this user has authored!\n",
    "        is_shared = np.any((group.patient_proportion < 0.8)&(group.patient_proportion > 0.2))\n",
    "        if is_shared:\n",
    "            return 'shared'\n",
    "        if np.sum(group.patient_proportion >= 0.8) / len(group) >= 0.5:\n",
    "            return 'p'\n",
    "        else:\n",
    "            return 'cg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_type_series = user_df.groupby('user_id').apply(compute_author_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in Counter(author_type_series).most_common():\n",
    "    print(key, value / len(author_type_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_type_df = author_type_series.rename(\"author_type\").reset_index()\n",
    "author_type_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the author type data\n",
    "author_type_dir = \"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/author_type\"\n",
    "author_type_dataframe_filepath = os.path.join(author_type_dir, 'author_types.df')\n",
    "author_type_df.to_feather(author_type_dataframe_filepath)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the author type data\n",
    "author_type_dir = \"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/author_type\"\n",
    "author_type_dataframe_filepath = os.path.join(author_type_dir, 'author_types.df')\n",
    "author_type_df = pd.read_feather(author_type_dataframe_filepath)\n",
    "len(author_type_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,1,num=100)\n",
    "plt.hist(user_proportions.predicted_label, bins=bins, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.20\n",
    "bin_vals, _ = np.histogram(user_proportions.predicted_label, bins=[0, threshold, 1 - threshold, 1])\n",
    "bin_vals / len(user_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vowpal Wabbit\n",
    "\n",
    "### Convert the data to VW format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lines = []\n",
    "neg_lines = []\n",
    "for i, annotation in annotation_df.iterrows():\n",
    "    label_section = \"-1\"\n",
    "    if annotation['label'] in [\"p\", \"P\"]:\n",
    "        label_section = \"+1\"\n",
    "    \n",
    "    identifier = \"sid%djoid%s\" % (annotation['site_id'], annotation['journal_oid'])\n",
    "    \n",
    "    formatted_journal = label_section + \" \" + identifier + \"|J \" + annotation['cleaned_body']\n",
    "    if label_section == \"+1\":\n",
    "        pos_lines.append(formatted_journal)\n",
    "    else:\n",
    "        neg_lines.append(formatted_journal)\n",
    "\n",
    "pos_lines[0], neg_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree of class imbalance\n",
    "pos_lines = np.array(pos_lines)\n",
    "neg_lines = np.array(neg_lines)\n",
    "len(pos_lines), len(neg_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify a balanced train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersample to create the training dataset\n",
    "max_majority_class_pct = 2  # maximum number of majority class to include, as a percentage of the minority class\n",
    "target_train_pos_lines = int(len(neg_lines) * max_majority_class_pct)\n",
    "pos_included, pos_extra = train_test_split(pos_lines, train_size=target_train_pos_lines)\n",
    "print(\"Pos included / left out:\", len(pos_included), len(pos_extra))\n",
    "\n",
    "holdout_percent = 0.2\n",
    "\n",
    "train_lines, true_val_lines = train_test_split(np.concatenate((pos_included, neg_lines)), test_size=holdout_percent)\n",
    "print(\"True validation:\", len(true_val_lines))\n",
    "\n",
    "#valid_lines = np.concatenate((true_val_lines, pos_extra))\n",
    "valid_lines = true_val_lines\n",
    "\n",
    "holdout_start_index = len(train_lines) + 1\n",
    "holdout_start_index, len(train_lines), len(valid_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_lines = []\n",
    "#valid_lines = []\n",
    "\n",
    "#holdout_percent = 0.2\n",
    "#holdout_start_index = len(lines) - int(len(lines) * holdout_percent)\n",
    "\n",
    "#train_lines = lines[:holdout_start_index]\n",
    "#valid_lines = lines[holdout_start_index:]\n",
    "\n",
    "#for i, annotation in enumerate(annotations):\n",
    "#    annotation['in_training_data'] = i < holdout_start_index\n",
    "\n",
    "#len(train_lines), len(valid_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the train/test splits to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the formatted lines to a file (after another shuffle)\n",
    "random.shuffle(train_lines)\n",
    "random.shuffle(valid_lines)\n",
    "\n",
    "vw_filename = \"author_journal_text_only.txt\"\n",
    "vw_filepath = os.path.join(vw_working_dir, vw_filename)\n",
    "with open(vw_filepath, 'w', encoding=\"utf-8\") as outfile:\n",
    "    for line in train_lines:\n",
    "        outfile.write(line + \"\\n\")\n",
    "    for line in valid_lines:\n",
    "        outfile.write(line + \"\\n\")\n",
    "holdout_after = len(train_lines)\n",
    "print(\"Finished writing VW-formatted file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 {vw_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vw_filename = \"author_journal_text_only.txt\"\n",
    "#vw_filepath = os.path.join(vw_working_dir, vw_filename)\n",
    "#holdout_after = 1917"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the VW model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {vw_working_dir} {vw_filepath} {holdout_after}\n",
    "holdout_after=\"${3}\"\n",
    "echo \"Training on training set (holdout after ${holdout_after} lines).\"\n",
    "train_file=${2}\n",
    "working_dir=${1}\n",
    "model_file=${working_dir}\n",
    "vw --binary -k -c -b 22 -d ${train_file} \\\n",
    "    -f ${working_dir}/patient_authored.model \\\n",
    "    --passes 20 \\\n",
    "    --holdout_after ${holdout_after} \\\n",
    "    --ngram 3 \\\n",
    "    --l2 0.000001 \\\n",
    "    --loss_function logistic\n",
    "\n",
    "# Other options:\n",
    "#    --skips 1 \\\n",
    ">&2 echo\n",
    ">&2 echo\n",
    "\n",
    "echo \"Making predictions.\"\n",
    "vw --binary \\\n",
    "   -t -i ${working_dir}/patient_authored.model \\\n",
    "   -p ${working_dir}/author_journal_text_only.pred \\\n",
    "   -d ${train_file} \\\n",
    "   -r ${working_dir}/author_journal_text_only.pred.raw\n",
    "\n",
    "#echo \"Predicting on test set.\"\n",
    "#vw -t -i data/site_features_multiclass.model -d wd/site_features_multiclass_test.txt -p data/site_features_multiclass_test.pred\n",
    "#echo\n",
    " \n",
    "echo \"Finished.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 /home/srivbane/shared/caringbridge/data/projects/qual-health-journeys/author_classification/vw/author_journal_text_only.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 /home/srivbane/shared/caringbridge/data/projects/qual-health-journeys/author_classification/vw/author_journal_text_only.pred.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the predictions made by VW on the training/validation data as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_predictions_with_weights(pred_filepath, raw_pred_filepath):\n",
    "    \n",
    "    with open(pred_filepath, 'r') as infile:\n",
    "        pred_lines = infile.readlines()\n",
    "    with open(raw_pred_filepath, 'r') as infile:\n",
    "        raw_pred_lines = infile.readlines()\n",
    "        \n",
    "    if len(pred_lines) != len(raw_pred_lines):\n",
    "        raise ValueError(\"Expected predicted and raw files to be the same length.\")\n",
    "    \n",
    "    dtype_dict = {\"site_id\": int, \n",
    "                     \"journal_oid\": str,\n",
    "                     \"raw_prob\": float,\n",
    "                     \"prob\": float,\n",
    "                     \"is_patient\": bool,\n",
    "                     \"is_patient_predicted\": bool,\n",
    "                     \"in_training_data\": bool}\n",
    "    columns = list(dtype_dict.keys())\n",
    "    dtype = [dtype_dict[col] for col in columns]\n",
    "    pred_df = pd.DataFrame(index=range(len(pred_lines)), columns=columns)\n",
    "    \n",
    "    is_train_map = {}\n",
    "    for train_line in train_lines:\n",
    "        tokens = train_line[:70].strip().split(\" \")\n",
    "        label, journal_id = tokens[:2]\n",
    "        journal_id = journal_id.split(\"|\")[0]\n",
    "        is_train_map[journal_id] = True\n",
    "    \n",
    "    is_pos_map = {}\n",
    "    for pos_line in pos_lines:\n",
    "        tokens = pos_line[:70].strip().split(\" \")\n",
    "        label, journal_id = tokens[:2]\n",
    "        journal_id = journal_id.split(\"|\")[0]\n",
    "        is_pos_map[journal_id] = True\n",
    "    \n",
    "    index = 0\n",
    "    for pred_line, raw_pred_line in zip(pred_lines, raw_pred_lines):\n",
    "        prediction, journal_id = pred_line.strip().split(\" \")\n",
    "        site_id, journal_oid = journal_id.split(\"joid\")\n",
    "        site_id = int(site_id[3:])\n",
    "        \n",
    "        pred_df.iloc[index][\"site_id\"] = site_id\n",
    "        pred_df.iloc[index][\"journal_oid\"] = journal_oid\n",
    "        pred_df.iloc[index][\"is_patient_predicted\"] = prediction == \"1\"\n",
    "        \n",
    "        pred_df.iloc[index][\"is_patient\"] = journal_id in is_pos_map\n",
    "        pred_df.iloc[index][\"in_training_data\"] = journal_id in is_train_map\n",
    "        \n",
    "        raw_prob, raw_journal_id = raw_pred_line.strip().split(\" \")\n",
    "        assert journal_id == raw_journal_id\n",
    "        pred_df.iloc[index][\"raw_prob\"] = float(raw_prob)\n",
    "        # if using a logistic loss function, we can recover the \"probability\" by applying the logistic function\n",
    "        pred_df.iloc[index][\"prob\"] = 1 / (1 + np.exp(-1 * float(raw_prob) ))\n",
    "        \n",
    "        index += 1\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = get_phase_predictions_with_weights(os.path.join(vw_working_dir, \"author_journal_text_only.pred\"),\n",
    "                                  os.path.join(vw_working_dir, \"author_journal_text_only.pred.raw\"))\n",
    "\n",
    "print(pred_df.dtypes)  # datatypes just stay object for now...\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the contents of the merged data\n",
    "np.sum(pred_df.is_patient) == len(pos_included), np.sum(pred_df.in_training_data) == len(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pred_df.is_patient) == len(pos_lines), len(pos_lines), np.sum(pred_df.is_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(site_id, journal_oid, port=5000):\n",
    "    url = \"http://127.0.0.1:%d/siteId/%d#%s\" % (port, site_id, journal_oid)\n",
    "    return '<a href=\"{}\">{}</a>'.format(url, url)\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)  # allow the entirety of the url to show by removing column width limits\n",
    "\n",
    "pred_df['annotation_url'] = [get_url(pred_df.iloc[i]['site_id'], pred_df.iloc[i]['journal_oid']) for i in range(len(pred_df))]\n",
    "pred_df.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull out the training and validation data and analyze accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pred_df[pred_df[\"in_training_data\"] == True]\n",
    "#print(len(train_df), len(train_lines))\n",
    "assert len(train_df) == len(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pred_df[pred_df[\"in_training_data\"] == False]\n",
    "assert len(valid_df) == len(valid_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "np.sum(train_df[\"is_patient_predicted\"] == train_df[\"is_patient\"]) / len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation accuracy\n",
    "np.sum(valid_df[\"is_patient_predicted\"] == valid_df[\"is_patient\"]) / len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(valid_df[\"is_patient_predicted\"]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(valid_df[\"is_patient\"]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1 if b else 0 for b in valid_df[\"is_patient\"]]\n",
    "y_pred = [1 if b else 0 for b in valid_df[\"is_patient_predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classes=[\"Non-Patient\", \"Patient\"]\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# vis code borrowed from: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues, print_cm=False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    if print_cm:\n",
    "        if normalize:\n",
    "            print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')\n",
    "        print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "fig_size = (9,9)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=fig_size)\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=fig_size)\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the predicted probabilities\n",
    "title = \"Distribution of the VW model's predicted probabilities\"\n",
    "fig, ax = pl.subplots(num=title, figsize=(8,8))\n",
    "x = np.array(valid_df['prob'], dtype=float)\n",
    "patches = plt.hist(x, bins=50, range=(0,1), label=\"All validation journals\")\n",
    "\n",
    "# Plot the distribution of specifically the wrongly-classified journal entries\n",
    "x2 = np.array(valid_df[valid_df[\"is_patient\"] != valid_df[\"is_patient_predicted\"]]['prob'], dtype=float)\n",
    "plt.hist(x2, bins=50, range=(0,1), label=\"Wrongly classified journals\")\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel(\"Probability of being patient-authored\")\n",
    "ax.set_ylabel(\"Number of journals\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.grid(axis=\"y\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within the errors, show the least-confident journal posts\n",
    "errors = valid_df[valid_df[\"is_patient\"] != valid_df[\"is_patient_predicted\"]]\n",
    "inds = np.argsort(np.abs(errors['prob'] - 0.5))\n",
    "\n",
    "# Show the n least-confident results that were wrongly classified\n",
    "n = 10\n",
    "toughest_n = errors.iloc[inds[:n]]\n",
    "\n",
    "HTML(toughest_n.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the predictions of the model on the training/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert predictions into annotation web client's database\n",
    "def insert_author_predictions(predictions, commit=True):\n",
    "    prediction_type = 'journal_author_type'\n",
    "    try:\n",
    "        db = get_annotation_db()\n",
    "        \n",
    "        for prediction in predictions:\n",
    "            site_id = prediction['site_id']\n",
    "            journal_oid = prediction['journal_oid']\n",
    "            data = prediction['data']\n",
    "            probability = prediction['probability']\n",
    "        \n",
    "            db.execute(\n",
    "                \"\"\"INSERT INTO journalPrediction \n",
    "                (site_id, journal_oid, prediction_type, data, probability) \n",
    "                VALUES (?, ?, ?, ?, ?)\"\"\",\n",
    "                (site_id, journal_oid, prediction_type, data, probability)\n",
    "            )\n",
    "        \n",
    "        \n",
    "        if commit:\n",
    "            db.commit()\n",
    "    finally:\n",
    "        db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_prediction_insertion = False\n",
    "if not skip_prediction_insertion:\n",
    "    predictions = [{'site_id': pred_df.iloc[i]['site_id'],\n",
    "                    'journal_oid': pred_df.iloc[i]['journal_oid'],\n",
    "                    'data': \"Patient\" if pred_df.iloc[i]['is_patient_predicted'] else \"Non-patient\",\n",
    "                    'probability': pred_df.iloc[i]['prob']}\n",
    "                   for i in range(len(pred_df))]\n",
    "    print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_prediction_insertion:\n",
    "    insert_author_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model to unlabeled data\n",
    "\n",
    "First, we'll create a file in VW format from a set of unlabeled sites.\n",
    "\n",
    "Then, we'll apply the model to the file to get predictions.\n",
    "\n",
    "Finally, we'll load in the predictions and analyze the highest and lowest confidence journals to see if there are clear error patterns or if there is a clear bias in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_site_working_dir = \"/home/srivbane/shared/caringbridge/data/projects/qual-health-journeys/identify_candidate_sites\"\n",
    "valid_sites_filename = os.path.join(candidate_site_working_dir, \"valid_classification_sites.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_sites_filename, 'r') as infile:\n",
    "    candidate_site_ids = [int(line.strip()) for line in infile]\n",
    "len(candidate_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select a subset of n sites to predict the labels for\n",
    "random.shuffle(candidate_site_ids)\n",
    "\n",
    "n = 1000\n",
    "candidate_subset = candidate_site_ids[:n]\n",
    "\n",
    "candidate_subset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format the selected sites in VW format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_vw_test_filepath = os.path.join(vw_working_dir, \"unlabeled_candidate_sites.txt\")\n",
    "\n",
    "# only create the file if it doesn't exist\n",
    "overwrite_unlabeled_file = True\n",
    "\n",
    "lines_written = 0\n",
    "if not os.path.exists(unlabeled_vw_test_filepath) or overwrite_unlabeled_file:\n",
    "    with open(unlabeled_vw_test_filepath, 'w', encoding=\"utf-8\") as outfile:\n",
    "        for site_id in tqdm(candidate_subset):\n",
    "            body_list = get_journal_texts(site_id)\n",
    "            for body in body_list:\n",
    "                journal_oid = body['journal_oid']\n",
    "                body_text = body['body']\n",
    "                if len(body_text) < 50:\n",
    "                    continue\n",
    "                cleaned_body = get_cleaned_text(body_text)\n",
    "\n",
    "                identifier = \"sid%djoid%s\" % (site_id, journal_oid)\n",
    "                formatted_journal = identifier + \"|J \" + cleaned_body\n",
    "\n",
    "                outfile.write(formatted_journal + \"\\n\")\n",
    "                lines_written += 1\n",
    "\n",
    "lines_written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 {unlabeled_vw_test_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the VW model to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {vw_working_dir} {unlabeled_vw_test_filepath}\n",
    "working_dir=${1}\n",
    "unlabeled_vw_test_filepath=${2}\n",
    "\n",
    "echo \"Making predictions on test file.\"\n",
    "vw --binary \\\n",
    "   -t -i ${working_dir}/patient_authored.model \\\n",
    "   -p ${working_dir}/unlabeled_text_only.pred \\\n",
    "   -r ${working_dir}/unlabeled_text_only.pred.raw \\\n",
    "   -d ${unlabeled_vw_test_filepath}\n",
    " \n",
    "echo \"Finished.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions_filepath = os.path.join(vw_working_dir, \"unlabeled_text_only.pred.raw\")\n",
    "assert os.path.exists(raw_predictions_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {raw_predictions_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the predictions into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_predictions(raw_predictions_filepath):\n",
    "    \n",
    "    with open(raw_predictions_filepath, 'r') as infile:\n",
    "        pred_lines = infile.readlines()\n",
    "    \n",
    "    dtype_dict = {\"site_id\": int, \n",
    "                     \"journal_oid\": str,\n",
    "                     \"prob\": float,\n",
    "                     \"is_patient\": bool}\n",
    "    columns = list(dtype_dict.keys())\n",
    "    dtype = [dtype_dict[col] for col in columns]\n",
    "    pred_df = pd.DataFrame(index=range(len(pred_lines)), columns=columns)\n",
    "    \n",
    "    site_id_list = []\n",
    "    journal_oid_list = []\n",
    "    prob_list = []\n",
    "    is_patient_list = []\n",
    "    url_list = []\n",
    "    index = 0\n",
    "    for pred_line in tqdm(pred_lines):\n",
    "        raw_prob, journal_id = pred_line.strip().split(\" \")\n",
    "        site_id, journal_oid = journal_id.split(\"joid\")\n",
    "        site_id = int(site_id[3:])\n",
    "        \n",
    "        # probability is computed assuming a logistic loss function\n",
    "        # if we change the model to use a different loss, this won't work anymore!\n",
    "        prob = 1 / (1 + np.exp(-1 * float(raw_prob) ))\n",
    "        assert prob >= 0 and prob <= 1\n",
    "        is_patient = prob >= 0.5\n",
    "        \n",
    "        url = get_url(site_id, journal_oid)\n",
    "        \n",
    "        site_id_list.append(site_id)\n",
    "        journal_oid_list.append(journal_oid)\n",
    "        prob_list.append(prob)\n",
    "        is_patient_list.append(is_patient)\n",
    "        url_list.append(url)\n",
    "        \n",
    "        #pred_df.iloc[index][\"site_id\"] = site_id\n",
    "        #pred_df.iloc[index][\"journal_oid\"] = journal_oid\n",
    "        #pred_df.iloc[index][\"is_patient\"] = is_patient\n",
    "        #pred_df.iloc[index][\"raw_prob\"] = float(raw_prob)\n",
    "        #pred_df.iloc[index][\"prob\"] = prob\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    pred_df['site_id'] = site_id_list\n",
    "    pred_df['journal_oid'] = journal_oid_list\n",
    "    pred_df['prob'] = prob_list\n",
    "    pred_df['is_patient'] = is_patient_list\n",
    "    pred_df['annotation_url'] = url_list\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df = get_author_predictions(raw_predictions_filepath)\n",
    "len(unlabeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the assigned predictions to the annotation database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to do some tedious nonsense here to avoid dealing with how slow pandas indexing is\n",
    "if not skip_prediction_insertion:\n",
    "    site_id_list = unlabeled_df['site_id'].tolist()\n",
    "    journal_oid_list = unlabeled_df['journal_oid'].tolist()\n",
    "    prob_list = unlabeled_df['prob'].tolist()\n",
    "    is_patient_list = unlabeled_df['is_patient'].tolist()\n",
    "    predictions = [{'site_id': site_id, \n",
    "                    'journal_oid': journal_oid,\n",
    "                    'data': \"Patient\" if is_patient else \"Non-patient\",\n",
    "                    'probability': prob} \n",
    "                   for site_id, journal_oid, is_patient, prob\n",
    "                   in tqdm(zip(site_id_list, journal_oid_list, is_patient_list, prob_list))]\n",
    "    del site_id_list, journal_oid_list, is_patient_list, prob_list\n",
    "    print(predictions[0], len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not skip_prediction_insertion:\n",
    "    insert_author_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are redundant with the df, and may take up quite a lot of memory, so delete them\n",
    "if not skip_prediction_insertion:\n",
    "    del predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the predictions produced by the model on the unlabeled sites\n",
    "\n",
    "We look at the general distribution of the probabilities and look at the journals about which the classifier is least certain.\n",
    "\n",
    "We then look at the distribution of patient posts within a site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(unlabeled_df['is_patient']).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of unlabeled journal predicted probabilities\n",
    "title = \"Distribution of predicted probabilities on unlabeled sites\"\n",
    "fig, ax = pl.subplots(num=title, figsize=(8,8))\n",
    "x = np.array(unlabeled_df['prob'], dtype=float)\n",
    "patches = plt.hist(x, bins=50, range=(0,1), label=\"Unlabeled candidate sites\")\n",
    "\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel(\"Probability of being patient-authored\")\n",
    "ax.set_ylabel(\"Number of journals\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.grid(axis=\"y\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within the errors, compute the least-confident journal posts (those with probability closest to 0.5)\n",
    "inds = np.argsort(np.abs(unlabeled_df['prob'] - 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the n least-confident results in a dataframe\n",
    "n = 16\n",
    "toughest_n = unlabeled_df.iloc[inds[:n]]\n",
    "\n",
    "HTML(toughest_n.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_proportions_dict = {}\n",
    "total_patient_journals = 0\n",
    "for site_id, group in unlabeled_df.groupby(by=\"site_id\"):\n",
    "    patient_journals = np.count_nonzero(group['is_patient'])\n",
    "    total_patient_journals += patient_journals\n",
    "    \n",
    "    num_journals = len(group)\n",
    "    \n",
    "    patient_proportion = patient_journals / num_journals\n",
    "    patient_proportions_dict[site_id] = patient_proportion\n",
    "print(\"Identified %d total journals as patient-authored.\" % total_patient_journals)\n",
    "len(patient_proportions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the site proportions data to a CSV file\n",
    "site_proportions_filepath = os.path.join(general_working_dir, \"site_proportions.csv\")\n",
    "with open(site_proportions_filepath, 'w') as outfile:\n",
    "    outfile.write(\"site_id,proportion_patient_authored\\n\")\n",
    "    for site_id in patient_proportions_dict:\n",
    "        proportion_patient_authored = patient_proportions_dict[site_id]\n",
    "        outfile.write(\"{},{}\\n\".format(site_id, proportion_patient_authored))\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([\"Mostly patient\" if p > 0.5 else \"Mostly non-patient\" for p in patient_proportions_dict.values()]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent that are mostly patient-authored\n",
    "np.sum([1 if p > 0.5 else 0 for p in patient_proportions_dict.values()]) / len(patient_proportions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of unlabeled journal predicted probabilities\n",
    "title = \"Proportion of patient-authored journals\"\n",
    "fig, ax = pl.subplots(num=title, figsize=(8,8))\n",
    "x = patient_proportions_dict.values()\n",
    "patches = plt.hist(x, bins=50, range=(0,1), label=\"Unlabeled candidate sites\")\n",
    "\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel(\"% of journals predicted to be patient-authored\")\n",
    "ax.set_ylabel(\"Number of sites\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.grid(axis=\"y\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the distribution over time (by bucketing by percentile journals complete on the site?)\n",
    "\n",
    "def get_journal_metadata_db():\n",
    "    db_filepath=\"/home/srivbane/shared/caringbridge/data/projects/qual-health-journeys/extract_site_features/journal_metadata.db\"\n",
    "    db = sqlite3.connect(\n",
    "            db_filepath,\n",
    "            detect_types=sqlite3.PARSE_DECLTYPES\n",
    "        )\n",
    "    db.row_factory = sqlite3.Row\n",
    "    return db\n",
    "\n",
    "def get_journal_times(site_id):\n",
    "    try:\n",
    "        db = get_journal_metadata_db()\n",
    "        cursor = db.execute(\"\"\"SELECT journal_oid, site_index, created_at\n",
    "                                    FROM journalMetadata\n",
    "                                    WHERE site_id = ? \n",
    "                                    GROUP BY journal_oid\n",
    "                                    ORDER BY id DESC\"\"\",\n",
    "                                (site_id,))\n",
    "        \n",
    "        times = []\n",
    "        results = cursor.fetchall()\n",
    "        if results is None:\n",
    "            return []\n",
    "        for result in results:\n",
    "            d = {key: result[key] for key in result.keys()}\n",
    "            times.append(d)\n",
    "        times.sort(key=lambda d: d['created_at'])\n",
    "        \n",
    "        journal_oid_dict = {}\n",
    "        for time_dict in times:\n",
    "            journal_oid_dict[time_dict['journal_oid']] = time_dict\n",
    "        return journal_oid_dict\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "buckets = 20\n",
    "percentile_gap = 1 / buckets\n",
    "print(\"%d buckets will each capture %.2f%% of a site's journals.\" % (buckets, percentile_gap * 100))\n",
    "\n",
    "total_bucket_counts = [0 for i in range(buckets)]\n",
    "is_patient_bucket_counts = [0 for i in range(buckets)]\n",
    "\n",
    "for site_id, group in tqdm(unlabeled_df.groupby(by=\"site_id\")):\n",
    "    if len(group) < 20:\n",
    "        continue  # only compute percentiles for sites with at least X journals\n",
    "    \n",
    "    journal_times = get_journal_times(site_id)\n",
    "    num_journals = len(journal_times)\n",
    "    assert num_journals >= len(group)\n",
    "    \n",
    "    journal_oid_list = group['journal_oid']\n",
    "    is_patient_list = group['is_patient'].tolist()\n",
    "    \n",
    "    for i, journal_oid in enumerate(journal_oid_list):\n",
    "        site_index = journal_times[journal_oid]['site_index']\n",
    "        created_at = journal_times[journal_oid]['created_at']\n",
    "        \n",
    "        percent_through_site = site_index / num_journals\n",
    "        assert percent_through_site < 1\n",
    "        \n",
    "        bucket_right_boundary = percentile_gap\n",
    "        bucket = 0\n",
    "        while percent_through_site > bucket_right_boundary:\n",
    "            bucket += 1\n",
    "            bucket_right_boundary += percentile_gap\n",
    "        \n",
    "        #print(site_index, percent_through_site, bucket)\n",
    "        if bucket_right_boundary >= 1:  # should be in the last bucket\n",
    "            bucket = buckets - 1\n",
    "            \n",
    "        total_bucket_counts[bucket] += 1\n",
    "        if is_patient_list[i]:\n",
    "            is_patient_bucket_counts[bucket] += 1\n",
    "np.mean(total_bucket_counts), np.mean(is_patient_bucket_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_patient_bucket_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_proportions = [patient / total for patient, total in zip(is_patient_bucket_counts, total_bucket_counts)]\n",
    "bucket_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Proportion of patient-authored journals by strata\"\n",
    "fig, ax = pl.subplots(num=title, figsize=(8,8))\n",
    "\n",
    "x = np.arange(0, 1, percentile_gap)\n",
    "y = bucket_proportions\n",
    "plt.bar(x, y, percentile_gap, align='edge')\n",
    "\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel(\"%% of site (in %d buckets)\" % buckets)\n",
    "ax.set_ylabel(\"Proportion of patient-authored journals\")\n",
    "\n",
    "#ax.set_xticks([i for i in range(0, 108, 4)])\n",
    "#ax.set_xticklabels([str(i) if i != 104 else \"+\" for i in range(0, 108, 4)])\n",
    "\n",
    "#ax.set_yticks([i for i in range(0, 430, 10)])\n",
    "\n",
    "ax.set_ylim((0, 1))\n",
    "\n",
    "ax.grid(axis=\"y\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model to all journals via streaming\n",
    "\n",
    "Everything below here was moved to a separate script: `predict_author_type.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/srivbane/levon003/repos/qual-health-journeys/annotation_data\")\n",
    "import journal as journal_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "journal_utils = reload(journal_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw_streaming_command = f\"vw --quiet -i {vw_working_dir}/patient_authored.model -t -p /dev/stdout\"\n",
    "vw_streaming_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(journal_utils.iter_journal_texts(limit=10))\n",
    "len(res), res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(results):\n",
    "    # expects results of the form \"0.564 sid912454joid5470dee0ca16b4c27bb0e9a5\\n\"\n",
    "    preds = []\n",
    "    try:\n",
    "        for result in results:\n",
    "            if result.strip() == '':\n",
    "                continue\n",
    "            pred, key = result.strip().split(\" \")\n",
    "            pred = float(pred)\n",
    "            siteId, journalOid = key.split(\"joid\")\n",
    "            siteId = int(siteId[3:])\n",
    "            pred_dict = {'author_type_raw_prediction': pred,\n",
    "                         'site_id': siteId,\n",
    "                         'journal_oid': journalOid}\n",
    "            preds.append(pred_dict)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(\"Potentially erroneous result:\", result)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_vw_format(journal):\n",
    "    text = journal_utils.get_journal_text_representation(journal)\n",
    "    if text is None:\n",
    "        return None\n",
    "    cleaned_text = get_cleaned_text(text, strip_tags=False)\n",
    "    identifier = \"sid%djoid%s\" % (journal['site_id'], journal['journal_oid'])\n",
    "    return \" \" + identifier + \"|J \" + cleaned_text + \"\\n\"\n",
    "\n",
    "\n",
    "def process_batch(batch_lines, vw_proc):\n",
    "    for line in batch_lines:\n",
    "        #print(line)\n",
    "        vw_proc.stdin.write(line)\n",
    "        vw_proc.stdin.flush()\n",
    "    results = []\n",
    "    for i in range(len(batch_lines)):\n",
    "        result = vw_proc.stdout.readline()\n",
    "        #print(result)\n",
    "        if result == '':\n",
    "            raise ValueError(\"Reached end of stdout, which is a surprise given that # inputs should == # outputs.\")\n",
    "        results.append(result)\n",
    "    preds = get_preds(results)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def write_preds(fd, preds):\n",
    "    for p in preds:\n",
    "        line = \"{},{},{}\\n\".format(p['site_id'], p['journal_oid'], p['author_type_raw_prediction'])\n",
    "        fd.write(line)\n",
    "\n",
    "def stream_journals(preds_filepath):\n",
    "    vw_proc = Popen(vw_streaming_command, \n",
    "                    shell=True, encoding='utf-8',\n",
    "                    stdout=PIPE, stdin=PIPE, stderr=None)\n",
    "    try:\n",
    "        \n",
    "        batch_size = 100\n",
    "        batch_lines = []\n",
    "        journal_iter = journal_utils.iter_journal_texts(limit=10000)\n",
    "        estimated_length = 15327592  # total number of journals\n",
    "        report_frequency = 1000\n",
    "        with open(preds_filepath, 'w') as outfile:\n",
    "            for i, journal in enumerate(journal_iter):\n",
    "                if i % report_frequency == 0:\n",
    "                    print(i, \"%.2f\" % (i / estimated_length * 100))\n",
    "                line = get_vw_format(journal)\n",
    "                if line is None:\n",
    "                    continue\n",
    "                batch_lines.append(line)\n",
    "                if len(batch_lines) == batch_size:\n",
    "                    batch_preds = process_batch(batch_lines, vw_proc)\n",
    "                    write_preds(outfile, batch_preds)\n",
    "                    batch_lines = []\n",
    "            if len(batch_lines) > 0:\n",
    "                batch_preds = process_batch(batch_lines, vw_proc)\n",
    "                write_preds(outfile, batch_preds)\n",
    "    finally:\n",
    "        vw_proc.stdin.close()\n",
    "        vw_proc.stdout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_filepath = os.path.join(vw_working_dir, \"all_journal_preds.csv\")\n",
    "stream_journals(preds_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# initial time estimate\n",
    "# 100000 journals processed in 314 seconds\n",
    "# Suggesting a full run will take 13.36 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail {preds_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l {preds_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpuonly",
   "language": "python",
   "name": "pytorch-cpuonly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
