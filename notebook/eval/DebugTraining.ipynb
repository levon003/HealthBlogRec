{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southeast-potter",
   "metadata": {},
   "source": [
    "Debug Training (& Testing)\n",
    "===\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import dateutil.parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "import cbrec.genconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cbrec.genconfig.TestConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.featuredb\n",
    "import cbrec.utils\n",
    "import cbrec.reccontext\n",
    "import cbrec.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list = cbrec.utils.get_metadata_list(config.metadata_filepath)\n",
    "len(md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cbrec.utils.create_metadata_dataframe(md_list)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.type.value_counts().rename(\"Total metadata count by type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-pricing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triples():\n",
    "    db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "    #triple_metadata = []\n",
    "    arrs = []\n",
    "    ys = []\n",
    "    \n",
    "    try:\n",
    "        for row in cbrec.featuredb.stream_triples(db):\n",
    "            #md = {key: row[key] for key in row.keys() if not key.endswith(\"_arr\")}\n",
    "            #triple_metadata.append(md)\n",
    "            target_feature_arr = np.concatenate([row['source_feature_arr'], row['target_feature_arr'], row['source_feature_arr'] - row['target_feature_arr'], row['source_target_feature_arr']])\n",
    "            alt_feature_arr = np.concatenate([row['source_feature_arr'], row['alt_feature_arr'], row['source_feature_arr'] - row['alt_feature_arr'], row['source_alt_feature_arr']])\n",
    "            arrs.append(target_feature_arr)\n",
    "            ys.append(1)\n",
    "            arrs.append(alt_feature_arr)\n",
    "            ys.append(0)\n",
    "        #df = pd.DataFrame(triple_metadata)\n",
    "        #return df\n",
    "    finally:\n",
    "        db.close()\n",
    "    return arrs, ys\n",
    "        \n",
    "feature_arrs, ys = get_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(feature_arrs)\n",
    "y_true = np.array(ys)\n",
    "X.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('scaler', sklearn.preprocessing.StandardScaler()),\n",
    "    ('clf', sklearn.linear_model.SGDClassifier(loss='log')),\n",
    "])\n",
    "clf.fit(X, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, looking good\n",
    "np.sum(clf.predict(X) == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-surgery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_list = cbrec.utils.get_test_metadata(md_list)\n",
    "len(test_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df[df.type == 'test']\n",
    "len(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_contexts(config, test_md_list, clf):\n",
    "    db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "    \n",
    "    try:\n",
    "        for md in test_md_list:\n",
    "            metadata_id = md['metadata_id']\n",
    "            test_context = cbrec.featuredb.get_test_context_by_metadata_id(db, metadata_id, config)\n",
    "            rc = cbrec.reccontext.RecContext.create_from_test_context(config, md, test_context)\n",
    "            \n",
    "            scorer = cbrec.evaluation.SklearnModelScorer(config, rc, clf, \"PointwiseLogreg\")\n",
    "            metric_dict = scorer.score_proba()\n",
    "            md['baseline_metrics']['PointwiseLogreg'] = metric_dict\n",
    "    finally:\n",
    "        db.close()\n",
    "        \n",
    "get_test_contexts(config, test_md_list, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = test_md_list[0]['baseline_metrics'].keys()\n",
    "print(models)\n",
    "model_df_dict = {}\n",
    "for model in tqdm(models):\n",
    "    metrics_list = []\n",
    "    for md in test_md_list:\n",
    "        metrics = md['baseline_metrics'][model]\n",
    "        metrics['metadata_id'] = md['metadata_id']\n",
    "        metrics_list.append(metrics)\n",
    "    mdf = pd.DataFrame(metrics_list)\n",
    "    mdf['reciprocal_rank'] = 1 / mdf.target_rank\n",
    "    model_df_dict[model] = mdf\n",
    "    print(model, len(mdf))\n",
    "len(model_df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for model in models:\n",
    "    mdf = model_df_dict[model][['target_raw_score', 'target_rank', 'reciprocal_rank', 'ndcg_1', 'ndcg_5', 'ndcg_10', 'ndcg_50']]\n",
    "    means = mdf.mean()\n",
    "    means = pd.concat([pd.Series([np.sum(mdf.target_rank <= 5) / len(mdf),], index=['% <= rank 5',]), means])\n",
    "    means = pd.concat([pd.Series([model,], index=['model',]), means])\n",
    "    scores.append(means)\n",
    "score_df = pd.DataFrame(scores).rename(columns={'target_rank': 'mean_rank', 'reciprocal_rank': 'mrr', 'target_raw_score': 'mean_raw_score'}).sort_values(by='mean_rank')\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-leisure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-adams",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-algorithm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-valve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-market",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-feature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
