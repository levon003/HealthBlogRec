{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c91a5fd",
   "metadata": {},
   "source": [
    "Matrix Factorization Compute\n",
    "===\n",
    "\n",
    "Collaborative filtering.\n",
    "\n",
    "Rendle: https://arxiv.org/pdf/2005.09683.pdf\n",
    "\n",
    "Fast.ai implementation: https://github.com/fastai/fastbook/blob/master/08_collab.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0bc388",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d464618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import dateutil.parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90068223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace packages\n",
    "import transformers\n",
    "import tokenizers\n",
    "import torch\n",
    "\n",
    "# more torch imports\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "import cbrec.genconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cbrec.genconfig.Config()\n",
    "#config.metadata_filepath += \"_old\"\n",
    "#config.feature_db_filepath += \"_old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908daa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.featuredb\n",
    "import cbrec.utils\n",
    "import cbrec.data\n",
    "import cbrec.reccontext\n",
    "import cbrec.evaluation\n",
    "import cbrec.torchmodel\n",
    "import cbrec.text.embeddingdb\n",
    "import cbrec.text.journalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.logutils\n",
    "cbrec.logutils.set_up_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off matplotlib logging\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)\n",
    "import cbcore.data.paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4601da",
   "metadata": {},
   "source": [
    "## Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77222dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath)]\n",
    "len(md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_md_list = [md for md in md_list if md['type'] == 'train']\n",
    "len(train_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd88f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add full USP info to MD list\n",
    "train_inds = {}\n",
    "for i, md in enumerate(train_md_list):\n",
    "    train_inds[md['metadata_id']] = i\n",
    "\n",
    "config = cbrec.genconfig.Config()\n",
    "db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "with db:\n",
    "    command = \"\"\"\n",
    "    SELECT \n",
    "        interaction_timestamp, \n",
    "        metadata_id,\n",
    "        source_user_id,\n",
    "        source_site_id,\n",
    "        target_user_id,\n",
    "        target_site_id,\n",
    "        alt_user_id,\n",
    "        alt_site_id\n",
    "    FROM triple\n",
    "    ORDER BY RANDOM()\n",
    "    \"\"\"\n",
    "    cursor = db.execute(command)\n",
    "    if cursor is None:\n",
    "        raise ValueError(\"Null cursor.\")\n",
    "    row = cursor.fetchone()\n",
    "    while row is not None:\n",
    "        i = train_inds[row['metadata_id']]\n",
    "        assert row['source_user_id'] == train_md_list[i]['source_user_id']\n",
    "        train_md_list[i].update(row)\n",
    "        row = cursor.fetchone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1464d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_md_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a5069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41674a45",
   "metadata": {},
   "source": [
    "## USP approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    \"\"\"\n",
    "    Derived from https://github.com/fastai/fastbook/blob/master/08_collab.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, n_source, n_candidate, n_factors):\n",
    "        self.source_factors = nn.Embedding(n_source, n_factors)\n",
    "        self.source_bias = nn.Embedding(n_source, 1)\n",
    "        self.candidate_factors = nn.Embedding(n_candidate, n_factors)\n",
    "        self.candidate_bias = nn.Embedding(n_candidate, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sources = self.source_factors(x[:,0])\n",
    "        candidates = self.candidate_factors(x[:,1])\n",
    "        res = (sources * candidates).sum(dim=1, keepdim=True)\n",
    "        res += self.source_bias(x[:,0]) + self.candidate_bias(x[:,1])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "source_c = Counter()\n",
    "target_c = Counter()\n",
    "alt_c = Counter()\n",
    "for md in train_md_list:\n",
    "    source_usp = (md['source_user_id'], md['source_site_id'])\n",
    "    target_usp = (md['target_user_id'], md['target_site_id'])\n",
    "    alt_usp = (md['alt_user_id'], md['alt_site_id'])\n",
    "    source_c.update([source_usp,])\n",
    "    target_c.update([target_usp,])\n",
    "    alt_c.update([alt_usp,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_counts_list = []\n",
    "for c in [source_c, target_c, alt_c]:\n",
    "    vcounts = pd.Series([count for usp, count in c.most_common()]).value_counts()\n",
    "    v_counts_list.append(vcounts)\n",
    "    print(f\"total={vcounts.sum()} total>1={vcounts[vcounts.index > 1].sum()} unique init counts={len(vcounts)} max inits={vcounts.index.max()} # w 1 inits={vcounts[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign every USP an integer ID\n",
    "source_usp_id_map = {}\n",
    "source_i = 1\n",
    "candidate_usp_id_map = {}\n",
    "candidate_i = 1\n",
    "\n",
    "for md in train_md_list:\n",
    "    source_usp = (md['source_user_id'], md['source_site_id'])\n",
    "    target_usp = (md['target_user_id'], md['target_site_id'])\n",
    "    alt_usp = (md['alt_user_id'], md['alt_site_id'])\n",
    "    \n",
    "    if source_c[source_usp] == 1:\n",
    "        md['source_usp_id'] = 0\n",
    "    else:\n",
    "        if source_usp not in source_usp_id_map:\n",
    "            source_usp_id_map[source_usp] = source_i\n",
    "            source_i += 1\n",
    "        md['source_usp_id'] = source_usp_id_map[source_usp]\n",
    "        \n",
    "    if target_c[target_usp] == 1:  # + alt_c[target_usp] == 1:\n",
    "        md['target_usp_id'] = 0\n",
    "    else:\n",
    "        if target_usp not in candidate_usp_id_map:\n",
    "            candidate_usp_id_map[target_usp] = candidate_i\n",
    "            candidate_i += 1\n",
    "        md['target_usp_id'] = candidate_usp_id_map[target_usp]\n",
    "        \n",
    "    if alt_c[alt_usp] == 1:\n",
    "        md['alt_usp_id'] = 0\n",
    "    else:\n",
    "        if alt_usp not in candidate_usp_id_map:\n",
    "            candidate_usp_id_map[alt_usp] = candidate_i\n",
    "            candidate_i += 1\n",
    "        md['alt_usp_id'] = candidate_usp_id_map[alt_usp]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([md['source_usp_id'] for md in train_md_list]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57570817",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([md['target_usp_id'] for md in train_md_list]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([md['alt_usp_id'] for md in train_md_list]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac550f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3be2210",
   "metadata": {},
   "source": [
    "## User -> Site approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d1d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b483da",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_alt = True\n",
    "MIN_OCCURENCE_COUNT = 1\n",
    "\n",
    "user_counter = Counter()\n",
    "site_counter = Counter()\n",
    "for md in train_md_list:\n",
    "    user_counter.update([md['source_user_id'],])\n",
    "    sites = [md['target_site_id'],]\n",
    "    if include_alt:\n",
    "        assert md['target_site_id'] != md['alt_site_id']\n",
    "        sites.append(md['alt_site_id'])\n",
    "    site_counter.update(sites)\n",
    "print(f\"n_users_total={len(user_counter)}; n_sites_total={len(site_counter)}\")\n",
    "\n",
    "# assign every user and site an integer ID\n",
    "user_id_map = {}\n",
    "user_i = 1\n",
    "site_id_map = {}\n",
    "site_i = 1\n",
    "\n",
    "for md in train_md_list:\n",
    "    source_user_id = md['source_user_id']\n",
    "    target_site_id = md['target_site_id']\n",
    "    alt_site_id = md['alt_site_id']\n",
    "    \n",
    "    if user_counter[source_user_id] <= MIN_OCCURENCE_COUNT:\n",
    "        md['source_user_emb_id'] = 0\n",
    "    else:\n",
    "        if source_user_id not in user_id_map:\n",
    "            user_id_map[source_user_id] = user_i\n",
    "            user_i += 1\n",
    "        md['source_user_emb_id'] = user_id_map[source_user_id]\n",
    "    \n",
    "    site_ids = [('target', target_site_id),]\n",
    "    if include_alt:\n",
    "        site_ids.append(('alt', alt_site_id))\n",
    "    for key, site_id in site_ids:\n",
    "        if site_counter[site_id] <= MIN_OCCURENCE_COUNT:\n",
    "            md[key+'_site_emb_id'] = 0\n",
    "        else:\n",
    "            if site_id not in site_id_map:\n",
    "                site_id_map[site_id] = site_i\n",
    "                site_i += 1\n",
    "            md[key+'_site_emb_id'] = site_id_map[site_id]\n",
    "\n",
    "n_users = len(user_id_map)\n",
    "n_sites = len(site_id_map)\n",
    "print(f\"{n_users=}; {n_sites=}\")\n",
    "source_counts = pd.Series([md['source_user_emb_id'] for md in train_md_list]).value_counts()\n",
    "assert len(source_counts) == len(user_id_map) + 1  # +1, as 0 isn't in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1ad99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    \"\"\"\n",
    "    Derived from https://github.com/fastai/fastbook/blob/master/08_collab.ipynb\n",
    "    \n",
    "    Note: this is NOT the \"NeuMF\" model, and I regret calling it that. This is just the plain Probabilistic Matrix Factorization approach to Collaborative Filtering\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_sites, n_factors):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.site_factors = nn.Embedding(n_sites, n_factors)\n",
    "        self.site_bias = nn.Embedding(n_sites, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        sites = self.site_factors(x[:,1])\n",
    "        res = (users * sites).sum(dim=1, keepdim=True)\n",
    "        res += self.user_bias(x[:,0]) + self.site_bias(x[:,1])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "#def get_train_tuple(user_emb_id, site_emb_id):\n",
    "#    return (user_id_map[user_emb_id] if user_emb_id in user_id_map else 0,\n",
    "#     site_id_map[site_emb_id] if site_emb_id in site_id_map else 0)\n",
    "\n",
    "for md in train_md_list:\n",
    "    X_train.append((md['source_user_emb_id'], md['target_site_emb_id']))\n",
    "    y_train.append(1)\n",
    "    if include_alt:\n",
    "        X_train.append((md['source_user_emb_id'], md['alt_site_emb_id']))\n",
    "        y_train.append(0)\n",
    "X = np.array(X_train)\n",
    "y_true = np.array(y_train)\n",
    "X.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total zeros in X\n",
    "(X == 0).sum() / (X.shape[0] * X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e77797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert types\n",
    "#X, y_true = X.astype('float64'), y_true.astype('float64')\n",
    "y_true = y_true.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(np.ceil(len(y_true) * 0.99))\n",
    "X_train = X[:n_train,:]\n",
    "X_test = X[n_train:,:]\n",
    "y_train = y_true[:n_train]\n",
    "y_test = y_true[n_train:]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb6412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2684cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def train_model(config, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    An adapted form of the implementation from cbrec.modeling.train.train_model\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"cbrec.modeling.train.train_model\")\n",
    "\n",
    "    n_train = len(y_train)\n",
    "    n_test = len(y_test)\n",
    "\n",
    "    minibatch_size = n_train\n",
    "    if hasattr(config, \"minibatch_size\"):\n",
    "        minibatch_size = config.minibatch_size\n",
    "        logger.info(f\"Using minibatch size {minibatch_size}.\")\n",
    "    minibatch_size = min(n_train, minibatch_size)  # if minibatch_size is larger than n_train, force it to n_train\n",
    "    n_minibatches = int(np.ceil(n_train / minibatch_size))\n",
    "\n",
    "    # create the net\n",
    "    net = NeuMF(config.mf_n_users, config.mf_n_sites, config.mf_n_factors)\n",
    "\n",
    "    n_epochs = config.train_n_epochs\n",
    "    criterion = nn.BCEWithLogitsLoss()  # pointwise loss function\n",
    "\n",
    "    X_test_tensor = torch.from_numpy(X_test)\n",
    "    y_test_tensor = torch.from_numpy(y_test)\n",
    "    X_train_tensor = torch.from_numpy(X_train)\n",
    "    y_train_tensor = torch.from_numpy(y_train)\n",
    "    y_train_tensor = y_train_tensor.view(-1, 1)  # make labels 2-dimensional\n",
    "    #y_train_tensor = y_train_tensor.type_as(X_train_tensor)\n",
    "    if config.train_verbose:\n",
    "        logger.info(f\"Input tensor sizes: {X_train_tensor.size()}, {y_train_tensor.size()}\")\n",
    "        logger.info(f\"Validating model every {int(1/config.train_validation_rate)} epochs for {n_epochs} epochs.\")\n",
    "\n",
    "    # _metrics[0] -> Epoch, metrics[1] -> loss, _metrics[2] -> accuracy\n",
    "    test_metrics = np.zeros((3,int(n_epochs*config.train_validation_rate+1))) #+1 to ensure space for final epoch metric\n",
    "    train_metrics = np.zeros((3,n_epochs))\n",
    "\n",
    "    assert n_epochs > 0\n",
    "    optimizer = optim.Adam(net.parameters(),\n",
    "                           lr=config.train_lr_init,\n",
    "                           betas=(config.train_Adam_beta1, config.train_Adam_beta2),\n",
    "                           eps=config.train_Adam_eps,\n",
    "                           weight_decay=config.train_weight_decay)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config.train_max_lr,\n",
    "        steps_per_epoch=n_minibatches,\n",
    "        epochs=n_epochs,\n",
    "    )\n",
    "    for epoch in range(n_epochs):\n",
    "        s = datetime.now()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # shuffle the training data\n",
    "        # I am not sure if this matters at all\n",
    "        epoch_order = torch.randperm(n_train)\n",
    "\n",
    "        mb_metrics = []  # store the minibatch_metrics, then average after\n",
    "        for minibatch in range(n_minibatches):\n",
    "            minibatch_start = minibatch * minibatch_size\n",
    "            minibatch_end = min(minibatch_start + minibatch_size, n_train)\n",
    "            if config.train_verbose and epoch == 0:\n",
    "                logger.info(f\"    Minibatch for inds in {minibatch_start} - {minibatch_end}.\")\n",
    "            minibatch_inds = epoch_order[minibatch_start:minibatch_end]\n",
    "\n",
    "            inputs = X_train_tensor[minibatch_inds]\n",
    "            train_labels = y_train_tensor[minibatch_inds]\n",
    "\n",
    "            net.train()\n",
    "            train_outputs = net(inputs)\n",
    "            train_loss = criterion(train_outputs, train_labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            # compute accuracy\n",
    "            y_train_pred = torch.sigmoid(train_outputs.detach()).view((-1,)).numpy()\n",
    "            y_train_pred = (y_train_pred >= 0.5).astype(int)  # binarize predictions with a 0.5 decision boundary\n",
    "            y_train_minibatch = y_train[minibatch_inds.numpy()]\n",
    "            train_acc = np.sum(y_train_pred == y_train_minibatch) / len(y_train_minibatch)\n",
    "\n",
    "            mb_metrics.append((train_loss.item(), train_acc))\n",
    "        train_loss, train_acc = np.mean(np.array(mb_metrics), axis=0)\n",
    "        train_metrics[0,epoch] = epoch\n",
    "        train_metrics[1,epoch] = train_loss\n",
    "        train_metrics[2,epoch] = train_acc\n",
    "\n",
    "        should_stop_early = train_loss < 0.001\n",
    "        if config.train_verbose and (epoch < 3 or epoch == n_epochs - 1 or epoch % 10 == 0 or should_stop_early):\n",
    "            logger.info(f\"{epoch:>3} ({datetime.now() - s}): train loss={train_loss:.4f} train accuracy={train_acc*100:.2f}% LR={optimizer.param_groups[0]['lr']:.2E}\")\n",
    "        if should_stop_early:\n",
    "            break\n",
    "\n",
    "        if epoch % (1/config.train_validation_rate) == 0:\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = net(X_test_tensor)\n",
    "                test_loss = criterion(test_outputs.detach(), y_test_tensor.unsqueeze(1).float())\n",
    "                y_test_pred = torch.sigmoid(test_outputs.detach()).view((-1,)).numpy()\n",
    "                y_test_pred = (y_test_pred >= 0.5).astype(int)\n",
    "                test_acc = np.sum(y_test_pred == y_test) / len(y_test)\n",
    "            logger.info(f\"    {epoch:>3}: test loss={test_loss:.4f} test accuracy={test_acc*100:.2f}%\")\n",
    "            metric_ind = int(epoch*config.train_validation_rate)\n",
    "            if metric_ind > 0 and test_loss <= np.min(test_metrics[1,:metric_ind]):\n",
    "                # this is the lowest loss we've reached\n",
    "                # TODO could consider saving `net` at `epoch`.\n",
    "                logger.info(f\"    Best validation lost achieved so far.\")\n",
    "            test_metrics[0,metric_ind] = epoch\n",
    "            test_metrics[1,metric_ind] = test_loss\n",
    "            test_metrics[2,metric_ind] = test_acc\n",
    "\n",
    "    if config.train_verbose and n_epochs > 0:\n",
    "        final_train_loss = train_loss\n",
    "        final_epoch_count = epoch + 1\n",
    "        logger.info(f\"Completed {final_epoch_count} epochs with a final train loss of {final_train_loss:.4f}.\")\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test)\n",
    "        outputs = net(X_test_tensor)\n",
    "        test_loss = criterion(outputs.detach(), y_test_tensor.unsqueeze(1).float())\n",
    "        logger.info(f\"Test loss: {test_loss.item():.4f}\")\n",
    "        y_test_pred = torch.sigmoid(outputs.detach()).view((-1,)).numpy()\n",
    "        y_test_pred = (y_test_pred >= 0.5).astype(int)\n",
    "        acc = np.sum(y_test_pred == y_test) / len(y_test)\n",
    "        logger.info(f\"Test acc: {acc*100:.2f}%\")\n",
    "        test_metrics[0, test_metrics.shape[1] - 1] = epoch\n",
    "        test_metrics[1, test_metrics.shape[1] - 1] = test_loss\n",
    "        test_metrics[2, test_metrics.shape[1] - 1] = acc\n",
    "    return net, train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.modeling\n",
    "import cbrec.modeling.modelconfig\n",
    "config = cbrec.modeling.modelconfig.ModelConfig()\n",
    "config.mf_n_users = len(user_id_map) + 1\n",
    "config.mf_n_sites = len(site_id_map) + 1\n",
    "config.mf_n_factors = 128\n",
    "config.train_verbose = True\n",
    "config.train_max_lr = 0.3\n",
    "config.train_lr_init = 0.8\n",
    "config.train_weight_decay = 0.0001\n",
    "config.train_n_epochs=1000\n",
    "net, train_metrics, test_metrics = train_model(config, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bae8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.modeling\n",
    "import cbrec.modeling.modelconfig\n",
    "nets = []\n",
    "for train_weight_decay in [0, 0.0001, 0.01, 0.1]:\n",
    "    for mf_n_factors in [8, 16, 32, 64, 128]:   # [16, 32, 64, 96, 128, 192] is the list used in the NMF revisited paper\n",
    "        config = cbrec.modeling.modelconfig.ModelConfig()\n",
    "        config.mf_n_users = len(user_id_map) + 1\n",
    "        config.mf_n_sites = len(site_id_map) + 1\n",
    "        config.mf_n_factors = mf_n_factors\n",
    "        config.train_verbose = False\n",
    "        config.train_max_lr = 0.3\n",
    "        config.train_lr_init = 0.8\n",
    "        config.train_weight_decay = train_weight_decay\n",
    "        config.train_n_epochs=100\n",
    "        net, train_metrics, test_metrics = train_model(config, X_train, y_train, X_test, y_test)\n",
    "        nets.append((f\"n_factors={mf_n_factors}; wd={train_weight_decay}\", net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9e81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4cbee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed00491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a38e91ea",
   "metadata": {},
   "source": [
    "## Score models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61615c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cbrec\n",
    "except:\n",
    "    sys.path.append(\"/home/lana/levon003/repos/recsys-peer-match/src\")\n",
    "\n",
    "import cbrec.featuredb\n",
    "import cbrec.genconfig\n",
    "import cbrec.utils\n",
    "import cbrec.evaluation\n",
    "import cbrec.reccontext\n",
    "import cbrec.recentActivityCounter\n",
    "import cbrec.modeling.text_loader\n",
    "import cbrec.modeling.reccontext_builder\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4da7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_list = [md for md in md_list if md['type'] == 'test' or md['type'] == 'predict']\n",
    "test_md_map = {md['metadata_id']: md for md in test_md_list}\n",
    "len(test_md_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_END_TIMESTAMP = datetime.strptime(\"2021-07-01\", \"%Y-%m-%d\").timestamp() * 1000\n",
    "    \n",
    "config = cbrec.genconfig.Config()\n",
    "db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "with db:\n",
    "    test_filepath = os.path.join(config.feature_data_dir, 'mf_metrics.ndjson')\n",
    "    scores_filepath = os.path.join(config.feature_data_dir, 'mf_coverage_scores.pkl')\n",
    "    scores = []\n",
    "    net.eval()\n",
    "    with open(test_filepath, 'w') as metrics_outfile, open(scores_filepath, 'wb') as scores_outfile, torch.no_grad():\n",
    "        \n",
    "        for test_context in tqdm(cbrec.featuredb.stream_test_contexts(db, config), desc='Streaming test contexts', total=len(test_md_map)):\n",
    "            test_context_md = test_md_map[test_context['metadata_id']]\n",
    "            \n",
    "            rc = cbrec.reccontext.RecContext.create_from_test_context(config, test_context_md, test_context)\n",
    "            has_target = rc.md['has_target']\n",
    "\n",
    "            #if has_target and md['timestamp'] <= VALIDATION_END_TIMESTAMP:\n",
    "            #    continue  # don't process validation timestamps\n",
    "            \n",
    "            save_scores = not has_target  # save scores if this is a prediction target (for coverage)\n",
    "            scorer = cbrec.evaluation.Scorer(config, rc, save_scores=save_scores)\n",
    "\n",
    "            #site_id_arr, _ = np.unique(coverage_rc.candidate_usp_arr[:,1], return_index=True)\n",
    "            site_id_arr = scorer.site_id_arr\n",
    "            X_pred = np.zeros((len(site_id_arr), 2), dtype='int64')\n",
    "            X_pred[:,0] = user_id_map[rc.source_user_id] if rc.source_user_id in user_id_map else 0\n",
    "            X_pred[:,1] = [\n",
    "                site_id_map[site_id] if site_id in site_id_map else 0\n",
    "                for site_id in site_id_arr\n",
    "            ]\n",
    "            rc.md['user_known'] = rc.source_user_id in user_id_map\n",
    "            rc.md['n_sites_known'] = int((X_pred[:,1] != 0).sum())\n",
    "            X_pred = torch.from_numpy(X_pred)\n",
    "            for net_name, net in nets:\n",
    "                outputs = net(X_pred)\n",
    "                y_score = torch.sigmoid(outputs.detach()).view((-1,)).numpy()\n",
    "                scorer.compute_metrics(y_score, net_name)\n",
    "            \n",
    "            rc.md['metrics'] = scorer.metrics_dict\n",
    "            if save_scores:\n",
    "                rc.md['scores'] = scorer.scores_dict # {key: list(value) for key, value in scorer.scores_dict.items()}\n",
    "                scores.append(rc.md)\n",
    "                if len(scores) == 1000:\n",
    "                    pickle.dump(scores, scores_outfile)\n",
    "                    logging.info(f\"Saved pickle with {len(scores)} scores.\")\n",
    "                    scores = []\n",
    "            else:\n",
    "                line = json.dumps(rc.md) + \"\\n\"\n",
    "                metrics_outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498603f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f00f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 /home/lana/shared/caringbridge/data/projects/recsys-peer-match/feature_data/mf_metrics.ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175beb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdfd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f895a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_END_TIMESTAMP = datetime.strptime(\"2021-07-01\", \"%Y-%m-%d\").timestamp() * 1000\n",
    "md_list = [md for md in md_list if md['type'] == 'test' or md['type'] == 'predict']\n",
    "validation_md_list = [md for md in md_list if md['has_target'] and md['timestamp'] <= VALIDATION_END_TIMESTAMP]\n",
    "test_md_list = [md for md in md_list if md['has_target'] and md['timestamp'] > VALIDATION_END_TIMESTAMP]\n",
    "print(len(test_md_list))\n",
    "validation_metadata_ids = set([md['metadata_id'] for md in validation_md_list])\n",
    "test_metadata_ids = set([md['metadata_id'] for md in test_md_list])\n",
    "len(test_metadata_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c67bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "all_target_ranks = {'validation': defaultdict(list), 'test': defaultdict(list)}\n",
    "baseline_test_filepath = os.path.join(config.feature_data_dir, 'mf_metrics.ndjson')\n",
    "with open(baseline_test_filepath, 'r') as metadata_file:\n",
    "    for line in tqdm(metadata_file, desc='Reading metrics', disable=False):\n",
    "        md = json.loads(line)\n",
    "        if md['metadata_id'] in test_metadata_ids:\n",
    "            target_ranks = all_target_ranks['test']\n",
    "        elif md['metadata_id'] in validation_metadata_ids:\n",
    "            target_ranks = all_target_ranks['validation']\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        metrics_dict = md[\"metrics\"]\n",
    "        for model_name, metrics in metrics_dict.items():\n",
    "            target_rank = metrics['target_rank']\n",
    "            target_ranks[model_name].append(target_rank)\n",
    "assert len(all_target_ranks['validation']) > 0\n",
    "print(all_target_ranks['validation'].keys())\n",
    "len(all_target_ranks['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "target_ranks = all_target_ranks['validation']\n",
    "for model_name in target_ranks.keys():\n",
    "    ranks = np.array(target_ranks[model_name])\n",
    "\n",
    "    mrr = (1 / ranks).mean()\n",
    "    hr1 = (ranks == 1).sum() / len(ranks) * 100\n",
    "    hr5 = (ranks <= 5).sum() / len(ranks) * 100\n",
    "    d.append({\n",
    "        'model': model_name,\n",
    "        'n': len(ranks),\n",
    "        'mrr': mrr,\n",
    "        'hr1': hr1,\n",
    "        'hr5': hr5,\n",
    "    })\n",
    "eval_df = pd.DataFrame(d).sort_values(by='mrr', ascending=False)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b24644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing all of these because I'm lazy, but we only report the best value \n",
    "d = []\n",
    "target_ranks = all_target_ranks['test']\n",
    "for model_name in target_ranks.keys():\n",
    "    ranks = np.array(target_ranks[model_name])\n",
    "\n",
    "    mrr = (1 / ranks).mean()\n",
    "    hr1 = (ranks == 1).sum() / len(ranks) * 100\n",
    "    hr5 = (ranks <= 5).sum() / len(ranks) * 100\n",
    "    d.append({\n",
    "        'model': model_name,\n",
    "        'n': len(ranks),\n",
    "        'mrr': mrr,\n",
    "        'hr1': hr1,\n",
    "        'hr5': hr5,\n",
    "    })\n",
    "eval_df = pd.DataFrame(d).sort_values(by='mrr', ascending=False)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Reasonable to think that the Matrix Factorization models drop off a LOT after the training period is over, since any user/site not seen at least twice in the training period gets assigned the unknown vector.\n",
    "# So, should do a time-based analysis, as was done in LinearnetResults.ipynb\n",
    "# but given the lack of difference between the validation and test periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e448b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e147c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageHelper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "# load cov_helper from pickle\n",
    "coverage_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/feature_data/coverage\"\n",
    "with open(os.path.join(coverage_dir, 'cov_helper.pkl'), 'rb') as coverage_helper_file:\n",
    "    cov_helper = pickle.load(coverage_helper_file)\n",
    "cov_helper.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cov_helper.sites_with_previous_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_ties(site_id_arr, y_score_site, sort_inds):\n",
    "    \"\"\"\n",
    "    This implementation is terrible, although I believe it works.\n",
    "    \"\"\"\n",
    "    highest_scores = []\n",
    "    highest_score_site_ids = []\n",
    "    n_ties_broken = 0\n",
    "    i = 0\n",
    "    while len(highest_scores) < 5:\n",
    "        i += 1\n",
    "        score = y_score_site[sort_inds[-i]]\n",
    "        if score == y_score_site[sort_inds[-(i+1)]]:\n",
    "            inds = np.flatnonzero(y_score_site == score)\n",
    "            n_remaining = 5 - len(highest_scores)\n",
    "            if len(inds) <= n_remaining:\n",
    "                highest_scores.extend([score,]*len(inds))\n",
    "                highest_score_site_ids.extend(site_id_arr[inds])\n",
    "                assert len(highest_scores) == len(highest_score_site_ids)\n",
    "                i += len(inds) - 1\n",
    "            else:\n",
    "                highest_scores.extend([score,]*n_remaining)\n",
    "                subset_inds = np.random.choice(inds, size=n_remaining, replace=False)\n",
    "                highest_score_site_ids.extend(site_id_arr[subset_inds])\n",
    "                assert len(highest_scores) == len(highest_score_site_ids)\n",
    "                n_ties_broken += 1\n",
    "        else:\n",
    "            highest_scores.append(score)\n",
    "            highest_score_site_ids.append(site_id_arr[sort_inds[-i]])\n",
    "            assert len(highest_scores) == len(highest_score_site_ids)\n",
    "        if len(highest_scores) == 5:\n",
    "            break\n",
    "    return np.array(highest_scores), np.array(highest_score_site_ids), n_ties_broken > 0\n",
    "\n",
    "def compute_coverage_metrics(model_coverage_scores, cov_helper):\n",
    "    model_recs = defaultdict(list)\n",
    "    n_ties_broken = 0\n",
    "    for scores_md in model_coverage_scores:\n",
    "        metadata_id = scores_md['metadata_id']\n",
    "        site_id_arr = cov_helper.site_id_arr_map[metadata_id]\n",
    "        for model_name, y_score_site in scores_md['scores'].items():\n",
    "        \n",
    "            assert y_score_site.shape == site_id_arr.shape\n",
    "\n",
    "            # create rec batch\n",
    "            sort_inds = np.argsort(y_score_site)\n",
    "            # TODO need to compute ranks if there are ties; for now, we'll assume there aren't any ties\n",
    "            # in the case of ties, not clear what order argsort prefers\n",
    "            #ranks = rankdata(-1 * y_score_site, method='max')\n",
    "\n",
    "            highest_scores = y_score_site[sort_inds[-(cov_helper.n+1):]]\n",
    "            if len(set(highest_scores)) != len(highest_scores):\n",
    "                highest_scores, highest_score_site_ids, ties_broken = break_ties(site_id_arr, y_score_site, sort_inds)\n",
    "                if not np.all(highest_scores == np.flip(y_score_site[sort_inds[-5:]])):\n",
    "                    print(highest_scores)\n",
    "                    print(y_score_site[sort_inds[-5:]])\n",
    "                    return y_score_site\n",
    "                if ties_broken:\n",
    "                    n_ties_broken += 1\n",
    "            else:\n",
    "                #highest_scores = y_score_site[sort_inds[-cov_helper.n:]]\n",
    "                highest_score_site_ids = site_id_arr[sort_inds[-cov_helper.n:]]\n",
    "            model_recs[model_name].append(list(highest_score_site_ids))\n",
    "    print(f\"{n_ties_broken=}\")\n",
    "    \n",
    "    cov_data = []\n",
    "    for model_name, recs in model_recs.items():\n",
    "        recced_sites = set()\n",
    "        for rec in recs:\n",
    "            recced_sites.update(rec)\n",
    "        nonrecced_sites = cov_helper.eligible_sites - recced_sites\n",
    "\n",
    "        recced_inted = len(recced_sites & cov_helper.sites_with_previous_ints) / len(recced_sites)\n",
    "        nonrecced_inted = len(nonrecced_sites & cov_helper.sites_with_previous_ints) / len(nonrecced_sites)\n",
    "\n",
    "        site_ages = []\n",
    "        for rec in recs:\n",
    "            ages = np.array([cov_helper.timestamp - cov_helper.site_first_journal_timestamp_map[site_id] for site_id in rec])\n",
    "            ages = ages / 1000 / 60 / 60 / 24 / 7  # convert to weeks\n",
    "            assert np.all(ages > 0)\n",
    "            site_ages.append({\n",
    "                'min': ages.min(),\n",
    "                #'mean': ages.mean(),\n",
    "                #'std': ages.std(),\n",
    "                'median': np.median(ages),\n",
    "                #'max': ages.max(),\n",
    "            })\n",
    "        mean_min_age = np.mean([a['min'] for a in site_ages])\n",
    "        mean_median_age = np.mean([a['median'] for a in site_ages])\n",
    "\n",
    "        cov_data.append({\n",
    "            'model': model_name,\n",
    "            'n_recced_sites': len(recced_sites),\n",
    "            'n_nonrecced_sites': len(nonrecced_sites),\n",
    "            'pct_eligible_recced': len(recced_sites) / len(cov_helper.eligible_sites),\n",
    "            'pct_unique_recs': len(recced_sites) / (5 * 1000),\n",
    "            'pct_recced_with_int': recced_inted,\n",
    "            'pct_nonrecced_with_int': nonrecced_inted,\n",
    "            'pct_recced_without_int': 1 - recced_inted,\n",
    "            'pct_nonrecced_without_int': 1 - nonrecced_inted,\n",
    "            'ratio_int': recced_inted / nonrecced_inted,\n",
    "            'ratio_noint': (1 - recced_inted) / (1 - nonrecced_inted),\n",
    "            'mean_min_age': mean_min_age,\n",
    "            'mean_median_age': mean_median_age,\n",
    "        })\n",
    "    return cov_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores_filepath = os.path.join(config.feature_data_dir, 'mf_coverage_scores.pkl')\n",
    "with open(baseline_scores_filepath, 'rb') as scores_file:\n",
    "    scores = pickle.load(scores_file)\n",
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_metrics = compute_coverage_metrics(scores, cov_helper)\n",
    "len(coverage_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coverage_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41075873",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = pd.DataFrame(coverage_metrics)\n",
    "for r in edf.itertuples():\n",
    "    print(f\"{r.model} &  &  &  & {r.n_recced_sites} & {r.pct_unique_recs:.2%} & {r.mean_min_age:.1f} weeks & {r.pct_recced_without_int:.1%} / {r.pct_nonrecced_without_int:.1%} = {r.ratio_noint:.2f} \\\\\\\\\".replace(\"%\", \"\\\\%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = eval_df.merge(pd.DataFrame(coverage_metrics), on='model')\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print in latex table form\n",
    "for r in edf.itertuples():\n",
    "    print(f\"{r.model} & {r.mrr:.3f} & {r.hr1:.2f}% & {r.hr5:.2f}% & {r.n_recced_sites} & {r.pct_unique_recs:.1%} & {r.mean_min_age:.1f} weeks & {r.pct_recced_without_int:.1%} / {r.pct_nonrecced_without_int:.1%} = {r.ratio_noint:.2f} \\\\\\\\\".replace(\"%\", \"\\\\%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9966c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
