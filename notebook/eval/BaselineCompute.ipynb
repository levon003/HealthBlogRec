{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed89941",
   "metadata": {},
   "source": [
    "Baseline Compute\n",
    "===\n",
    "\n",
    "Code to compute test and coverage metrics for the baselines.\n",
    "\n",
    "Note this approach assumes that the baselines have no hyperparameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e48144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import dateutil.parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ad0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace packages\n",
    "import transformers\n",
    "import tokenizers\n",
    "import torch\n",
    "\n",
    "# more torch imports\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3cb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "import cbrec.genconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cbrec.genconfig.Config()\n",
    "#config.metadata_filepath += \"_old\"\n",
    "#config.feature_db_filepath += \"_old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.featuredb\n",
    "import cbrec.utils\n",
    "import cbrec.data\n",
    "import cbrec.reccontext\n",
    "import cbrec.evaluation\n",
    "import cbrec.torchmodel\n",
    "import cbrec.text.embeddingdb\n",
    "import cbrec.text.journalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53685d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.logutils\n",
    "cbrec.logutils.set_up_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80935ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off matplotlib logging\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae64c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)\n",
    "import cbcore.data.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5965439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa76e4d",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42239d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal metadata\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read interactions dataframe\n",
    "s = datetime.now()\n",
    "model_data_dir = '/home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data'\n",
    "ints_df = pd.read_feather(os.path.join(model_data_dir, 'ints_df.feather'))\n",
    "print(f\"Read {len(ints_df)} rows ({len(set(ints_df.user_id))} unique users) in {datetime.now() - s}.\")\n",
    "ints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df = ints_df.sort_values(by='created_at').drop_duplicates(subset=['user_id', 'site_id'], keep='first').copy()\n",
    "len(inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df['usp'] = [(user_id, site_id) for user_id, site_id in zip(inits_df.user_id, inits_df.site_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_usp_set = set([(user_id, site_id) for user_id, site_id in zip(journal_df.user_id, journal_df.site_id)])\n",
    "len(author_usp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b176f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inits_df = inits_df[~inits_df.usp.isin(author_usp_set)]\n",
    "len(inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f6bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e082317",
   "metadata": {},
   "source": [
    "## Create fast look-ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "usp_journal_timestamp_map = {}\n",
    "\n",
    "current_usp = None\n",
    "current_timestamp_list = []\n",
    "for row in tqdm(journal_df[journal_df.published_at > 0].sort_values(by=['user_id', 'site_id', 'published_at']).itertuples(), total=len(journal_df), desc=\"JournalIdLookup map construction\"):\n",
    "    usp = (row.user_id, row.site_id)\n",
    "    if usp != current_usp:\n",
    "        current_usp = usp\n",
    "        current_timestamp_list = []\n",
    "        usp_journal_timestamp_map[usp] = current_timestamp_list\n",
    "    current_timestamp_list.append(row.published_at)\n",
    "logging.info(f\"Translated {len(journal_df)} journals into a map of {len(usp_journal_timestamp_map)} USPs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa861c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_journal_data(usps, timestamp):\n",
    "    journal_data_list = []\n",
    "    for usp in usps:\n",
    "        journal_data = {\n",
    "            'n_recent_journals': 0,\n",
    "        }\n",
    "    return journal_data_list\n",
    "\n",
    "def get_journal_updates_before(self, usp, timestamp):\n",
    "if usp in self.usp_journal_timestamp_map:\n",
    "    timestamp_list = self.usp_journal_timestamp_map[usp]\n",
    "    end_ind = bisect.bisect_right(timestamp_list, timestamp)\n",
    "    if end_ind is None:\n",
    "        return []\n",
    "    start_ind = max(end_ind - self.config.journal_update_memory, 0)\n",
    "    journal_id_list = self.usp_journal_id_map[usp]\n",
    "    journal_ids = journal_id_list[start_ind:end_ind]\n",
    "    return journal_ids\n",
    "else:\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd17cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.compute_metrics(y_score_site_count, 'MostInitiatedWithRecently')\n",
    "#self.compute_metrics(y_score_site_recent, 'MostRecentlyInitiatedWith')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06401669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.recentActivityCounter\n",
    "initiation_counter = cbrec.recentActivityCounter.RecentActivityCounter(config.activity_count_duration_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath)]\n",
    "len(md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for md in md_list:\n",
    "    if not md['is_initiation_eligible'] and not md['is_self_initiation']:\n",
    "        c += 1\n",
    "        print(md)\n",
    "        break\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for md in md_list:\n",
    "    if md['is_initiation_eligible']:\n",
    "        c += 1\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53c9bf",
   "metadata": {},
   "source": [
    "## Compute baselines\n",
    "\n",
    "Score all test_contexts (including test and predict contexts) with the baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cbrec\n",
    "except:\n",
    "    sys.path.append(\"/home/lana/levon003/repos/recsys-peer-match/src\")\n",
    "\n",
    "import cbrec.featuredb\n",
    "import cbrec.genconfig\n",
    "import cbrec.utils\n",
    "import cbrec.evaluation\n",
    "import cbrec.reccontext\n",
    "import cbrec.recentActivityCounter\n",
    "import cbrec.modeling.text_loader\n",
    "import cbrec.modeling.reccontext_builder\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test' or md['type'] == 'predict']\n",
    "test_md_map = {md['metadata_id']: md for md in test_md_list}\n",
    "len(test_md_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34686d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath)]\n",
    "print(f\"Tracking initiations from {len(md_list)} captured initations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aceaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_END_TIMESTAMP = datetime.strptime(\"2021-07-01\", \"%Y-%m-%d\").timestamp() * 1000\n",
    "\n",
    "def compute_scores(rc, scorer, rac):\n",
    "    # produce scores for the baselines that generate scores for all source/candidate usp pairs\n",
    "    y_score_mat = scorer.get_empty_score_arr('full')\n",
    "    for j in range(y_score_mat.shape[1]):  # for each source_usp\n",
    "        start_ind = j * len(rc.candidate_usp_arr)\n",
    "        stop_ind = start_ind + len(rc.candidate_usp_arr)\n",
    "        user_feats = rc.user_pair_mat[start_ind:stop_ind,0:3]\n",
    "        y_score_mat[:,j] = user_feats.sum(axis=1)\n",
    "    y_score_site = scorer.reduce_usp_ranking_to_site(scorer.merge_multisource_rankings(y_score_mat))\n",
    "    scorer.compute_metrics(y_score_site, 'NaiveNetwork')\n",
    "\n",
    "    # produce scores for the baselines that generate scores for all candidate usp pairs\n",
    "    y_score_usp = scorer.get_empty_score_arr('merged')\n",
    "    assert y_score_usp.shape == rc.candidate_usp_mat[:,0].shape\n",
    "    y_score_usp = rc.candidate_usp_mat[:,11].copy()  # time to first update\n",
    "    y_score_usp = np.abs(y_score_usp) * -1\n",
    "    y_score_site = scorer.reduce_usp_ranking_to_site(y_score_usp)\n",
    "    scorer.compute_metrics(y_score_site, \"NewestAuthor\")\n",
    "    \n",
    "    #y_score_usp = scorer.get_empty_score_arr('merged')\n",
    "    y_score_usp = rc.candidate_usp_mat[:,3].copy()  # n_recent journal\n",
    "    y_score_site = scorer.reduce_usp_ranking_to_site(y_score_usp)\n",
    "    scorer.compute_metrics(y_score_site, \"MostJournalsRecently\")\n",
    "    \n",
    "    #y_score_usp = scorer.get_empty_score_arr('merged')\n",
    "    y_score_usp = rc.candidate_usp_mat[:,4].copy()  # time_to_most_recent journal\n",
    "    y_score_usp[y_score_usp == 0] = y_score_usp.max() + 1  # set all zeros to be the largest value\n",
    "    y_score_usp = np.abs(y_score_usp) * -1\n",
    "    y_score_site = scorer.reduce_usp_ranking_to_site(y_score_usp)\n",
    "    scorer.compute_metrics(y_score_site, \"MostRecentJournal\")\n",
    "    \n",
    "    y_score_usp = rc.candidate_usp_mat[:,[5,7,9]].sum(axis=1)  # n_recent amp + comment + guestbook\n",
    "    y_score_site = scorer.reduce_usp_ranking_to_site(y_score_usp)\n",
    "    scorer.compute_metrics(y_score_site, \"MostInteractiveAuthorRecently\")\n",
    "    \n",
    "    #user_ids = self.test_context.candidate_usp_arr[:,0]\n",
    "    #for i, user_id in enumerate(user_ids):\n",
    "    #    first_journal_timestamp = ram.get_first_journal_update_timestamp(user_id)\n",
    "    #    first_journal_timestamp = first_journal_timestamp / self.config.ms_per_hour if first_journal_timestamp is not None else np.finfo(featuredb.NUMPY_DTYPE).max\n",
    "    #    y_score_usp[i] = first_journal_timestamp\n",
    "    #y_score_usp -= ram.get_first_journal_update_timestamp(self.test_context.source_user_id) / self.config.ms_per_hour\n",
    "    #y_score_usp = np.abs(y_score_usp) * -1\n",
    "    #y_score_site = self.reduce_usp_ranking_to_site(y_score_usp)\n",
    "    #scorer.compute_metrics(y_score_site, \"ClosestToStart\")\n",
    "    \n",
    "    # note: probably don't implement this one (a global counter of the most-initiated-with sites...)\n",
    "    #y_score_usp = self.compute_MostInitiatedWith()\n",
    "    #y_score_site = self.reduce_usp_ranking_to_site(y_score_usp)\n",
    "    #self.compute_metrics(y_score_site, 'MostInitiatedWith')\n",
    "\n",
    "    # produce scores for the baselines that generate scores for all sites\n",
    "    #y_score_site_count, y_score_site_recent = self.compute_MostInitiatedWithRecently(activity_manager)\n",
    "    #self.compute_metrics(y_score_site_count, 'MostInitiatedWithRecently')\n",
    "    #self.compute_metrics(y_score_site_recent, 'MostRecentlyInitiatedWith')    \n",
    "    \n",
    "    y_score_site_count, y_score_site_recent = get_scores_from_site_counter(scorer, rac)\n",
    "    scorer.compute_metrics(y_score_site_count, 'MostInitiatedWithRecently')\n",
    "    scorer.compute_metrics(y_score_site_recent, 'MostRecentlyInitiatedWith')\n",
    "    \n",
    "    # Random baseline\n",
    "    y_score_site_random = scorer.get_empty_score_arr('reduced')\n",
    "    y_score_site_random = config.rng.uniform(0, 1, size=y_score_site_random.shape)\n",
    "    scorer.compute_metrics(y_score_site_random, 'Random')\n",
    "\n",
    "\n",
    "def get_scores_from_site_counter(scorer, rac):\n",
    "    \"\"\"\n",
    "    Generate y_score_site arrays from the given RecentActivityCounter, which is assumed to be tracking site_ids.\n",
    "\n",
    "    :rac -- recentActivityCounter.RecentActivityCounter\n",
    "\n",
    "    :returns\n",
    "        y_score_site_count -- count of recent activity\n",
    "        y_score_site_recent -- number of seconds to current timestamp\n",
    "    \"\"\"\n",
    "    y_score_site_count = scorer.get_empty_score_arr('reduced')\n",
    "    y_score_site_recent = scorer.get_empty_score_arr('reduced')\n",
    "    no_recent_score = scorer.test_context.timestamp / scorer.config.ms_per_hour\n",
    "    for i, site_id in enumerate(scorer.site_id_arr):\n",
    "        n_recent = rac.get_count(site_id)\n",
    "        if n_recent > 0:\n",
    "            most_recent = rac.get_most_recent_activity(site_id)\n",
    "            if most_recent is None:\n",
    "                \n",
    "            time_to_most_recent = scorer.test_context.timestamp - most_recent\n",
    "            # convert difference from ms to hours\n",
    "            time_to_most_recent /= scorer.config.ms_per_hour\n",
    "            #time_to_most_recent *= -1  # invert most recent, so that the highest possible value is 0 and the lowest possible value is self.test_context.timestamp\n",
    "        else:\n",
    "            time_to_most_recent = no_recent_score\n",
    "\n",
    "        y_score_site_count[i] = n_recent\n",
    "        y_score_site_recent[i] = time_to_most_recent\n",
    "    y_score_site_recent *= -1\n",
    "    return y_score_site_count, y_score_site_recent\n",
    "    \n",
    "config = cbrec.genconfig.Config()\n",
    "\n",
    "db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "with db:\n",
    "    baseline_test_filepath = os.path.join(config.feature_data_dir, 'baseline_metrics.ndjson')\n",
    "    baseline_scores_filepath = os.path.join(config.feature_data_dir, 'baseline_coverage_scores.pkl')\n",
    "    scores = []\n",
    "    with open(baseline_test_filepath, 'w') as metrics_outfile, open(baseline_scores_filepath, 'wb') as scores_outfile:\n",
    "        curr_timestamp = 0\n",
    "        md_list_counter = 0\n",
    "        initiation_counter = cbrec.recentActivityCounter.RecentActivityCounter(config.activity_count_duration_ms)\n",
    "        \n",
    "        for test_context in tqdm(cbrec.featuredb.stream_test_contexts(db, config), desc='Streaming test contexts', total=len(test_md_map)):\n",
    "            test_context_md = test_md_map[test_context['metadata_id']]\n",
    "            if test_context_md['timestamp'] > curr_timestamp:\n",
    "                curr_timestamp = test_context_md['timestamp']\n",
    "                while md_list_counter < len(md_list) and md_list[md_list_counter]['timestamp'] < curr_timestamp:\n",
    "                    md = md_list[md_list_counter]\n",
    "                    if md['has_target'] and not md['is_self_initiation']:\n",
    "                    #if md['is_initiation_eligible']:\n",
    "                        initiation_counter.add_interaction(md['target_site_id'], md['timestamp'])\n",
    "                    md_list_counter += 1\n",
    "\n",
    "            rc = cbrec.reccontext.RecContext.create_from_test_context(config, test_context_md, test_context)\n",
    "            has_target = rc.md['has_target']\n",
    "\n",
    "            if has_target and md['timestamp'] <= VALIDATION_END_TIMESTAMP:\n",
    "                continue  # don't process validation timestamps\n",
    "            \n",
    "            save_scores = not has_target  # save scores if this is a prediction target\n",
    "            scorer = cbrec.evaluation.Scorer(config, rc, save_scores=save_scores)\n",
    "\n",
    "            if has_target:\n",
    "                initiation_counter.update_counts(curr_timestamp)\n",
    "            compute_scores(rc, scorer, initiation_counter)\n",
    "\n",
    "            rc.md['metrics'] = scorer.metrics_dict\n",
    "            if save_scores:\n",
    "                rc.md['scores'] = scorer.scores_dict # {key: list(value) for key, value in scorer.scores_dict.items()}\n",
    "                scores.append(rc.md)\n",
    "                if len(scores) == 1000:\n",
    "                    pickle.dump(scores, scores_outfile)\n",
    "                    logging.info(f\"Saved pickle with {len(scores)} scores.\")\n",
    "                    scores = []\n",
    "                #scores[rc.metadata_id] = scorer.scores_dict\n",
    "                #line = json.dumps(rc.md) + \"\\n\"\n",
    "                #scores_outfile.write(line)\n",
    "            else:\n",
    "                line = json.dumps(rc.md) + \"\\n\"\n",
    "                metrics_outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /home/lana/shared/caringbridge/data/projects/recsys-peer-match/feature_data/baseline_metrics.ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list[md_list_counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e00c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_END_TIMESTAMP = datetime.strptime(\"2021-07-01\", \"%Y-%m-%d\").timestamp() * 1000\n",
    "md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test' or md['type'] == 'predict']\n",
    "test_md_list = [md for md in md_list if md['has_target'] and md['timestamp'] > VALIDATION_END_TIMESTAMP]\n",
    "print(len(test_md_list))\n",
    "test_metadata_ids = set([md['metadata_id'] for md in test_md_list])\n",
    "len(test_metadata_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483db927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "target_ranks = defaultdict(list)\n",
    "baseline_test_filepath = os.path.join(config.feature_data_dir, 'baseline_metrics.ndjson')\n",
    "with open(baseline_test_filepath, 'r') as metadata_file:\n",
    "    for line in tqdm(metadata_file, total=len(test_md_list), desc='Reading baseline metrics', disable=False):\n",
    "        md = json.loads(line)\n",
    "        if md['metadata_id'] not in test_metadata_ids:\n",
    "            continue\n",
    "        metrics_dict = md[\"metrics\"]\n",
    "        for model_name, metrics in metrics_dict.items():\n",
    "            target_rank = metrics['target_rank']\n",
    "            target_ranks[model_name].append(target_rank)\n",
    "assert len(target_ranks) > 0\n",
    "print(target_ranks.keys())\n",
    "len(target_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for model_name in target_ranks.keys():\n",
    "    ranks = np.array(target_ranks[model_name])\n",
    "\n",
    "    mrr = (1 / ranks).mean()\n",
    "    hr1 = (ranks == 1).sum() / len(ranks) * 100\n",
    "    hr5 = (ranks <= 5).sum() / len(ranks) * 100\n",
    "    d.append({\n",
    "        'model': model_name,\n",
    "        'n': len(ranks),\n",
    "        'mrr': mrr,\n",
    "        'hr1': hr1,\n",
    "        'hr5': hr5,\n",
    "    })\n",
    "eval_df = pd.DataFrame(d).sort_values(by='mrr', ascending=False)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageHelper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "# load cov_helper from pickle\n",
    "coverage_dir = \"/home/lana/shared/caringbridge/data/projects/recsys-peer-match/feature_data/coverage\"\n",
    "with open(os.path.join(coverage_dir, 'cov_helper.pkl'), 'rb') as coverage_helper_file:\n",
    "    cov_helper = pickle.load(coverage_helper_file)\n",
    "cov_helper.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cov_helper.sites_with_previous_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d89d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_ties(site_id_arr, y_score_site, sort_inds):\n",
    "    \"\"\"\n",
    "    This implementation is terrible, although I believe it works.\n",
    "    \"\"\"\n",
    "    highest_scores = []\n",
    "    highest_score_site_ids = []\n",
    "    n_ties_broken = 0\n",
    "    i = 0\n",
    "    while len(highest_scores) < 5:\n",
    "        i += 1\n",
    "        score = y_score_site[sort_inds[-i]]\n",
    "        if score == y_score_site[sort_inds[-(i+1)]]:\n",
    "            inds = np.flatnonzero(y_score_site == score)\n",
    "            n_remaining = 5 - len(highest_scores)\n",
    "            if len(inds) <= n_remaining:\n",
    "                highest_scores.extend([score,]*len(inds))\n",
    "                highest_score_site_ids.extend(site_id_arr[inds])\n",
    "                assert len(highest_scores) == len(highest_score_site_ids)\n",
    "                i += len(inds) - 1\n",
    "            else:\n",
    "                highest_scores.extend([score,]*n_remaining)\n",
    "                subset_inds = np.random.choice(inds, size=n_remaining, replace=False)\n",
    "                highest_score_site_ids.extend(site_id_arr[subset_inds])\n",
    "                assert len(highest_scores) == len(highest_score_site_ids)\n",
    "                n_ties_broken += 1\n",
    "        else:\n",
    "            highest_scores.append(score)\n",
    "            highest_score_site_ids.append(site_id_arr[sort_inds[-i]])\n",
    "            assert len(highest_scores) == len(highest_score_site_ids)\n",
    "        if len(highest_scores) == 5:\n",
    "            break\n",
    "    return np.array(highest_scores), np.array(highest_score_site_ids), n_ties_broken > 0\n",
    "\n",
    "def compute_coverage_metrics(model_coverage_scores, cov_helper):\n",
    "    model_recs = defaultdict(list)\n",
    "    n_ties_broken = 0\n",
    "    for scores_md in model_coverage_scores:\n",
    "        metadata_id = scores_md['metadata_id']\n",
    "        site_id_arr = cov_helper.site_id_arr_map[metadata_id]\n",
    "        for model_name, y_score_site in scores_md['scores'].items():\n",
    "        \n",
    "            assert y_score_site.shape == site_id_arr.shape\n",
    "\n",
    "            # create rec batch\n",
    "            sort_inds = np.argsort(y_score_site)\n",
    "            # TODO need to compute ranks if there are ties; for now, we'll assume there aren't any ties\n",
    "            # in the case of ties, not clear what order argsort prefers\n",
    "            #ranks = rankdata(-1 * y_score_site, method='max')\n",
    "\n",
    "            highest_scores = y_score_site[sort_inds[-(cov_helper.n+1):]]\n",
    "            if len(set(highest_scores)) != len(highest_scores):\n",
    "                highest_scores, highest_score_site_ids, ties_broken = break_ties(site_id_arr, y_score_site, sort_inds)\n",
    "                if not np.all(highest_scores == np.flip(y_score_site[sort_inds[-5:]])):\n",
    "                    print(highest_scores)\n",
    "                    print(y_score_site[sort_inds[-5:]])\n",
    "                    return y_score_site\n",
    "                if ties_broken:\n",
    "                    n_ties_broken += 1\n",
    "            else:\n",
    "                #highest_scores = y_score_site[sort_inds[-cov_helper.n:]]\n",
    "                highest_score_site_ids = site_id_arr[sort_inds[-cov_helper.n:]]\n",
    "            model_recs[model_name].append(list(highest_score_site_ids))\n",
    "    print(f\"{n_ties_broken=}\")\n",
    "    \n",
    "    cov_data = []\n",
    "    for model_name, recs in model_recs.items():\n",
    "        recced_sites = set()\n",
    "        for rec in recs:\n",
    "            recced_sites.update(rec)\n",
    "        nonrecced_sites = cov_helper.eligible_sites - recced_sites\n",
    "\n",
    "        recced_inted = len(recced_sites & cov_helper.sites_with_previous_ints) / len(recced_sites)\n",
    "        nonrecced_inted = len(nonrecced_sites & cov_helper.sites_with_previous_ints) / len(nonrecced_sites)\n",
    "\n",
    "        site_ages = []\n",
    "        for rec in recs:\n",
    "            ages = np.array([cov_helper.timestamp - cov_helper.site_first_journal_timestamp_map[site_id] for site_id in rec])\n",
    "            ages = ages / 1000 / 60 / 60 / 24 / 7  # convert to weeks\n",
    "            assert np.all(ages > 0)\n",
    "            site_ages.append({\n",
    "                'min': ages.min(),\n",
    "                #'mean': ages.mean(),\n",
    "                #'std': ages.std(),\n",
    "                'median': np.median(ages),\n",
    "                #'max': ages.max(),\n",
    "            })\n",
    "        mean_min_age = np.mean([a['min'] for a in site_ages])\n",
    "        mean_median_age = np.mean([a['median'] for a in site_ages])\n",
    "\n",
    "        cov_data.append({\n",
    "            'model': model_name,\n",
    "            'n_recced_sites': len(recced_sites),\n",
    "            'n_nonrecced_sites': len(nonrecced_sites),\n",
    "            'pct_eligible_recced': len(recced_sites) / len(cov_helper.eligible_sites),\n",
    "            'pct_unique_recs': len(recced_sites) / (5 * 1000),\n",
    "            'pct_recced_with_int': recced_inted,\n",
    "            'pct_nonrecced_with_int': nonrecced_inted,\n",
    "            'pct_recced_without_int': 1 - recced_inted,\n",
    "            'pct_nonrecced_without_int': 1 - nonrecced_inted,\n",
    "            'ratio_int': recced_inted / nonrecced_inted,\n",
    "            'ratio_noint': (1 - recced_inted) / (1 - nonrecced_inted),\n",
    "            'mean_min_age': mean_min_age,\n",
    "            'mean_median_age': mean_median_age,\n",
    "        })\n",
    "    return cov_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3dd046",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores_filepath = os.path.join(config.feature_data_dir, 'baseline_coverage_scores.pkl')\n",
    "with open(baseline_scores_filepath, 'rb') as scores_file:\n",
    "    scores = pickle.load(scores_file)\n",
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_metrics = compute_coverage_metrics(scores, cov_helper)\n",
    "len(coverage_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coverage_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = eval_df.merge(pd.DataFrame(coverage_metrics), on='model')\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print in latex table form\n",
    "for r in edf.itertuples():\n",
    "    print(f\"{r.model} & {r.mrr:.3f} & {r.hr1:.2f}% & {r.hr5:.2f}% & {r.n_recced_sites} & {r.pct_unique_recs:.1%} & {r.mean_min_age:.1f} weeks & {r.pct_recced_without_int:.1%} / {r.pct_nonrecced_without_int:.1%} = {r.ratio_noint:.2f} \\\\\\\\\".replace(\"%\", \"\\\\%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb3d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpuonly",
   "language": "python",
   "name": "pytorch-cpuonly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
