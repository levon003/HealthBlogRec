{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35badd4d",
   "metadata": {},
   "source": [
    "PyTorch Training\n",
    "===\n",
    "\n",
    "Experiments with PyTorch optimization of rec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e13791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82319d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import dateutil.parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace packages\n",
    "import transformers\n",
    "import tokenizers\n",
    "import torch\n",
    "\n",
    "# more torch imports\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(git_root_dir, 'src'))\n",
    "import cbrec.genconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cbrec.genconfig.Config()\n",
    "#config.metadata_filepath += \"_old\"\n",
    "#config.feature_db_filepath += \"_old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.featuredb\n",
    "import cbrec.utils\n",
    "import cbrec.data\n",
    "import cbrec.reccontext\n",
    "import cbrec.evaluation\n",
    "import cbrec.torchmodel\n",
    "import cbrec.text.embeddingdb\n",
    "import cbrec.text.journalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bac88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.logutils\n",
    "cbrec.logutils.set_up_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off matplotlib logging\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d247eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "caringbridge_core_path = \"/home/lana/levon003/repos/caringbridge_core\"\n",
    "sys.path.append(caringbridge_core_path)\n",
    "import cbcore.data.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e495f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87dd2a67",
   "metadata": {},
   "source": [
    "## One-time startup: identfying journal_oids\n",
    "\n",
    "### Training triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7587e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.feature_loader\n",
    "fl = cbrec.feature_loader.FeatureLoader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jil = fl.journal_id_lookup\n",
    "\n",
    "journal_oids = set()\n",
    "n_not_enough_source = 0\n",
    "n_not_enough_target = 0\n",
    "n_not_enough_alt = 0\n",
    "n_skipped = 0\n",
    "db = cbrec.featuredb.get_db_by_filepath(fl.config.feature_db_filepath)\n",
    "with db:\n",
    "    for triple_dict in tqdm(cbrec.featuredb.stream_triples(db), desc='Streaming train triples'):\n",
    "        source_usp = (triple_dict['source_user_id'], triple_dict['source_site_id'])\n",
    "        target_usp = (triple_dict['target_user_id'], triple_dict['target_site_id'])\n",
    "        alt_usp = (triple_dict['alt_user_id'], triple_dict['alt_site_id'])\n",
    "        source_journal_ids = jil.get_journal_updates_before(source_usp, triple_dict['interaction_timestamp'])\n",
    "        target_journal_ids = jil.get_journal_updates_before(target_usp, triple_dict['interaction_timestamp'])\n",
    "        alt_journal_ids = jil.get_journal_updates_before(alt_usp, triple_dict['interaction_timestamp'])\n",
    "        error = False\n",
    "        if len(source_journal_ids) < 3:\n",
    "            n_not_enough_source += 1\n",
    "            error = True\n",
    "        if len(target_journal_ids) < 3:\n",
    "            n_not_enough_target += 1\n",
    "            error = True\n",
    "        if len(alt_journal_ids) < 3:\n",
    "            n_not_enough_alt += 1\n",
    "            error = True\n",
    "        if not error:\n",
    "            journal_oids.update(source_journal_ids)\n",
    "            journal_oids.update(target_journal_ids)\n",
    "            journal_oids.update(alt_journal_ids)\n",
    "        else:\n",
    "            n_skipped += 1\n",
    "logging.info(f\"{len(journal_oids)} journal ids identified for training triples.\")\n",
    "logging.info(f\"Skipped {n_skipped} triples with insufficient journals available. (source missing = {n_not_enough_source}; target missing = {n_not_enough_target}; alt missing = {n_not_enough_alt})\")\n",
    "required_journal_ids_filepath = os.path.join(config.model_data_dir, 'train_journal_oids.txt')\n",
    "with open(required_journal_ids_filepath, 'w') as outfile:\n",
    "    for journal_oid in journal_oids:\n",
    "        outfile.write(journal_oid + \"\\n\")\n",
    "logging.info(f\"Wrote {len(journal_oids)} journal ids to '{required_journal_ids_filepath}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b400f",
   "metadata": {},
   "source": [
    "### Test RecContexts\n",
    "\n",
    "Note: this takes about 40 minutes. It adds all required journal updates to the file.\n",
    "\n",
    "Note that eval processes assume the availability of these texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test' or md['type'] == 'predict']\n",
    "test_md_map = {md['metadata_id']: md for md in test_md_list}\n",
    "\n",
    "required_journal_ids = set()\n",
    "n_invalid = 0\n",
    "n_error = 0\n",
    "\n",
    "db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "with db:\n",
    "    for test_context in tqdm(cbrec.featuredb.stream_test_contexts(db, config), desc='Streaming test contexts', total=33592):\n",
    "        test_context_md = test_md_map[test_context['metadata_id']]\n",
    "        interaction_timestamp = int(test_context_md['timestamp'])\n",
    "        source_usp_arr = test_context['source_usp_arr']\n",
    "        source_usps = [(source_usp_arr[i,0], source_usp_arr[i,1]) for i in range(source_usp_arr.shape[0])]\n",
    "        candidate_usp_arr = test_context['candidate_usp_arr']\n",
    "        candidate_usps = [(candidate_usp_arr[i,0], candidate_usp_arr[i,1]) for i in range(candidate_usp_arr.shape[0])]\n",
    "        error = False\n",
    "        for usp in source_usps + candidate_usps:\n",
    "            journal_ids = fl.journal_id_lookup.get_journal_updates_before(usp, interaction_timestamp)\n",
    "            if len(journal_ids) < 3:\n",
    "                n_invalid += 1\n",
    "                error = True\n",
    "            else:\n",
    "                required_journal_ids.update(journal_ids)\n",
    "        if error:\n",
    "            n_error += 1\n",
    "logging.info(f\"Identified {len(required_journal_ids)}, of which {n_error} had 1+ errors (with {n_invalid} total errors).\")\n",
    "\n",
    "required_journal_ids_filepath = os.path.join(config.model_data_dir, 'test_journal_oids.txt')\n",
    "with open(required_journal_ids_filepath, 'w') as outfile:\n",
    "    for journal_oid in required_journal_ids:\n",
    "        outfile.write(journal_oid + \"\\n\")\n",
    "logging.info(f\"Wrote {len(required_journal_ids)} journal ids to '{required_journal_ids_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d778996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ca729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "482f5b46",
   "metadata": {},
   "source": [
    "## Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09071087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00770884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.feature_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c803169",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = cbrec.feature_loader.FeatureLoader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca149b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true, missing_journal_id_list = fl.get_pointwise_training_triples()\n",
    "# 2021-09-30 11:43:20,364 - cbrec.feature_loader.FeatureLoader.get_input_arrs_from_triple_dicts - DEBUG - After processing 254776 triple dicts, identified 226 invalid (and an additional 4 invalid due to missing text features)\n",
    "X.shape, y_true.shape, len(missing_journal_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y_true.shape, len(missing_journal_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91170307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train features\n",
    "feature_cache_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'torch_experiments', 'feature_cache')\n",
    "with open(os.path.join(feature_cache_dir, 'X_train_raw.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(X, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(feature_cache_dir, 'y_train_raw.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(y_true, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57930e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42284cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0bcf4b8",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_list = [md for md in cbrec.utils.stream_metadata_list(config.metadata_filepath) if md['type'] == 'test']\n",
    "len(test_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe0c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2e35c89",
   "metadata": {},
   "source": [
    "### Create test2train triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645aa222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only read entries >= first_metadata_id\n",
    "# the first_metadata_id should be the name of the checkpoint that started generating the \n",
    "# note for Sept 22: 734780 - 847406 should be the metadata_ids generated during the test period\n",
    "first_metadata_id = 734780\n",
    "timestamp = 0\n",
    "test2train_md_list = []\n",
    "for md in test_md_list:\n",
    "    if md['metadata_id'] < first_metadata_id:\n",
    "        continue\n",
    "    if md['metadata_id'] >= 863252 and md['metadata_id'] < 865053:\n",
    "        continue\n",
    "    if md['timestamp'] < timestamp and len(test2train_md_list) < 1000:\n",
    "        print(f\"reset, dropping {len(test2train_md_list)} test contexts\")\n",
    "        test2train_md_list = []\n",
    "    timestamp = md['timestamp']\n",
    "    test2train_md_list.append(md)\n",
    "len(test2train_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2train_md_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.utcfromtimestamp(test2train_md_list[0]['timestamp'] / 1000).isoformat(), datetime.utcfromtimestamp(test2train_md_list[-1]['timestamp'] / 1000).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804b89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally: 36307 triples, 223979 required journals\n",
    "triple_dicts, required_journal_ids = fl.create_train_triples_from_test_contexts(test2train_md_list)\n",
    "len(triple_dicts), len(required_journal_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config.model_data_dir, 'test2train_triple_dicts.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(triple_dicts, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(config.model_data_dir, 'test2train_required_journal_oids.txt'), 'w') as outfile:\n",
    "    for journal_oid in required_journal_ids:\n",
    "        outfile.write(str(journal_oid) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d17c3a",
   "metadata": {},
   "source": [
    "Can top up on the fly:\n",
    "\n",
    "````\n",
    "python cbrec/text/createTextFeatureSqlite.py --text-id-txt /home/lana/shared/caringbridge/data/projects/recsys-peer-match/model_data/test2train_required_journal_oids.txt --n-processes 3\n",
    "````\n",
    "\n",
    "OR\n",
    "\n",
    "````\n",
    "sbatch -p amdsmall make_text_features_test2train.sh\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config.model_data_dir, 'test2train_triple_dicts.pkl'), 'rb') as infile:\n",
    "    triple_dicts = pickle.load(infile)\n",
    "len(triple_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_arrs, ys, missing_journal_id_list = fl.get_input_arrs_from_triple_dicts(triple_dicts)\n",
    "y_true = np.array(ys)\n",
    "if len(feature_arrs) > 0:\n",
    "    X = np.vstack(feature_arrs)\n",
    "else:\n",
    "    X = np.array(feature_arrs)\n",
    "X.shape, y_true.shape, len(missing_journal_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train features\n",
    "feature_cache_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'torch_experiments', 'feature_cache')\n",
    "with open(os.path.join(feature_cache_dir, 'X_test2train_raw.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(X, outfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(feature_cache_dir, 'y_test2train_raw.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(y_true, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train features\n",
    "feature_cache_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'torch_experiments', 'feature_cache')\n",
    "filenames = [\n",
    "    ('X_train_raw.pkl', 'y_train_raw.pkl'),\n",
    "    ('X_test2train_raw.pkl', 'y_test2train_raw.pkl'),\n",
    "]\n",
    "Xs = []\n",
    "ys = []\n",
    "for x_filename, y_filename in filenames:\n",
    "    with open(os.path.join(feature_cache_dir, x_filename), 'rb') as infile:\n",
    "        X = pickle.load(infile)\n",
    "        Xs.append(X)\n",
    "    with open(os.path.join(feature_cache_dir, y_filename), 'rb') as infile:\n",
    "        y = pickle.load(infile)\n",
    "        ys.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(Xs, axis=0)\n",
    "y_true = np.concatenate(ys, axis=0)\n",
    "X.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d80114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "inds = np.arange(len(X))\n",
    "np.random.shuffle(inds)\n",
    "X = X[inds]\n",
    "y_true = y_true[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ebb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbrec.modeling.modelconfig\n",
    "import cbrec.modeling.scorer\n",
    "import cbrec.modeling.manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30be772",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = cbrec.modeling.modelconfig.ModelConfig()\n",
    "model_config.train_n_epochs = 700\n",
    "model_config.experiment_name = 'main'\n",
    "model_config.train_weight_decay = 0.0001\n",
    "model_config.LinearNet_dropout_p = 0.5\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ef607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = cbrec.modeling.manager.ModelManager(model_config, config=config)\n",
    "model_manager.model_config.output_basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_manager.train_model(X, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager.model_trainer.load_model_state_dict(description='e560')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1202a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = cbrec.modeling.manager.ModelManager.load_from_model_name('LinearNet', 'main')\n",
    "model_manager.load_model(load_training_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager.model_trainer.load_model_state_dict(description='e750')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57036a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've loaded the saved model data, we can \n",
    "train_metrics, test_metrics = model_manager.model_trainer.get_train_metrics()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "xs = test_metrics.T[:,0]\n",
    "ys = test_metrics.T[:,1]\n",
    "ax.plot(xs, ys, label='Test')\n",
    "\n",
    "xs = train_metrics.T[:,0]\n",
    "ys = train_metrics.T[:,1]\n",
    "ax.plot(xs, ys, label='Train')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16db2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99af72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get the interaction_timestamp of one of the metadata ids generated last week\n",
    "# later edit: why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100894af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_metadata_id = 866854\n",
    "last_metadata_id = 866933\n",
    "md_list = []\n",
    "for md in cbrec.utils.stream_metadata_list(config.metadata_filepath):\n",
    "    if md['type'] != 'predict':\n",
    "        continue\n",
    "    metadata_id = md['metadata_id']\n",
    "    if metadata_id >= first_metadata_id and metadata_id <= last_metadata_id:\n",
    "        md_list.append(md)\n",
    "len(md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb44450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get participant data\n",
    "participant_id_filepath = os.path.join(git_root_dir, 'data/email/participant_ids.tsv')\n",
    "participant_df = pd.read_csv(participant_id_filepath, sep='\\t', header=0)\n",
    "print(len(participant_df))\n",
    "participant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify unsubscribed people\n",
    "email_address = 'unsubscribed@example.com'\n",
    "participant_df[participant_df.real_email_address == email_address]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any unsubscribed people\n",
    "new_md_list = []\n",
    "for md in md_list:\n",
    "    if md['source_user_id'] in [0, 0, 0, 0, 0, 0]:\n",
    "        continue\n",
    "    new_md_list.append(md)\n",
    "md_list = new_md_list\n",
    "len(md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e3f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdfd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595101f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict which sites are considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the site data\n",
    "s = datetime.now()\n",
    "site_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/site_metadata\"\n",
    "site_metadata_filepath = os.path.join(site_metadata_dir, \"site_metadata.feather\")\n",
    "site_df = pd.read_feather(site_metadata_filepath)\n",
    "print(f\"Read {len(site_df)} site_df rows in {datetime.now() - s}.\")\n",
    "site_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the journal metadata\n",
    "s = datetime.now()\n",
    "journal_metadata_dir = \"/home/lana/shared/caringbridge/data/derived/journal_metadata\"\n",
    "journal_metadata_filepath = os.path.join(journal_metadata_dir, \"journal_metadata.feather\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0338547",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df.published_at.notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0217209",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.strptime('2021-05-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "start_timestamp = int(start_time.timestamp() * 1000)\n",
    "end_time = datetime.strptime('2021-12-01', '%Y-%m-%d').replace(tzinfo=pytz.UTC)\n",
    "end_timestamp = int(end_time.timestamp() * 1000)\n",
    "sdf = journal_df[(journal_df.created_at >= start_timestamp)&(journal_df.created_at <= end_timestamp)]\n",
    "\n",
    "curr_time = start_time\n",
    "bins = []\n",
    "while curr_time < end_time:\n",
    "    bins.append(int(curr_time.timestamp() * 1000))\n",
    "    curr_time += relativedelta(days=1)\n",
    "bins.append(int(curr_time.timestamp() * 1000))\n",
    "print(f'{len(bins)} bins from {start_time} to {end_time}')\n",
    "print(f'(actual from {datetime.utcfromtimestamp(bins[0] / 1000)} to {datetime.utcfromtimestamp(bins[-1] / 1000)})')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 2))\n",
    "\n",
    "x = sdf.created_at\n",
    "total_counts, bin_edges = np.histogram(x, bins=bins)\n",
    "ax.plot(bin_edges[:-1], total_counts, linestyle='-', linewidth=2)\n",
    "\n",
    "ax.set_title(f\"{len(sdf):,} journals\")\n",
    "\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, y: f\"{datetime.utcfromtimestamp(x / 1000).strftime('%m-%d')}\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_journal = journal_df.groupby('site_id').published_at.max()\n",
    "len(most_recent_journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e73ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_journal.value_counts(dropna=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we allow only sites that have had a journal update in the last 12 days (to account for the delay between generating and sending out)\n",
    "recency_required_days = 12\n",
    "required_recent_journal_timestamp = int(datetime.strptime('2021-11-24 08:55', '%Y-%m-%d %H:%M').timestamp() * 1000) - (recency_required_days * 1000 * 60 * 60 * 24)\n",
    "print(required_recent_journal_timestamp)\n",
    "\n",
    "# otherwise, we insist on low-privacy, searchable, non-spam sites\n",
    "eligible_site_df = site_df[(~site_df.isDeactivated)&(site_df.privacy == 'low')&(site_df.isGoogleable == '1')&(site_df.isSearchable == '1')]\n",
    "print(len(eligible_site_df))\n",
    "\n",
    "sites_with_recent_updates = set(most_recent_journal[most_recent_journal >= required_recent_journal_timestamp].index)\n",
    "eligible_site_df = eligible_site_df[eligible_site_df.site_id.isin(sites_with_recent_updates)]\n",
    "print(len(eligible_site_df))\n",
    "\n",
    "eligible_site_ids = set(eligible_site_df.site_id)\n",
    "len(eligible_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7032ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(most_recent_journal >= required_recent_journal_timestamp), len(sites_with_recent_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ef034",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl.rec_input_matrix_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0304881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(md_list))\n",
    "rc_list = fl.get_reccontexts_from_test_contexts(md_list, site_allowlist=eligible_site_ids)\n",
    "len(rc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fab209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5224d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for rc in rc_list:\n",
    "    scorer = model_manager.score_reccontext(rc)\n",
    "    predictions.append(scorer)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fee8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map of participant_id -> list of (site_id, score) tuples\n",
    "site_scores_map = {}\n",
    "for rc, scorer in zip(rc_list, predictions):\n",
    "    #rc = prediction[0]\n",
    "    #scorer = prediction[1]\n",
    "    \n",
    "    participant_id = rc.source_user_id\n",
    "    site_id_arr = scorer.site_id_arr\n",
    "    y_score_site = scorer.scores_dict[model_manager.model_config.model_name]\n",
    "    site_scores_map[participant_id] = [(int(site_id), float(score)) for site_id, score in zip(site_id_arr, y_score_site)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92033f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].scores_dict[model_manager.model_config.model_name].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id_arr = scorer.site_id_arr\n",
    "y_score_site = scorer.scores_dict[model_manager.model_config.model_name]\n",
    "\n",
    "sort_inds = np.argsort(y_score_site)\n",
    "\n",
    "ranks = rankdata(-1 * y_score_site, method='max')\n",
    "\n",
    "n=10\n",
    "highest_scores = y_score_site[sort_inds[-n:]]\n",
    "highest_score_site_ids = site_id_arr[sort_inds[-n:]]\n",
    "for site_id, score in zip(highest_score_site_ids, highest_scores):\n",
    "    print(f\"{site_id:>10} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66aaea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e740cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arrs = []\n",
    "for scorer in predictions:\n",
    "    pred_arr = scorer.scores_dict[model_manager.model_config.model_name]\n",
    "    pred_arrs.append(pred_arr)\n",
    "len(pred_arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "for pred_arr in pred_arrs:\n",
    "    ys = sorted(list(pred_arr), reverse=True)\n",
    "    xs = range(len(ys))\n",
    "    ax.plot(xs, ys, linestyle='-', color='black', alpha=0.2)\n",
    "    \n",
    "ax.set_title(f\"Distribution of predicted scores for ~{np.mean([len(pred_arr) for pred_arr in pred_arrs]):,.1f} sites\")\n",
    "ax.set_xlabel(\"Site rank\")\n",
    "ax.set_ylabel(\"Site score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efe40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_rank_dict = {}\n",
    "for scorer in predictions:\n",
    "    site_id_arr = scorer.site_id_arr\n",
    "    y_score_site = scorer.scores_dict[model_manager.model_config.model_name]\n",
    "    sort_inds = np.argsort(y_score_site)\n",
    "    ranks = rankdata(-1 * y_score_site, method='max')\n",
    "    for site_id, rank in zip(site_id_arr, ranks):\n",
    "        if site_id not in site_rank_dict:\n",
    "            site_rank_dict[site_id] = []\n",
    "        site_ranks = site_rank_dict[site_id]\n",
    "        site_ranks.append(rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6878ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable fast look-ups of site features\n",
    "site_index = site_df.set_index('site_id')\n",
    "site_index.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for site_id, ranks in site_rank_dict.items():\n",
    "    ranks = np.array(ranks)\n",
    "    n_top_appearances = [site_id, site_index.at[site_id, 'name']] + [np.sum(ranks <= i) for i in [1, 5, 10]]\n",
    "    mean_rank = f\"{np.mean(ranks):.1f}\"\n",
    "    n_visits = site_index.at[site_id, 'visits']\n",
    "    n_top_appearances += [mean_rank, n_visits,]\n",
    "    s.append(n_top_appearances)\n",
    "s.sort(key = lambda t: t[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "s[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest-ranking are spam and relatively inactive users\n",
    "sorted(s, key=lambda t: float(t[-2]), reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eligible sites with fewer than 1918 visits are in the bottom-25% by visit count\n",
    "np.quantile([t[-1] for t in s], 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array([t[-1] for t in s]) < 1000) / len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f529fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array([t[-1] for t in s]) < 10000) / len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that there are no ties\n",
    "def get_ties(ranks, y_score_site):\n",
    "    #unique, counts = np.unique(ranks, return_counts=True)\n",
    "    #return np.sum(counts > 1)\n",
    "    return len(y_score_site) - len(ranks)\n",
    "ties_list = []\n",
    "for rc, scorer in zip(rc_list, predictions):\n",
    "    participant_id = rc.source_user_id\n",
    "    site_id_arr = scorer.site_id_arr\n",
    "    y_score_site = scorer.scores_dict[model_manager.model_config.model_name]\n",
    "    sort_inds = np.argsort(y_score_site)\n",
    "    ranks = rankdata(-1 * y_score_site, method='max')\n",
    "    n_ties = get_ties(ranks, y_score_site)\n",
    "    ties_list.append(n_ties)\n",
    "ties_list = np.array(ties_list)\n",
    "np.mean(ties_list), np.sum(ties_list > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in recommendations from previous rounds\n",
    "participant_data_dir = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant')\n",
    "\n",
    "d = []\n",
    "for batch_id in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    participant_data_filepath = os.path.join(participant_data_dir, f'participant_rec_data_b{batch_id}.ndjson')\n",
    "    with open(participant_data_filepath, 'r') as infile:\n",
    "        for line in infile:\n",
    "            participant = json.loads(line)\n",
    "            del participant['site_scores']\n",
    "            participant['batch_id'] = batch_id\n",
    "            d.append(participant)\n",
    "\n",
    "batch_df = pd.DataFrame(d)\n",
    "\n",
    "participant_recced_site_map = {}\n",
    "for participant_id, group in batch_df.groupby('participant_id'):\n",
    "    recced_site_ids = []\n",
    "    for sse_site_list in group.sse_site_list:\n",
    "        recced_site_ids.extend([site['site_id'] for site in sse_site_list])\n",
    "    assert len(recced_site_ids) == len(set(recced_site_ids)), \"Duplicate rec was given.\"\n",
    "    recced_site_ids = list(set(recced_site_ids))\n",
    "    participant_recced_site_map[participant_id] = recced_site_ids\n",
    "#participant_recced_site_map = {row.participant_id: [site['site_id'] for site in row.sse_site_list] for row in batch_df.itertuples()}\n",
    "len(participant_recced_site_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30263fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_recced_sites = [len(v) for v in participant_recced_site_map.values()]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "\n",
    "ax.hist(n_recced_sites)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_recced_site_ids = set()\n",
    "previously_recced_site_ids.update(*[[site['site_id'] for site in row.sse_site_list] for row in batch_df.itertuples()])\n",
    "len(previously_recced_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee03b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b2638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea875a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach for building rec lists: use greedy algorithm\n",
    "# \"draft\" sites, with a maximum of 10 top-5 appearances for any site with < 1918 visits\n",
    "# to do this, just choose a random traversal order over the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "participant_site_dict = {}  # map of participant_id -> list of site_ids\n",
    "n_picks = 5\n",
    "rng = np.random.default_rng(13)\n",
    "\n",
    "restricted_site_count = {site_id: 10\n",
    "    for site_id in site_rank_dict.keys()\n",
    "}  # if site_index.at[site_id, 'visits'] < 100000\n",
    "disallowed_sites = [\n",
    "    #0,  # sept17th, deleted journal update\n",
    "    0,  # oct1, health misinformation https://www.caringbridge.org/visit/prayforkainoa/journal\n",
    "    0,  # oct8, weird self-promotion, possibly mirroring posts from LinkedIn (markkageyama)\n",
    "]\n",
    "participant_incurred_loss = defaultdict(float)  # how much did participants \"give up\" by not getting their top picks?\n",
    "\n",
    "n_missed_picks = 0\n",
    "n_previous_rec_attempts = 0\n",
    "n_disallowed_picks = 0\n",
    "\n",
    "finished_drafting = set()\n",
    "while len(finished_drafting) < len(predictions):\n",
    "    inds = np.arange(len(predictions))\n",
    "    inds = rng.permutation(inds)\n",
    "    for ind in inds:\n",
    "        rc = rc_list[ind]\n",
    "        scorer = predictions[ind]\n",
    "        participant_id = rc.source_user_id\n",
    "        if participant_id not in participant_site_dict:\n",
    "            participant_site_dict[participant_id] = []\n",
    "        participant_sites = participant_site_dict[participant_id]\n",
    "        if len(participant_sites) >= n_picks:\n",
    "            finished_drafting.add(participant_id)\n",
    "            continue\n",
    "        slot_to_fill = len(participant_sites) # e.g. 0 if the first pick, 4 if the 5th pick\n",
    "    \n",
    "        site_id_arr = scorer.site_id_arr\n",
    "        y_score_site = scorer.scores_dict[model_manager.model_config.model_name]\n",
    "\n",
    "        sort_inds = np.argsort(y_score_site)\n",
    "        pick_made = False\n",
    "        max_score = 0\n",
    "        while not pick_made:\n",
    "            preferred_site = site_id_arr[sort_inds[-(slot_to_fill) - 1]]\n",
    "            if preferred_site in participant_sites:  # attempted duplicate pick\n",
    "                slot_to_fill += 1\n",
    "                continue\n",
    "                \n",
    "            # check if site has previously been recced\n",
    "            if participant_id in participant_recced_site_map and preferred_site in participant_recced_site_map[participant_id]:\n",
    "                slot_to_fill += 1\n",
    "                n_previous_rec_attempts += 1\n",
    "                continue\n",
    "            # check if site is manually disallowed\n",
    "            if preferred_site in disallowed_sites:\n",
    "                slot_to_fill += 1\n",
    "                n_disallowed_picks += 1\n",
    "                continue\n",
    "            # check if preferred_site is available\n",
    "            if preferred_site in restricted_site_count:\n",
    "                if restricted_site_count[preferred_site] <= 0:\n",
    "                    restricted_site_count[preferred_site] -= 1\n",
    "                    n_missed_picks += 1\n",
    "                    slot_to_fill += 1\n",
    "                    max_score = max(max_score, y_score_site[sort_inds[-(slot_to_fill) - 1]])\n",
    "                else:\n",
    "                    restricted_site_count[preferred_site] -= 1\n",
    "                    pick_made = True\n",
    "            else:\n",
    "                pick_made = True\n",
    "        if max_score > 0:\n",
    "            incurred_loss = max_score - y_score_site[sort_inds[-(slot_to_fill) - 1]]\n",
    "            participant_incurred_loss[participant_id] += incurred_loss\n",
    "        participant_sites.append(preferred_site)\n",
    "n_missed_picks, n_disallowed_picks, n_previous_rec_attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091708b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of number of times picked for the \"restricted distribution\" sites\n",
    "# 10 sites were picked the maximum number of times (and thus could trigger conflicts)\n",
    "# note this doesn't show number of ATTEMPTED picks, which could be higher (and thus the number of sites with 11+ attempted picks could be lower than the number below)\n",
    "pd.Series(data=[10 - v for k, v in restricted_site_count.items() if v < 10], index=[k for k, v in restricted_site_count.items() if v < 10]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda98ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data=[v for k, v in restricted_site_count.items() if v < 10], index=[k for k, v in restricted_site_count.items() if v < 10]).sort_values().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfdd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much are participants \"giving up\" due to the duplication restriction?\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax.hist([v for v in participant_incurred_loss.values()], bins=len(participant_incurred_loss), log=False)\n",
    "\n",
    "plt.show()\n",
    "np.max(list(participant_incurred_loss.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f998a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_counts = defaultdict(int)\n",
    "for participant_id, site_ids in participant_site_dict.items():\n",
    "    for site_id in site_ids[:5]:  # include all sites that appear in the top 5\n",
    "        site_counts[site_id] += 1\n",
    "site_counts = list(site_counts.items())\n",
    "site_counts.sort(key=lambda t: t[1], reverse=True)\n",
    "site_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4115e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1599fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site_id, count in site_counts:\n",
    "    site_name = site_index.at[site_id, 'name']\n",
    "    print(f\"{count:>3} {site_id:>9}{'*' if site_id in previously_recced_site_ids else ' '} {site_index.at[site_id, 'visits']:>7} https://www.caringbridge.org/visit/{site_name}/journal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be62ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b23f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the participant_df\n",
    "# get participant data\n",
    "participant_id_filepath = os.path.join(git_root_dir, 'data/email/participant_ids.tsv')\n",
    "participant_df = pd.read_csv(participant_id_filepath, sep='\\t', header=0)\n",
    "print(len(participant_df))\n",
    "participant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the participant data contains entries for every person we are generating recs for\n",
    "assert len(set(participant_site_dict.keys()) - set(participant_df.user_id)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the batch_id\n",
    "batch_id = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get customized survey links\n",
    "\n",
    "survey_link_filepath = os.path.join(git_root_dir, f'data/survey/CaringBridge_Author_Recommendations_Feedback__b{batch_id}-Distribution_History.csv')\n",
    "survey_link_df = pd.read_csv(survey_link_filepath)\n",
    "print(len(survey_link_df))\n",
    "survey_link_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33423f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge feedback_survey_link into the participant_df\n",
    "survey_link_df['real_email_address'] = survey_link_df.Email.map(lambda e: e.strip().lower())\n",
    "survey_link_df['feedback_survey_link'] = survey_link_df.Link\n",
    "print(len(participant_df))\n",
    "participant_df = participant_df.merge(survey_link_df[['real_email_address', 'feedback_survey_link']], how='left', on='real_email_address', validate='one_to_one')\n",
    "print(len(participant_df))\n",
    "participant_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_index = participant_df.set_index('user_id')\n",
    "participant_index.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbsend.compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbsend.templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2317d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbrec.text import textdb\n",
    "\n",
    "def truncate_body(body):\n",
    "    if len(body) > 175:\n",
    "        return body[:175] + \" ...\"\n",
    "    return body\n",
    "\n",
    "generated_messages = []\n",
    "participant_data_list = []\n",
    "\n",
    "td = textdb.TextDatabase(config)\n",
    "text_db = td.get_text_db()\n",
    "try:\n",
    "    for participant_id, site_ids in tqdm(participant_site_dict.items(), total=len(participant_site_dict)):\n",
    "        email_address = participant_index.at[participant_id, 'real_email_address']\n",
    "        first_name = participant_index.at[participant_id, 'first_name']\n",
    "        feedback_survey_link = participant_index.at[participant_id, 'feedback_survey_link']\n",
    "        #print(email_address, first_name, feedback_survey_link)\n",
    "        rec_list = []\n",
    "    \n",
    "        empty_title = False\n",
    "        empty_body = False\n",
    "        for site_id in site_ids[:5]:  # include all sites that appear in the top 5\n",
    "            site_name = site_index.at[site_id, 'name']\n",
    "            site_title = site_index.at[site_id, 'title']\n",
    "            recent_update = int(most_recent_journal[most_recent_journal >= required_recent_journal_timestamp][site_id])\n",
    "            journal_oid = journal_df[(journal_df.site_id == site_id)&(journal_df.published_at == recent_update)].journal_oid.iloc[0]\n",
    "            raw_title, raw_body = td.get_raw_journal_text_from_db(text_db, journal_oid)\n",
    "            title = cbrec.text.textdb.clean_text(raw_title)\n",
    "            body = cbrec.text.textdb.clean_text(raw_body.replace(\"</div>\", \"</div> \"))\n",
    "            \n",
    "            body = body.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \")\n",
    "            body = truncate_body(body)\n",
    "            if title.strip() == \"\":\n",
    "                empty_title = True\n",
    "            if body.strip() == \"\":\n",
    "                empty_body = True\n",
    "            \n",
    "            # identify links and replace them with \"[link] \"\n",
    "            link_replaced = False\n",
    "            if not empty_body:\n",
    "                # search for links\n",
    "                delinked_body = re.sub('https?:\\/\\/[\\\\S]*\\s', '[link] ', body)\n",
    "                if delinked_body != body:\n",
    "                    print(f\"Removed link from '{body}', new text '{delinked_body}'\")\n",
    "                    body = delinked_body\n",
    "                    link_replaced = True\n",
    "                # look for things that google might think are links...\n",
    "                if re.search('(^|\\\\s|\\/)[^\\\\.\\\\s][^\\\\.\\\\s]?[^\\\\.\\\\s]?\\\\.[^\\\\.\\\\s][^\\\\s]*', body):\n",
    "                    print(f\"Suspicious maybe-link: '{body}'\")\n",
    "            if link_replaced:\n",
    "                # TODO search for a new body eligible body text?\n",
    "                pass\n",
    "            \n",
    "            #print(site_name, site_title, recent_update, title)\n",
    "            #print(body)\n",
    "            rec = {\n",
    "                'site_id': int(site_id),  # convert from int64\n",
    "                'site_name': site_name,\n",
    "                'site_title': site_title,\n",
    "                'journal_oid': journal_oid,\n",
    "                'journal_timestamp': recent_update,\n",
    "                'journal_body': body,\n",
    "                'journal_title': title,\n",
    "            }\n",
    "            rec_list.append(rec)\n",
    "        \n",
    "        #email_address = 'zwlevonian@gmail.com'  # override recipient email during testing\n",
    "        msg = cbsend.compose.create_email(participant_id, batch_id, email_address, first_name, feedback_survey_link, rec_list)\n",
    "        generated_messages.append((email_address, msg))\n",
    "        \n",
    "        d = {\n",
    "            'participant_id': participant_id,\n",
    "            'real_email_address': email_address,\n",
    "            'first_name': first_name,\n",
    "            'feedback_survey_link': feedback_survey_link,\n",
    "            'site_scores': site_scores_map[participant_id],\n",
    "            'sse_site_list': rec_list,\n",
    "            'sse_sent_timestamp': -1,\n",
    "        }\n",
    "        participant_data_list.append(d)\n",
    "finally:\n",
    "    text_db.close()\n",
    "len(generated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af946e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddafe709",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_address, msg = generated_messages[4]\n",
    "cbsend.compose.send_email(\"zwlevonian@gmail.com\", msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65801d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: is this what you expect it to be?\n",
    "batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_sent = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_address_send_time = {}\n",
    "for email_address, msg in tqdm(generated_messages, desc='Sending emails'):\n",
    "    if email_address in emails_sent:\n",
    "        raise ValueError(email_address)\n",
    "    result = cbsend.compose.send_email(email_address, msg)\n",
    "    email_address_send_time[email_address] = int(datetime.now().timestamp() * 1000)\n",
    "    if result:\n",
    "        emails_sent.add(email_address)\n",
    "len(emails_sent), len(email_address_send_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can manually verify that all emails were sent by looking in Sent folder\n",
    "email_address, msg = generated_messages[36]\n",
    "email_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if one wasn't sent, attempt resending manually\n",
    "email_address, msg = generated_messages[36]\n",
    "cbsend.compose.send_email(email_address, msg)\n",
    "email_address_send_time[email_address] = int(datetime.now().timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sent times to the participant_data_list\n",
    "for participant_data in participant_data_list:\n",
    "    participant_id = participant_data['participant_id']\n",
    "    email_address = participant_data['real_email_address']\n",
    "    participant_data['sse_sent_timestamp'] = email_address_send_time[email_address]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc84e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_data_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ed5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_data_filepath = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant', f'participant_rec_data_b{batch_id}.ndjson')\n",
    "with open(participant_data_filepath, 'w') as outfile:\n",
    "    for participant_data in participant_data_list:\n",
    "        outfile.write(json.dumps(participant_data) + \"\\n\")\n",
    "print(f\"Finished writing {participant_data_filepath}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42aabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: we accidentally used batch_id 3 for batch 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28acccd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11951a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first batch: hard-coded\n",
    "email_send_date = datetime.strptime(\"2021-09-02 14:57:26\", \"%Y-%m-%d %H:%M:%S\").astimezone(pytz.timezone('US/Central'))\n",
    "email_send_timestamp = int(email_send_date.timestamp() * 1000)\n",
    "email_send_date.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67a7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c0b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9957e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341100a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ae19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04592bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe12eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp\n",
    "# save everything we can for the future\n",
    "# this was replaced with the code above..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e070f",
   "metadata": {},
   "source": [
    "What do we want to save about a batch?\n",
    "\n",
    " - For each participant:\n",
    "   - All site ids + scores\n",
    "   - Sites included in order, and what order\n",
    "     - The messages included in the email, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e10f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_scores_map = {}\n",
    "for prediction in predictions:\n",
    "    rc = prediction[0]\n",
    "    scorer = prediction[1]\n",
    "    \n",
    "    participant_id = rc.source_user_id\n",
    "    site_id_arr = scorer.site_id_arr\n",
    "    y_score_site = scorer.scores_dict['PointwiseLinearTorchModel']\n",
    "    site_scores_map[participant_id] = [(int(site_id), float(score)) for site_id, score in zip(site_id_arr, y_score_site)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_rec_emails_filepath = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant', 'sent_rec_emails.tsv')\n",
    "email_send_timestamps = pd.read_csv(sent_rec_emails_filepath, sep='\\t', header=None, names=['email_send_timestamp', 'email_address']).set_index('email_address').email_send_timestamp\n",
    "email_send_timestamps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data that was generated for each participant\n",
    "td = textdb.TextDatabase(config)\n",
    "text_db = td.get_text_db()\n",
    "try:\n",
    "    participant_data_filepath = os.path.join(cbcore.data.paths.projects_data_dir, 'recsys-peer-match', 'participant', 'participant_rec_data.ndjson')\n",
    "    with open(participant_data_filepath, 'w') as outfile:\n",
    "        for participant_id, site_ids in tqdm(participant_site_dict.items(), total=len(participant_site_dict)):\n",
    "            email_address = participant_index.at[participant_id, 'real_email_address']\n",
    "            first_name = participant_index.at[participant_id, 'first_name']\n",
    "            feedback_survey_link = participant_index.at[participant_id, 'feedback_survey_link']\n",
    "\n",
    "            rec_list = []\n",
    "            for site_id in site_ids[:5]:  # include all sites that appear in the top 5\n",
    "                site_name = site_index.at[site_id, 'name']\n",
    "                site_title = site_index.at[site_id, 'title']\n",
    "                recent_update = int(most_recent_journal[most_recent_journal >= required_recent_journal_timestamp][site_id])\n",
    "                journal_oid = journal_df[(journal_df.site_id == site_id)&(journal_df.published_at == recent_update)].journal_oid.iloc[0]\n",
    "\n",
    "                raw_title, raw_body = td.get_raw_journal_text_from_db(text_db, journal_oid)\n",
    "                title = cbrec.text.textdb.clean_text(raw_title)\n",
    "                body = cbrec.text.textdb.clean_text(raw_body.replace(\"</div>\", \"</div> \"))\n",
    "                body = body.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "                body = truncate_body(body)\n",
    "\n",
    "                rec = {\n",
    "                    'site_id': site_id,\n",
    "                    'site_name': site_name,\n",
    "                    'site_title': site_title,\n",
    "                    'journal_oid': journal_oid,\n",
    "                    'journal_timestamp': recent_update,\n",
    "                    'cleaned_journal_body': body,\n",
    "                    'cleaned_journal_title': title,\n",
    "                }\n",
    "                rec_list.append(rec)\n",
    "            d = {\n",
    "                'participant_id': participant_id,\n",
    "                'real_email_address': email_address,\n",
    "                'first_name': first_name,\n",
    "                'feedback_survey_link': feedback_survey_link,\n",
    "                'site_scores': site_scores_map[participant_id],\n",
    "                'sse_site_list': rec_list,\n",
    "                'sse_sent_timestamp': int(email_send_timestamps.at[email_address]),\n",
    "            }\n",
    "            outfile.write(json.dumps(d) + \"\\n\")\n",
    "finally:\n",
    "    text_db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b74b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b6b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60a0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16532ae5",
   "metadata": {},
   "source": [
    "### Original training implementation\n",
    "\n",
    "Maybe some useful stuff here, but generally now defunct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_list = cbrec.utils.get_test_metadata(md_list)\n",
    "torch_model.test_model(test_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triples():\n",
    "    db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "    #triple_metadata = []\n",
    "    arrs = []\n",
    "    ys = []\n",
    "    \n",
    "    try:\n",
    "        for row in cbrec.featuredb.stream_triples(db):\n",
    "            #md = {key: row[key] for key in row.keys() if not key.endswith(\"_arr\")}\n",
    "            #triple_metadata.append(md)\n",
    "            target_feature_arr = np.concatenate([row['source_feature_arr'], row['target_feature_arr'], row['source_target_feature_arr']])\n",
    "            alt_feature_arr = np.concatenate([row['source_feature_arr'], row['alt_feature_arr'], row['source_alt_feature_arr']])\n",
    "            arrs.append(target_feature_arr)\n",
    "            ys.append(1)\n",
    "            arrs.append(alt_feature_arr)\n",
    "            ys.append(0)\n",
    "        #df = pd.DataFrame(triple_metadata)\n",
    "        #return df\n",
    "    finally:\n",
    "        db.close()\n",
    "    return arrs, ys\n",
    "        \n",
    "feature_arrs, ys = get_triples()\n",
    "len(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(feature_arrs)\n",
    "y_true = np.array(ys)\n",
    "X.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5be6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what numbers evenly divide out the total?\n",
    "for i in range(2, len(y_true) + 1):\n",
    "    if np.isclose((len(y_true) // i) - (len(y_true) / i), 0):\n",
    "        print(i, len(y_true) / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much memory is being used?\n",
    "import resource\n",
    "kbytes = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "f\"{kbytes / 1024 / 1024:.2f}GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_hidden, dropout_p=0.2):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # note: 768 is the size of the roBERTa outputs\n",
    "        self.fc1 = nn.Linear(27, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, 1, bias=False)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_p)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)  # note: not using F.sigmoid here, as the loss used includes the Sigmoid transformation\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def train_pytorch_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch-based Neural Net using the parameters defined in the learner_config\n",
    "    \"\"\"\n",
    "    \n",
    "    n_train = len(y_train)\n",
    "    n_test = len(y_test)\n",
    "    \n",
    "    verbose = True\n",
    "    n_hidden = 100\n",
    "    n_epochs = 100\n",
    "    lr_init = 0.01\n",
    "    max_lr = 0.1\n",
    "    dropout_p = 0.1\n",
    "    minibatch_size = len(y_train)\n",
    "    minibatch_size = min(n_train, minibatch_size)  # if minibatch_size is larger than n_train, force it to n_train\n",
    "    n_minibatches = int(np.ceil(n_train / minibatch_size))\n",
    "    \n",
    "    net = LinearNet(n_hidden, dropout_p)\n",
    "    \n",
    "    #optimizer = optim.SGD(net.parameters(), lr=lr_init, momentum=0.9)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr_init)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=max_lr,\n",
    "        steps_per_epoch=n_minibatches,\n",
    "        epochs=n_epochs,\n",
    "    )\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()  # pointwise loss function\n",
    "    \n",
    "    X_train_tensor = torch.from_numpy(X_train)\n",
    "    y_train_tensor = torch.from_numpy(y_train)\n",
    "    y_train_tensor = y_train_tensor.view(-1, 1)  # make labels 2-dimensional\n",
    "    y_train_tensor = y_train_tensor.type_as(X_train_tensor)\n",
    "    if verbose:\n",
    "        print(X_train_tensor.size(), y_train_tensor.size())\n",
    "    \n",
    "    net.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        s = datetime.now()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # shuffle the training data\n",
    "        # I am not sure if this matters at all\n",
    "        epoch_order = torch.randperm(n_train)\n",
    "        \n",
    "        mb_metrics = []  # store the minibatch_metrics, then average after\n",
    "        for minibatch in range(n_minibatches):\n",
    "            minibatch_start = minibatch * minibatch_size\n",
    "            minibatch_end = min(minibatch_start + minibatch_size, n_train)\n",
    "            if verbose and epoch == 0:\n",
    "                print(f\"    Minibatch for inds in {minibatch_start} - {minibatch_end}.\")\n",
    "            minibatch_inds = epoch_order[minibatch_start:minibatch_end]\n",
    "            \n",
    "            inputs = X_train_tensor[minibatch_inds]\n",
    "            labels = y_train_tensor[minibatch_inds]\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # compute and log the loss\n",
    "            y_train_pred = torch.sigmoid(outputs.detach()).view((-1,)).numpy()\n",
    "            y_train_pred = (y_train_pred >= 0.5).astype(int)  # binarize predictions with a 0.5 decision boundary\n",
    "            y_train_minibatch = y_train[minibatch_inds.numpy()]\n",
    "            acc = np.sum(y_train_pred == y_train_minibatch) / len(y_train_minibatch)\n",
    "            \n",
    "            mb_metrics.append((loss.item(), acc))\n",
    "        loss, acc = np.mean(np.array(mb_metrics), axis=0)\n",
    "            \n",
    "        should_stop_early = loss < 0.001\n",
    "        if verbose and (epoch < 5 or epoch == n_epochs - 1 or epoch % 10 == 0 or should_stop_early):\n",
    "            print(f\"{epoch:>3} ({datetime.now() - s}): loss={loss:.4f} accuracy={acc*100:.2f}% LR={optimizer.param_groups[0]['lr']:.2E}\")\n",
    "        if should_stop_early:\n",
    "            break\n",
    "    # this is a hack, but we store training results info back through the learner_config dictionary\n",
    "    final_train_loss = loss\n",
    "    final_epoch_count = epoch + 1\n",
    "    if verbose:\n",
    "        print(f\"Completed {final_epoch_count} epochs with a final train loss of {final_train_loss:.4f}.\")\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test)\n",
    "        outputs = net(X_test_tensor)\n",
    "        y_test_pred = torch.sigmoid(outputs.detach()).view((-1,)).numpy()\n",
    "        y_test_pred = (y_test_pred >= 0.5).astype(int)\n",
    "        acc = np.sum(y_test_pred == y_test) / len(y_test)\n",
    "        print(f\"Test acc: {acc*100:.2f}%\")\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff01d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f17071",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sklearn.preprocessing.StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(np.ceil(len(y_true) * 0.99))\n",
    "X_train = X[:n_train,:]\n",
    "X_test = X[n_train:,:]\n",
    "y_train = y_true[:n_train]\n",
    "y_test = y_true[n_train:]\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = train_pytorch_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1038dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_synth = np.random.normal(size=X.shape).astype(np.float32)\n",
    "y_true_synth = np.zeros(y_true.shape).astype(int)\n",
    "y_true_synth[np.arange(0, len(y_true_synth), 2)] = 1\n",
    "print(np.sum(y_true_synth) / len(y_true_synth))\n",
    "X_synth[y_true_synth == 1,0] += 3\n",
    "\n",
    "X_train_synth = X_synth[:n_train,:]\n",
    "X_test_synth = X_synth[n_train:,:]\n",
    "y_train_synth = y_true_synth[:n_train]\n",
    "y_test_synth = y_true_synth[n_train:]\n",
    "X_train_synth.shape, X_test_synth.shape, y_train_synth.shape, y_test_synth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,7))\n",
    "ax.hexbin(X_synth[:,0], X_synth[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = train_pytorch_model(X_train_synth, y_train_synth, X_test_synth, y_test_synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35aa50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md_list = cbrec.utils.get_test_metadata(md_list)\n",
    "len(test_md_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = df[df.type == 'test']\n",
    "len(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf21c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_contexts(config, test_md_list, clf):\n",
    "    db = cbrec.featuredb.get_db_by_filepath(config.feature_db_filepath)\n",
    "    \n",
    "    try:\n",
    "        for md in test_md_list:\n",
    "            metadata_id = md['metadata_id']\n",
    "            test_context = cbrec.featuredb.get_test_context_by_metadata_id(db, metadata_id, config)\n",
    "            rc = cbrec.reccontext.RecContext.create_from_test_context(config, md, test_context)\n",
    "            \n",
    "            scorer = cbrec.evaluation.SklearnModelScorer(config, rc, clf, \"PointwiseLogreg\")\n",
    "            metric_dict = scorer.score_proba()\n",
    "            md['baseline_metrics']['PointwiseLogreg'] = metric_dict\n",
    "    finally:\n",
    "        db.close()\n",
    "        \n",
    "get_test_contexts(config, test_md_list, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58f41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c975e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = test_md_list[0]['baseline_metrics'].keys()\n",
    "print(models)\n",
    "model_df_dict = {}\n",
    "for model in tqdm(models):\n",
    "    metrics_list = []\n",
    "    for md in test_md_list:\n",
    "        metrics = md['baseline_metrics'][model]\n",
    "        metrics['metadata_id'] = md['metadata_id']\n",
    "        metrics['source_user_initiated_in_train_period'] = md['source_user_initiated_in_train_period']\n",
    "        metrics['target_site_initiated_with_in_train_period'] = md['target_site_initiated_with_in_train_period']\n",
    "        metrics_list.append(metrics)\n",
    "    mdf = pd.DataFrame(metrics_list)\n",
    "    mdf['reciprocal_rank'] = 1 / mdf.target_rank\n",
    "    #mdf['reciprocal_rank_10'] = 1 / mdf.target_rank\n",
    "    model_df_dict[model] = mdf\n",
    "    print(model, len(mdf))\n",
    "len(model_df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for model in models:\n",
    "    mdf = model_df_dict[model][['target_raw_score', 'target_rank', 'reciprocal_rank', 'ndcg_1', 'ndcg_5', 'ndcg_10', 'ndcg_50']]\n",
    "    means = mdf.mean()\n",
    "    means = pd.concat([pd.Series([np.sum(mdf.target_rank <= 5) / len(mdf),], index=['% <= rank 5',]), means])\n",
    "    means = pd.concat([pd.Series([np.sum(mdf.target_rank <= 1) / len(mdf),], index=['% rank 1',]), means])\n",
    "    means = pd.concat([pd.Series([model,], index=['model',]), means])\n",
    "    scores.append(means)\n",
    "score_df = pd.DataFrame(scores).rename(columns={'target_rank': 'mean_rank', 'reciprocal_rank': 'mrr', 'target_raw_score': 'mean_raw_score'}).sort_values(by='mean_rank')\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cacfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = model_df_dict['PointwiseLinearTorchModel']\n",
    "print(len(mdf))\n",
    "mdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b21ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "\n",
    "ax.hist(mdf.acc, bins=np.linspace(0, 1, 20), log=True)\n",
    "ax.axvline(np.mean(mdf.acc), color='black', linestyle='--', alpha=0.8, label=f'Mean ({np.mean(mdf.acc):.2f})')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b42a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "\n",
    "ax.set_title(\"Distribution of predictions for targets\")\n",
    "ax.hist(mdf.target_raw_score, bins=np.linspace(0, 1, 20), log=True)\n",
    "ax.axvline(np.mean(mdf.target_raw_score), color='black', linestyle='--', alpha=0.8, label=f'Mean ({np.mean(mdf.target_raw_score):.2f})')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a68748",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "\n",
    "ax.set_title(\"Distribution of predictions for targets\")\n",
    "for source_in_train in [False, True]:\n",
    "    for target_in_train in [False, True]:\n",
    "        sdf = mdf[(mdf.source_user_initiated_in_train_period == source_in_train)&(mdf.target_site_initiated_with_in_train_period == target_in_train)]\n",
    "        ax.hist(sdf.target_raw_score, bins=np.linspace(0, 1, 20), log=True, alpha=0.5)\n",
    "        ax.axvline(np.mean(sdf.target_raw_score), color='black', linestyle='--', alpha=0.8, label=f'Mean (n={len(sdf)},s={source_in_train},t={target_in_train}) ({np.mean(sdf.target_raw_score):.2f})')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3edad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "\n",
    "ax.set_title(\"Distribution of target ranks\")\n",
    "bins = np.linspace(0, 5000, 20)\n",
    "for source_in_train in [False, True]:\n",
    "    for target_in_train in [False, True]:\n",
    "        sdf = mdf[(mdf.source_user_initiated_in_train_period == source_in_train)&(mdf.target_site_initiated_with_in_train_period == target_in_train)]\n",
    "        _, _, patches = ax.hist(sdf.target_rank, bins=bins, log=True, alpha=0.5)\n",
    "        ax.axvline(np.mean(sdf.target_rank), color=patches[0]._facecolor, linestyle='--', alpha=0.8, label=f'Mean Rank = {np.mean(sdf.target_rank):.1f} (n={len(sdf)},s={source_in_train},t={target_in_train})')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7627d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(mdf.source_user_initiated_in_train_period, mdf.target_site_initiated_with_in_train_period, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e71631",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(mdf.source_user_initiated_in_train_period, mdf.target_site_initiated_with_in_train_period, margins=True, values=mdf.target_rank, aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905acc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4b455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
